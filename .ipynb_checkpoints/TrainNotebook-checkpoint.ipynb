{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/shared/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/shared/anaconda3/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/datasets/base.py:198: retry (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use the retry module or similar alternatives.\n"
     ]
    }
   ],
   "source": [
    "# As usual, a bit of setup\n",
    "import time\n",
    "from time import process_time\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import LOUPE.WILLOW.loupe as lp\n",
    "import tensorflow as tf\n",
    "import h5py\n",
    "import pandas as pd\n",
    "import csv\n",
    "import copy\n",
    "import math\n",
    "from utils.data_utils import *\n",
    "import sys\n",
    "import re\n",
    "from utils.spj import Config\n",
    "from utils.spj import SPJ\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = (10.0, 8.0) # set default size of plots\n",
    "plt.rcParams['image.interpolation'] = 'nearest'\n",
    "plt.rcParams['image.cmap'] = 'gray'\n",
    "\n",
    "# for auto-reloading external modules\n",
    "# see http://stackoverflow.com/questions/1907993/autoreload-of-modules-in-ipython\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Specify Model Directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "DIRECTORY SET TO:  /home/martnzjulio_a/songze\n",
      "VERSION SET TO  :  test5\n"
     ]
    }
   ],
   "source": [
    "home_dir = \"/home/martnzjulio_a/songze\"\n",
    "#home_dir = \"/home/songzeli\"\n",
    "version = \"test5\"\n",
    "minibatch_size = 25\n",
    "\n",
    "print()\n",
    "print(\"DIRECTORY SET TO: \", home_dir)\n",
    "print(\"VERSION SET TO  : \", version)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of words in all captions:  532264\n",
      "Vocabulary Size (Unique):  11125\n"
     ]
    }
   ],
   "source": [
    "# Load Vocabulary\n",
    "embedding_size =512\n",
    "pad_len, num_steps = 30, 30\n",
    "max_num_proposals = 10\n",
    "vocabulary,vocab_size = caption_preprocess(home_dir)\n",
    "emb_matrix,word2id,id2word = get_wordvector(embedding_size,vocab_size,vocabulary)\n",
    "num_classes = len(word2id)\n",
    "\n",
    "# Word Embedding Matrix\n",
    "emb_matrix, word2id, id2word = get_wordvector(embedding_size,vocab_size,vocabulary) #changed by Songze"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load All Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "Number of Training Examples: 100\n",
      "\n",
      "VideoIds_train.shape:  (100,)\n",
      "Framestamps_train.shape:  (100, 2, 10)\n",
      "Xcaptions_train.shape:  (100, 10, 30)\n",
      "Ycaptions_train.shape:  (100, 10, 30)\n",
      "H_train.shape:  (100, 500, 10)\n",
      "Ipast_train.shape:  (100, 10, 10)\n",
      "Ifuture_train.shape:  (100, 10, 10)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "num_train = 100\n",
    "#num_train = 2000\n",
    "\n",
    "# Load Training Data\n",
    "train_file = home_dir + \"/SPJ/train_2400.csv\"\n",
    "train_ids,train_data,train_padded_proposals,train_padded_framestamps = video_preprocess(home_dir, train_file, max_num_proposals)\n",
    "\n",
    "# Train Captions\n",
    "train_padded_sentences,train_padded_sentences_2,train_padded_sentences_id = get_padded_sentences_id(pad_len, train_ids, train_data, word2id, max_num_proposals) \n",
    "Ycaptions_train = np.transpose(copy.deepcopy(train_padded_sentences_2),axes=(0,2,1)).astype(np.int32)[:num_train,:,1:]\n",
    "Xcaptions_train = np.transpose(copy.deepcopy(train_padded_sentences),axes=(0,2,1)).astype(np.int32)[:num_train]\n",
    "\n",
    "Ycaptions_train = truncate_captions(Ycaptions_train)\n",
    "Xcaptions_train = truncate_captions(Xcaptions_train)\n",
    "\n",
    "\n",
    "# Train Features \n",
    "VideoIds_train = train_ids[:num_train]\n",
    "Framestamps_train = train_padded_framestamps[:num_train]\n",
    "H_train = train_padded_proposals.astype(np.float32)[:num_train]\n",
    "Ipast_train = temporal_indicator(train_padded_framestamps, mode=\"past\").astype(np.float32)[:num_train]\n",
    "Ifuture_train = temporal_indicator(train_padded_framestamps, mode=\"future\").astype(np.float32)[:num_train]\n",
    "\n",
    "num_train = len(train_ids[:num_train])\n",
    "print(\"Number of Training Examples:\", num_train)\n",
    "print()\n",
    "print(\"VideoIds_train.shape: \", VideoIds_train.shape)\n",
    "print(\"Framestamps_train.shape: \", Framestamps_train.shape)\n",
    "print(\"Xcaptions_train.shape: \", Xcaptions_train.shape)\n",
    "print(\"Ycaptions_train.shape: \", Ycaptions_train.shape)\n",
    "print(\"H_train.shape: \", H_train.shape)\n",
    "print(\"Ipast_train.shape: \", Ipast_train.shape)\n",
    "print(\"Ifuture_train.shape: \", Ifuture_train.shape)\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Validation Examples: 100\n",
      "\n",
      "VideoIds_val.shape:  (100,)\n",
      "Framestamps_val.shape:  (100, 2, 10)\n",
      "Xcaptions_val.shape:  (100, 10, 30)\n",
      "Ycaptions_val.shape:  (100, 10, 30)\n",
      "H_val.shape:  (100, 500, 10)\n",
      "Ipast_val.shape:  (100, 10, 10)\n",
      "Ifuture_val.shape:  (100, 10, 10)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "num_val = 100\n",
    "#num_val = 225\n",
    "\n",
    "# Load Validation Data\n",
    "val_file = home_dir + \"/SPJ/train_val_300.csv\"\n",
    "val_ids,val_data,val_padded_proposals,val_padded_framestamps = video_preprocess(home_dir, val_file, max_num_proposals)\n",
    "\n",
    "# Train Captions\n",
    "val_padded_sentences,val_padded_sentences_2,val_padded_sentences_id = get_padded_sentences_id(pad_len, val_ids, val_data, word2id, max_num_proposals) \n",
    "Ycaptions_val = np.transpose(copy.deepcopy(val_padded_sentences_2),axes=(0,2,1)).astype(np.int32)[:num_val,:,1:]\n",
    "Xcaptions_val = np.transpose(copy.deepcopy(val_padded_sentences),axes=(0,2,1)).astype(np.int32)[:num_val]\n",
    "Ycaptions_val = truncate_captions(Ycaptions_val)\n",
    "Xcaptions_val = truncate_captions(Xcaptions_val)\n",
    "\n",
    "\n",
    "# Train Features \n",
    "VideoIds_val = val_ids[:num_val]\n",
    "Framestamps_val = val_padded_framestamps[:num_val]\n",
    "H_val = val_padded_proposals.astype(np.float32)[:num_val]\n",
    "Ipast_val = temporal_indicator(val_padded_framestamps, mode=\"past\").astype(np.float32)[:num_val]\n",
    "Ifuture_val = temporal_indicator(val_padded_framestamps, mode=\"future\").astype(np.float32)[:num_val]\n",
    "\n",
    "num_val = len(val_ids[:num_val])\n",
    "print(\"Number of Validation Examples:\", num_val)\n",
    "print()\n",
    "print(\"VideoIds_val.shape: \", VideoIds_val.shape)\n",
    "print(\"Framestamps_val.shape: \", Framestamps_val.shape)\n",
    "print(\"Xcaptions_val.shape: \", Xcaptions_val.shape)\n",
    "print(\"Ycaptions_val.shape: \", Ycaptions_val.shape)\n",
    "print(\"H_val.shape: \", H_val.shape)\n",
    "print(\"Ipast_val.shape: \", Ipast_val.shape)\n",
    "print(\"Ifuture_val.shape: \", Ifuture_val.shape)\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model(all_train, all_val, starter_learning_rate, keep_prob, num_epochs, home_dir, version, print_cost = True):\n",
    "    \"\"\"\n",
    "    Implements a tensorflow neural network: C3D->ATTENTION->CAPTIONING\n",
    "    \n",
    "    Arguments:\n",
    "    H_train -- training set, of shape = [n_train,num_c3d_features,num_proposals]\n",
    "    Y_train -- caption labels, of shape = [n_train,num_proposals,num_steps+1]\n",
    "    H_test -- training set, of shape = [n_test,num_c3d_features,num_proposals]\n",
    "    Y_test -- caption labels, of shape = [n_test,num_proposals,num_steps+1]\n",
    "    learning_rate -- learning rate of the optimization\n",
    "    num_epochs -- number of epochs of the optimization loop\n",
    "    minibatch_size -- size of a minibatch\n",
    "    print_cost -- True to print the cost every 100 epochs\n",
    "    \n",
    "    Returns:\n",
    "    parameters -- parameters learnt by the model. They can then be used to predict.\n",
    "    \"\"\"\n",
    "    (VideoIds_train, Framestamps_train, H_train, Ipast_train, Ifuture_train, Ycaptions_train, Xcaptions_train) = all_train\n",
    "    (VideoIds_val, Framestamps_val, H_val,   Ipast_val,   Ifuture_val,   Ycaptions_val,   Xcaptions_val)   = all_val\n",
    "    \n",
    "    # Directory to Save Checkpoint\n",
    "    checkpoint_dir = home_dir + \"/checkpoints_\" + str(version) + \"/\"\n",
    "    tensorboard_dir =  home_dir + \"/tensorboard_\" + str(version) + \"/\"\n",
    "    print(\"Checkpoint directory: \", checkpoint_dir)\n",
    "    print(\"Tensorboard directory: \", tensorboard_dir)\n",
    "    \n",
    "    # Reset Graph\n",
    "    tf.reset_default_graph()    \n",
    "    \n",
    "    # For Consistency\n",
    "    tf.set_random_seed(1)                             \n",
    "    seed = 3                                         \n",
    "    \n",
    "    # Number of Training Examples\n",
    "    num_train = H_train.shape[0] \n",
    "    num_val = H_val.shape[0] \n",
    "    \n",
    "    # to keep track of costs\n",
    "    costs = []\n",
    "    \n",
    "    \n",
    "    # Model\n",
    "    config = Config()\n",
    "    spj = SPJ(config)\n",
    "    \n",
    "    # Print Hyperparameters\n",
    "    print()\n",
    "    print(\"Hyperparameters:\")\n",
    "    print(\"----------------\")\n",
    "    print(\"Starter Learning Rate: \", starter_learning_rate)\n",
    "    print(\"Number of Proposals: \", spj.config.num_proposals)\n",
    "    print(\"C3D Features Dim: \", spj.config.num_c3d_features )\n",
    "    print(\"Batch Size: \", spj.config.batch_size)\n",
    "    print(\"Dropout Keep Prob: \", keep_prob)\n",
    "    print(\"Vocab Size: \", spj.config.num_classes)\n",
    "    print(\"Number of LSTM Time Steps: \", spj.config.num_steps)\n",
    "    print(\"Word Embedding Size: \" , spj.config.hidden_dim)\n",
    "    print(\"LSTM Hidden Dim: \" , spj.config.hidden_dim)\n",
    "    print(\"LSTM Num Layers: \" , spj.config.num_layers)\n",
    "    \n",
    "    # Global Epoch Number\n",
    "    global_step = tf.Variable(0, name='global_step', trainable=False)\n",
    "\n",
    "    # Learning Rate Decay\n",
    "    learning_rate = tf.train.exponential_decay(\n",
    "        learning_rate = starter_learning_rate, \n",
    "        global_step = global_step,\n",
    "        decay_steps = 100000, \n",
    "        decay_rate = 0.96, \n",
    "        staircase=True)\n",
    "    \n",
    "    # Backpropagation: Define the tensorflow optimizer. Use an AdamOptimizer.\n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate = learning_rate).minimize(spj._loss, global_step=global_step)\n",
    "    #optimizer = tf.train.MomentumOptimizer(learning_rate=learning_rate,momentum=0.9).minimize(spj._loss, global_step=global_step)  \n",
    "    \n",
    "    # Passing global_step to minimize() will increment it at each step.\n",
    "    learning_step = (optimizer)\n",
    "    \n",
    "    # Initialize all the variables\n",
    "    init = tf.global_variables_initializer()\n",
    "    \n",
    "    # Add ops to save and restore all the variables.\n",
    "    saver = tf.train.Saver(max_to_keep=5)\n",
    "    \n",
    "    # Tensorboard Loss\n",
    "    #training_summary = tf.summary.scalar(\"training_loss\", spj.loss)\n",
    "    #validation_summary = tf.summary.scalar(\"validation_loss\", spj.loss)\n",
    "    #writer = tf.train.SummaryWriter(...)\n",
    "    \n",
    "\n",
    "    # Start the session to compute the tensorflow graph\n",
    "    with tf.Session() as sess:\n",
    "        \n",
    "        #Tensorboard\n",
    "        summary_writer = tf.summary.FileWriter(tensorboard_dir,sess.graph)\n",
    "    \n",
    "        # check for latest checkpoint\n",
    "        latest_checkpoint = tf.train.latest_checkpoint(checkpoint_dir)\n",
    "        if latest_checkpoint == None:\n",
    "            # If no check point run the initialization\n",
    "            print()\n",
    "            print(\"No checkpoint exists, initializing parameters...\")\n",
    "            sess.run(init)\n",
    "        else:\n",
    "            print()\n",
    "            print(\"Restoring from latest checkpoint...\")\n",
    "            saver.restore(sess, latest_checkpoint)\n",
    "        \n",
    "        # Training Loop\n",
    "        for epoch in range(num_epochs):\n",
    "            \n",
    "            # Record start time\n",
    "            print()\n",
    "            start = process_time() \n",
    "            \n",
    "            # Variable to store cost\n",
    "            epoch_train_loss = 0.0\n",
    "            epoch_val_loss = 0.0\n",
    "            train_losses = []\n",
    "            val_losses = []\n",
    "            \n",
    "            # Get minibatches\n",
    "            num_train_minibatches = num_train // spj.config.batch_size \n",
    "            num_val_minibatches = num_val // spj.config.batch_size \n",
    "            seed = seed + 1\n",
    "            train_minibatches = random_mini_batches(VideoIds_train, Framestamps_train, H_train, Ipast_train, Ifuture_train, Ycaptions_train, Xcaptions_train, spj.config.batch_size , seed)\n",
    "            val_minibatches = random_mini_batches(VideoIds_val, Framestamps_val, H_val, Ipast_val,   Ifuture_val,   Ycaptions_val,   Xcaptions_val,   spj.config.batch_size , seed)\n",
    "            \n",
    "            for counter, train_minibatch in enumerate(train_minibatches):\n",
    "                \n",
    "                # Select minibatch\n",
    "                (minibatch_VideoIds_train, minibatch_Framestamps_train, minibatch_H_train, minibatch_Ipast_train, minibatch_Ifuture_train, minibatch_Ycaptions_train, minibatch_Xcaptions_train) = train_minibatch\n",
    "                minibatch_Ycaptions_train = id_2_one_hot_void_padding(minibatch_Ycaptions_train, spj.config.num_classes, void_dim=0)\n",
    "                \n",
    "                # Run Train Session\n",
    "                train_feed={spj._H: minibatch_H_train, \n",
    "                      spj._Ipast: minibatch_Ipast_train, \n",
    "                      spj._Ifuture: minibatch_Ifuture_train, \n",
    "                      spj._x: minibatch_Xcaptions_train, \n",
    "                      spj._y: minibatch_Ycaptions_train, \n",
    "                      spj._keep_prob: keep_prob,\n",
    "                      spj._reg: 0.0}\n",
    "                _ , minibatch_train_loss, lr = sess.run([optimizer, spj._loss, learning_rate], feed_dict=train_feed)\n",
    "                train_losses.append(minibatch_train_loss)\n",
    "                \n",
    "            for counter, val_minibatch in enumerate(val_minibatches):\n",
    "                \n",
    "                # Select minibatch\n",
    "                (minibatch_VideoIds_val, minibatch_Framestamps_val, minibatch_H_val, minibatch_Ipast_val, minibatch_Ifuture_val, minibatch_Ycaptions_val, minibatch_Xcaptions_val) = val_minibatch\n",
    "                minibatch_Ycaptions_val = id_2_one_hot_void_padding(minibatch_Ycaptions_val, spj.config.num_classes, void_dim=0)\n",
    "                \n",
    "                # Run Validation Session\n",
    "                val_feed={spj._H: minibatch_H_val, \n",
    "                          spj._Ipast: minibatch_Ipast_val, \n",
    "                          spj._Ifuture: minibatch_Ifuture_val, \n",
    "                          spj._x: minibatch_Xcaptions_val, \n",
    "                          spj._y: minibatch_Ycaptions_val, \n",
    "                          spj._keep_prob: 1.0,\n",
    "                          spj._reg: 0.0}\n",
    "                minibatch_val_loss = sess.run([spj._loss], feed_dict=val_feed) #\n",
    "                val_losses.append(minibatch_val_loss)\n",
    "            \n",
    "            epoch_train_loss = np.mean(train_losses)\n",
    "            epoch_val_loss = np.mean(val_losses)\n",
    "            \n",
    "            # Print cost\n",
    "            if print_cost == True:\n",
    "                global_epoch = tf.train.global_step(sess, global_step)//num_train_minibatches\n",
    "                print(\"Epoch: \", global_epoch)\n",
    "                print(\"Current Learning Rate\", lr)\n",
    "                print (\"Training Loss: \", epoch_train_loss)\n",
    "                print (\"Validation Loss: \", epoch_val_loss)\n",
    "                # Add and Write to Tensorboard\n",
    "                train_summary = tf.Summary()\n",
    "                val_summary = tf.Summary()\n",
    "                train_summary.value.add(tag=\"train_losss\", simple_value=epoch_train_loss)\n",
    "                train_summary.value.add(tag=\"val_losss\", simple_value=epoch_val_loss)\n",
    "                summary_writer.add_summary(train_summary, global_epoch)\n",
    "                summary_writer.add_summary(val_summary, global_epoch)\n",
    "\n",
    "            \n",
    "            # Save Model (every 20 epochs)\n",
    "            if global_epoch % 10 == 0:\n",
    "                print(\"Saving Checkpoint for global_step \" + str(global_epoch))\n",
    "                saver.save(sess, checkpoint_dir + 'model', global_step = global_epoch)\n",
    "        \n",
    "            # Save and Print Processed Time\n",
    "            end = process_time() \n",
    "            print()\n",
    "            print(\"Time Elapased: \", end - start)\n",
    "        \n",
    "        return 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoint directory:  /home/martnzjulio_a/songze/checkpoints_test5/\n",
      "Tensorboard directory:  /home/martnzjulio_a/songze/tensorboard_test5/\n",
      "\n",
      "Hyperparameters:\n",
      "----------------\n",
      "Starter Learning Rate:  0.01\n",
      "Number of Proposals:  10\n",
      "C3D Features Dim:  500\n",
      "Batch Size:  25\n",
      "Dropout Keep Prob:  1.0\n",
      "Vocab Size:  10999\n",
      "Number of LSTM Time Steps:  30\n",
      "Word Embedding Size:  512\n",
      "LSTM Hidden Dim:  512\n",
      "LSTM Num Layers:  2\n",
      "\n",
      "No checkpoint exists, initializing parameters...\n",
      "\n",
      "Epoch:  1\n",
      "Current Learning Rate 0.01\n",
      "Training Loss:  2.1883602\n",
      "Validation Loss:  1.1025133\n",
      "\n",
      "Time Elapased:  7.479494244999998\n",
      "\n",
      "Epoch:  2\n",
      "Current Learning Rate 0.01\n",
      "Training Loss:  1.677424\n",
      "Validation Loss:  1.1018224\n",
      "\n",
      "Time Elapased:  7.7254880910000026\n",
      "\n",
      "Epoch:  3\n",
      "Current Learning Rate 0.01\n",
      "Training Loss:  1.5710958\n",
      "Validation Loss:  1.0790377\n",
      "\n",
      "Time Elapased:  6.5889433749999995\n",
      "\n",
      "Epoch:  4\n",
      "Current Learning Rate 0.01\n",
      "Training Loss:  1.5224636\n",
      "Validation Loss:  1.0685241\n",
      "\n",
      "Time Elapased:  6.541827893999994\n",
      "\n",
      "Epoch:  5\n",
      "Current Learning Rate 0.01\n",
      "Training Loss:  1.4837018\n",
      "Validation Loss:  1.0606045\n",
      "\n",
      "Time Elapased:  6.331223225000002\n",
      "\n",
      "Epoch:  6\n",
      "Current Learning Rate 0.01\n",
      "Training Loss:  1.4636008\n",
      "Validation Loss:  1.0625802\n",
      "\n",
      "Time Elapased:  6.515099198999991\n",
      "\n",
      "Epoch:  7\n",
      "Current Learning Rate 0.01\n",
      "Training Loss:  1.4449651\n",
      "Validation Loss:  1.0612793\n",
      "\n",
      "Time Elapased:  6.963932216999993\n",
      "\n",
      "Epoch:  8\n",
      "Current Learning Rate 0.01\n",
      "Training Loss:  1.4290535\n",
      "Validation Loss:  1.0687313\n",
      "\n",
      "Time Elapased:  6.870308558000005\n",
      "\n",
      "Epoch:  9\n",
      "Current Learning Rate 0.01\n",
      "Training Loss:  1.410729\n",
      "Validation Loss:  1.0683217\n",
      "\n",
      "Time Elapased:  6.566521926000007\n",
      "\n",
      "Epoch:  10\n",
      "Current Learning Rate 0.01\n",
      "Training Loss:  1.3960922\n",
      "Validation Loss:  1.0681226\n",
      "Saving Checkpoint for global_step 10\n",
      "\n",
      "Time Elapased:  7.511177708999995\n",
      "\n",
      "Epoch:  11\n",
      "Current Learning Rate 0.01\n",
      "Training Loss:  1.385365\n",
      "Validation Loss:  1.0648394\n",
      "\n",
      "Time Elapased:  6.923928729000011\n",
      "\n",
      "Epoch:  12\n",
      "Current Learning Rate 0.01\n",
      "Training Loss:  1.3702159\n",
      "Validation Loss:  1.0703976\n",
      "\n",
      "Time Elapased:  6.656056141999983\n",
      "\n",
      "Epoch:  13\n",
      "Current Learning Rate 0.01\n",
      "Training Loss:  1.3585467\n",
      "Validation Loss:  1.0737123\n",
      "\n",
      "Time Elapased:  6.510104963999993\n",
      "\n",
      "Epoch:  14\n",
      "Current Learning Rate 0.01\n",
      "Training Loss:  1.3489017\n",
      "Validation Loss:  1.0756342\n",
      "\n",
      "Time Elapased:  6.861203751000005\n",
      "\n",
      "Epoch:  15\n",
      "Current Learning Rate 0.01\n",
      "Training Loss:  1.3385899\n",
      "Validation Loss:  1.0811605\n",
      "\n",
      "Time Elapased:  6.617462577999987\n",
      "\n",
      "Epoch:  16\n",
      "Current Learning Rate 0.01\n",
      "Training Loss:  1.3287976\n",
      "Validation Loss:  1.0854535\n",
      "\n",
      "Time Elapased:  7.287711064999996\n",
      "\n",
      "Epoch:  17\n",
      "Current Learning Rate 0.01\n",
      "Training Loss:  1.3206812\n",
      "Validation Loss:  1.0872564\n",
      "\n",
      "Time Elapased:  6.92946507100001\n",
      "\n",
      "Epoch:  18\n",
      "Current Learning Rate 0.01\n",
      "Training Loss:  1.3108261\n",
      "Validation Loss:  1.0897325\n",
      "\n",
      "Time Elapased:  6.557545728000008\n",
      "\n",
      "Epoch:  19\n",
      "Current Learning Rate 0.01\n",
      "Training Loss:  1.3019131\n",
      "Validation Loss:  1.093796\n",
      "\n",
      "Time Elapased:  7.122461466000004\n",
      "\n",
      "Epoch:  20\n",
      "Current Learning Rate 0.01\n",
      "Training Loss:  1.2926587\n",
      "Validation Loss:  1.0948358\n",
      "Saving Checkpoint for global_step 20\n",
      "\n",
      "Time Elapased:  7.778424086000001\n",
      "\n",
      "Epoch:  21\n",
      "Current Learning Rate 0.01\n",
      "Training Loss:  1.2882347\n",
      "Validation Loss:  1.099794\n",
      "\n",
      "Time Elapased:  6.783229672999994\n",
      "\n",
      "Epoch:  22\n",
      "Current Learning Rate 0.01\n",
      "Training Loss:  1.2811575\n",
      "Validation Loss:  1.1010137\n",
      "\n",
      "Time Elapased:  6.756467240000006\n",
      "\n",
      "Epoch:  23\n",
      "Current Learning Rate 0.01\n",
      "Training Loss:  1.272409\n",
      "Validation Loss:  1.1067656\n",
      "\n",
      "Time Elapased:  6.6040123119999805\n",
      "\n",
      "Epoch:  24\n",
      "Current Learning Rate 0.01\n",
      "Training Loss:  1.2685349\n",
      "Validation Loss:  1.1097746\n",
      "\n",
      "Time Elapased:  6.552408509000003\n",
      "\n",
      "Epoch:  25\n",
      "Current Learning Rate 0.01\n",
      "Training Loss:  1.2590538\n",
      "Validation Loss:  1.1103374\n",
      "\n",
      "Time Elapased:  6.6346448479999935\n",
      "\n",
      "Epoch:  26\n",
      "Current Learning Rate 0.01\n",
      "Training Loss:  1.2520099\n",
      "Validation Loss:  1.1158645\n",
      "\n",
      "Time Elapased:  6.418237575000006\n",
      "\n",
      "Epoch:  27\n",
      "Current Learning Rate 0.01\n",
      "Training Loss:  1.2474452\n",
      "Validation Loss:  1.1191809\n",
      "\n",
      "Time Elapased:  6.340585111999985\n",
      "\n",
      "Epoch:  28\n",
      "Current Learning Rate 0.01\n",
      "Training Loss:  1.2419055\n",
      "Validation Loss:  1.1211292\n",
      "\n",
      "Time Elapased:  6.350590562999997\n",
      "\n",
      "Epoch:  29\n",
      "Current Learning Rate 0.01\n",
      "Training Loss:  1.2369926\n",
      "Validation Loss:  1.123557\n",
      "\n",
      "Time Elapased:  6.960537527000014\n",
      "\n",
      "Epoch:  30\n",
      "Current Learning Rate 0.01\n",
      "Training Loss:  1.229852\n",
      "Validation Loss:  1.1278589\n",
      "Saving Checkpoint for global_step 30\n",
      "\n",
      "Time Elapased:  7.078722113000026\n",
      "\n",
      "Epoch:  31\n",
      "Current Learning Rate 0.01\n",
      "Training Loss:  1.2205209\n",
      "Validation Loss:  1.1312244\n",
      "\n",
      "Time Elapased:  6.969249005999984\n",
      "\n",
      "Epoch:  32\n",
      "Current Learning Rate 0.01\n",
      "Training Loss:  1.2124547\n",
      "Validation Loss:  1.1341112\n",
      "\n",
      "Time Elapased:  7.146445708999977\n",
      "\n",
      "Epoch:  33\n",
      "Current Learning Rate 0.01\n",
      "Training Loss:  1.2103635\n",
      "Validation Loss:  1.1354456\n",
      "\n",
      "Time Elapased:  7.627954531\n",
      "\n",
      "Epoch:  34\n",
      "Current Learning Rate 0.01\n",
      "Training Loss:  1.204422\n",
      "Validation Loss:  1.1404922\n",
      "\n",
      "Time Elapased:  7.496864963000007\n",
      "\n",
      "Epoch:  35\n",
      "Current Learning Rate 0.01\n",
      "Training Loss:  1.1967713\n",
      "Validation Loss:  1.1403037\n",
      "\n",
      "Time Elapased:  6.931704169\n",
      "\n",
      "Epoch:  36\n",
      "Current Learning Rate 0.01\n",
      "Training Loss:  1.1935853\n",
      "Validation Loss:  1.1446542\n",
      "\n",
      "Time Elapased:  7.15150913399998\n",
      "\n",
      "Epoch:  37\n",
      "Current Learning Rate 0.01\n",
      "Training Loss:  1.1867803\n",
      "Validation Loss:  1.1479329\n",
      "\n",
      "Time Elapased:  7.112986293000006\n",
      "\n",
      "Epoch:  38\n",
      "Current Learning Rate 0.01\n",
      "Training Loss:  1.1795148\n",
      "Validation Loss:  1.1487395\n",
      "\n",
      "Time Elapased:  7.258426267000004\n",
      "\n",
      "Epoch:  39\n",
      "Current Learning Rate 0.01\n",
      "Training Loss:  1.1806239\n",
      "Validation Loss:  1.1551809\n",
      "\n",
      "Time Elapased:  7.609152039000037\n",
      "\n",
      "Epoch:  40\n",
      "Current Learning Rate 0.01\n",
      "Training Loss:  1.1867867\n",
      "Validation Loss:  1.1518745\n",
      "Saving Checkpoint for global_step 40\n",
      "\n",
      "Time Elapased:  8.071036209999988\n",
      "\n",
      "Epoch:  41\n",
      "Current Learning Rate 0.01\n",
      "Training Loss:  1.1735117\n",
      "Validation Loss:  1.1594733\n",
      "\n",
      "Time Elapased:  7.716164805999995\n",
      "\n",
      "Epoch:  42\n",
      "Current Learning Rate 0.01\n",
      "Training Loss:  1.1706985\n",
      "Validation Loss:  1.1585572\n",
      "\n",
      "Time Elapased:  7.277424316000008\n",
      "\n",
      "Epoch:  43\n",
      "Current Learning Rate 0.01\n",
      "Training Loss:  1.1666133\n",
      "Validation Loss:  1.159395\n",
      "\n",
      "Time Elapased:  7.422514503000002\n",
      "\n",
      "Epoch:  44\n",
      "Current Learning Rate 0.01\n",
      "Training Loss:  1.1655483\n",
      "Validation Loss:  1.1610852\n",
      "\n",
      "Time Elapased:  6.525327070999992\n",
      "\n",
      "Epoch:  45\n",
      "Current Learning Rate 0.01\n",
      "Training Loss:  1.161797\n",
      "Validation Loss:  1.1598715\n",
      "\n",
      "Time Elapased:  7.28193225900003\n",
      "\n",
      "Epoch:  46\n",
      "Current Learning Rate 0.01\n",
      "Training Loss:  1.1587013\n",
      "Validation Loss:  1.1654793\n",
      "\n",
      "Time Elapased:  7.845012589000021\n",
      "\n",
      "Epoch:  47\n",
      "Current Learning Rate 0.01\n",
      "Training Loss:  1.1504862\n",
      "Validation Loss:  1.1660179\n",
      "\n",
      "Time Elapased:  7.6673963490000006\n",
      "\n",
      "Epoch:  48\n",
      "Current Learning Rate 0.01\n",
      "Training Loss:  1.1454494\n",
      "Validation Loss:  1.1634743\n",
      "\n",
      "Time Elapased:  6.859013015000016\n",
      "\n",
      "Epoch:  49\n",
      "Current Learning Rate 0.01\n",
      "Training Loss:  1.1420562\n",
      "Validation Loss:  1.1761699\n",
      "\n",
      "Time Elapased:  7.428271908999989\n",
      "\n",
      "Epoch:  50\n",
      "Current Learning Rate 0.01\n",
      "Training Loss:  1.1427699\n",
      "Validation Loss:  1.1685789\n",
      "Saving Checkpoint for global_step 50\n",
      "\n",
      "Time Elapased:  7.128799528000002\n",
      "\n",
      "Epoch:  51\n",
      "Current Learning Rate 0.01\n",
      "Training Loss:  1.1400328\n",
      "Validation Loss:  1.1754198\n",
      "\n",
      "Time Elapased:  6.760794852999993\n",
      "\n",
      "Epoch:  52\n",
      "Current Learning Rate 0.01\n",
      "Training Loss:  1.1316035\n",
      "Validation Loss:  1.1777759\n",
      "\n",
      "Time Elapased:  6.308636342\n",
      "\n",
      "Epoch:  53\n",
      "Current Learning Rate 0.01\n",
      "Training Loss:  1.1271898\n",
      "Validation Loss:  1.1778423\n",
      "\n",
      "Time Elapased:  6.737882172000013\n",
      "\n",
      "Epoch:  54\n",
      "Current Learning Rate 0.01\n",
      "Training Loss:  1.1286329\n",
      "Validation Loss:  1.1796525\n",
      "\n",
      "Time Elapased:  7.335843924000017\n",
      "\n",
      "Epoch:  55\n",
      "Current Learning Rate 0.01\n",
      "Training Loss:  1.1245115\n",
      "Validation Loss:  1.1838388\n",
      "\n",
      "Time Elapased:  7.173376523999991\n",
      "\n",
      "Epoch:  56\n",
      "Current Learning Rate 0.01\n",
      "Training Loss:  1.1173108\n",
      "Validation Loss:  1.1865237\n",
      "\n",
      "Time Elapased:  6.741905970999994\n",
      "\n",
      "Epoch:  57\n",
      "Current Learning Rate 0.01\n",
      "Training Loss:  1.1169051\n",
      "Validation Loss:  1.1833457\n",
      "\n",
      "Time Elapased:  7.008143109999992\n",
      "\n",
      "Epoch:  58\n",
      "Current Learning Rate 0.01\n",
      "Training Loss:  1.1125972\n",
      "Validation Loss:  1.1895504\n",
      "\n",
      "Time Elapased:  6.76501891800001\n",
      "\n",
      "Epoch:  59\n",
      "Current Learning Rate 0.01\n",
      "Training Loss:  1.1054103\n",
      "Validation Loss:  1.1920106\n",
      "\n",
      "Time Elapased:  6.829763475999982\n",
      "\n",
      "Epoch:  60\n",
      "Current Learning Rate 0.01\n",
      "Training Loss:  1.1025457\n",
      "Validation Loss:  1.1944976\n",
      "Saving Checkpoint for global_step 60\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Time Elapased:  6.736685659999978\n",
      "\n",
      "Epoch:  61\n",
      "Current Learning Rate 0.01\n",
      "Training Loss:  1.0960027\n",
      "Validation Loss:  1.1945207\n",
      "\n",
      "Time Elapased:  6.930310195000004\n",
      "\n",
      "Epoch:  62\n",
      "Current Learning Rate 0.01\n",
      "Training Loss:  1.0936599\n",
      "Validation Loss:  1.1969573\n",
      "\n",
      "Time Elapased:  6.337747799999988\n",
      "\n",
      "Epoch:  63\n",
      "Current Learning Rate 0.01\n",
      "Training Loss:  1.0910994\n",
      "Validation Loss:  1.1984897\n",
      "\n",
      "Time Elapased:  6.343249513000046\n",
      "\n",
      "Epoch:  64\n",
      "Current Learning Rate 0.01\n",
      "Training Loss:  1.082052\n",
      "Validation Loss:  1.2025576\n",
      "\n",
      "Time Elapased:  6.339634589000013\n",
      "\n",
      "Epoch:  65\n",
      "Current Learning Rate 0.01\n",
      "Training Loss:  1.0772638\n",
      "Validation Loss:  1.204035\n",
      "\n",
      "Time Elapased:  6.615925517999983\n",
      "\n",
      "Epoch:  66\n",
      "Current Learning Rate 0.01\n",
      "Training Loss:  1.0735271\n",
      "Validation Loss:  1.2069702\n",
      "\n",
      "Time Elapased:  6.923991332000014\n",
      "\n",
      "Epoch:  67\n",
      "Current Learning Rate 0.01\n",
      "Training Loss:  1.0692346\n",
      "Validation Loss:  1.2057606\n",
      "\n",
      "Time Elapased:  6.775114160000044\n",
      "\n",
      "Epoch:  68\n",
      "Current Learning Rate 0.01\n",
      "Training Loss:  1.0627449\n",
      "Validation Loss:  1.2127413\n",
      "\n",
      "Time Elapased:  6.539697571999909\n",
      "\n",
      "Epoch:  69\n",
      "Current Learning Rate 0.01\n",
      "Training Loss:  1.0597756\n",
      "Validation Loss:  1.2129288\n",
      "\n",
      "Time Elapased:  6.6332228990000885\n",
      "\n",
      "Epoch:  70\n",
      "Current Learning Rate 0.01\n",
      "Training Loss:  1.0496345\n",
      "Validation Loss:  1.2140028\n",
      "Saving Checkpoint for global_step 70\n",
      "\n",
      "Time Elapased:  6.8681332080000175\n",
      "\n",
      "Epoch:  71\n",
      "Current Learning Rate 0.01\n",
      "Training Loss:  1.0459386\n",
      "Validation Loss:  1.2212454\n",
      "\n",
      "Time Elapased:  6.405749730000025\n",
      "\n",
      "Epoch:  72\n",
      "Current Learning Rate 0.01\n",
      "Training Loss:  1.039965\n",
      "Validation Loss:  1.2238646\n",
      "\n",
      "Time Elapased:  6.923887067999999\n",
      "\n",
      "Epoch:  73\n",
      "Current Learning Rate 0.01\n",
      "Training Loss:  1.0395654\n",
      "Validation Loss:  1.2244228\n",
      "\n",
      "Time Elapased:  6.434477767999965\n",
      "\n",
      "Epoch:  74\n",
      "Current Learning Rate 0.01\n",
      "Training Loss:  1.040623\n",
      "Validation Loss:  1.2262515\n",
      "\n",
      "Time Elapased:  6.467898562000073\n",
      "\n",
      "Epoch:  75\n",
      "Current Learning Rate 0.01\n",
      "Training Loss:  1.0342948\n",
      "Validation Loss:  1.2267727\n",
      "\n",
      "Time Elapased:  6.338724488999901\n",
      "\n",
      "Epoch:  76\n",
      "Current Learning Rate 0.01\n",
      "Training Loss:  1.0374513\n",
      "Validation Loss:  1.2300576\n",
      "\n",
      "Time Elapased:  6.698686840999926\n",
      "\n",
      "Epoch:  77\n",
      "Current Learning Rate 0.01\n",
      "Training Loss:  1.0541419\n",
      "Validation Loss:  1.2295526\n",
      "\n",
      "Time Elapased:  6.615290810000033\n",
      "\n",
      "Epoch:  78\n",
      "Current Learning Rate 0.01\n",
      "Training Loss:  1.0591177\n",
      "Validation Loss:  1.2287412\n",
      "\n",
      "Time Elapased:  6.877111852000098\n",
      "\n",
      "Epoch:  79\n",
      "Current Learning Rate 0.01\n",
      "Training Loss:  1.0403527\n",
      "Validation Loss:  1.2295911\n",
      "\n",
      "Time Elapased:  6.3321477939999795\n",
      "\n",
      "Epoch:  80\n",
      "Current Learning Rate 0.01\n",
      "Training Loss:  1.0404477\n",
      "Validation Loss:  1.2362826\n",
      "Saving Checkpoint for global_step 80\n",
      "\n",
      "Time Elapased:  7.0575418279998985\n",
      "\n",
      "Epoch:  81\n",
      "Current Learning Rate 0.01\n",
      "Training Loss:  1.038706\n",
      "Validation Loss:  1.2343713\n",
      "\n",
      "Time Elapased:  6.83258738699999\n",
      "\n",
      "Epoch:  82\n",
      "Current Learning Rate 0.01\n",
      "Training Loss:  1.0334535\n",
      "Validation Loss:  1.2371113\n",
      "\n",
      "Time Elapased:  7.309201679000012\n",
      "\n",
      "Epoch:  83\n",
      "Current Learning Rate 0.01\n",
      "Training Loss:  1.0309542\n",
      "Validation Loss:  1.2338573\n",
      "\n",
      "Time Elapased:  6.854116242999908\n",
      "\n",
      "Epoch:  84\n",
      "Current Learning Rate 0.01\n",
      "Training Loss:  1.0272195\n",
      "Validation Loss:  1.2372792\n",
      "\n",
      "Time Elapased:  7.4303985420000345\n",
      "\n",
      "Epoch:  85\n",
      "Current Learning Rate 0.01\n",
      "Training Loss:  1.020766\n",
      "Validation Loss:  1.242018\n",
      "\n",
      "Time Elapased:  7.58262787700005\n",
      "\n",
      "Epoch:  86\n",
      "Current Learning Rate 0.01\n",
      "Training Loss:  1.024107\n",
      "Validation Loss:  1.2346379\n",
      "\n",
      "Time Elapased:  7.345438812999987\n",
      "\n",
      "Epoch:  87\n",
      "Current Learning Rate 0.01\n",
      "Training Loss:  1.009703\n",
      "Validation Loss:  1.2362707\n",
      "\n",
      "Time Elapased:  7.709574418999978\n",
      "\n",
      "Epoch:  88\n",
      "Current Learning Rate 0.01\n",
      "Training Loss:  1.0066112\n",
      "Validation Loss:  1.2381059\n",
      "\n",
      "Time Elapased:  6.679877002000012\n",
      "\n",
      "Epoch:  89\n",
      "Current Learning Rate 0.01\n",
      "Training Loss:  1.0011127\n",
      "Validation Loss:  1.2420007\n",
      "\n",
      "Time Elapased:  7.046317922000071\n",
      "\n",
      "Epoch:  90\n",
      "Current Learning Rate 0.01\n",
      "Training Loss:  1.0009142\n",
      "Validation Loss:  1.244349\n",
      "Saving Checkpoint for global_step 90\n",
      "\n",
      "Time Elapased:  7.45836565500008\n",
      "\n",
      "Epoch:  91\n",
      "Current Learning Rate 0.01\n",
      "Training Loss:  0.9926474\n",
      "Validation Loss:  1.2426033\n",
      "\n",
      "Time Elapased:  7.508945240999992\n",
      "\n",
      "Epoch:  92\n",
      "Current Learning Rate 0.01\n",
      "Training Loss:  0.9844284\n",
      "Validation Loss:  1.2477146\n",
      "\n",
      "Time Elapased:  7.777050356000018\n",
      "\n",
      "Epoch:  93\n",
      "Current Learning Rate 0.01\n",
      "Training Loss:  0.9828689\n",
      "Validation Loss:  1.2485025\n",
      "\n",
      "Time Elapased:  7.697478756999999\n",
      "\n",
      "Epoch:  94\n",
      "Current Learning Rate 0.01\n",
      "Training Loss:  0.9818963\n",
      "Validation Loss:  1.2527436\n",
      "\n",
      "Time Elapased:  6.946676355999898\n",
      "\n",
      "Epoch:  95\n",
      "Current Learning Rate 0.01\n",
      "Training Loss:  0.97356784\n",
      "Validation Loss:  1.2526629\n",
      "\n",
      "Time Elapased:  6.437964119999947\n",
      "\n",
      "Epoch:  96\n",
      "Current Learning Rate 0.01\n",
      "Training Loss:  0.96618736\n",
      "Validation Loss:  1.2567935\n",
      "\n",
      "Time Elapased:  6.553574769999955\n",
      "\n",
      "Epoch:  97\n",
      "Current Learning Rate 0.01\n",
      "Training Loss:  0.9601515\n",
      "Validation Loss:  1.2557425\n",
      "\n",
      "Time Elapased:  7.526080675999992\n",
      "\n",
      "Epoch:  98\n",
      "Current Learning Rate 0.01\n",
      "Training Loss:  0.960018\n",
      "Validation Loss:  1.2579503\n",
      "\n",
      "Time Elapased:  6.739567542999907\n",
      "\n",
      "Epoch:  99\n",
      "Current Learning Rate 0.01\n",
      "Training Loss:  0.9605296\n",
      "Validation Loss:  1.2594979\n",
      "\n",
      "Time Elapased:  7.003080895999915\n",
      "\n",
      "Epoch:  100\n",
      "Current Learning Rate 0.01\n",
      "Training Loss:  0.95712316\n",
      "Validation Loss:  1.2596829\n",
      "Saving Checkpoint for global_step 100\n",
      "\n",
      "Time Elapased:  7.642560988000014\n",
      "\n",
      "Epoch:  101\n",
      "Current Learning Rate 0.01\n",
      "Training Loss:  0.95297736\n",
      "Validation Loss:  1.2593025\n",
      "\n",
      "Time Elapased:  7.454656056999966\n",
      "\n",
      "Epoch:  102\n",
      "Current Learning Rate 0.01\n",
      "Training Loss:  0.94762605\n",
      "Validation Loss:  1.2627349\n",
      "\n",
      "Time Elapased:  7.621045079999931\n",
      "\n",
      "Epoch:  103\n",
      "Current Learning Rate 0.01\n",
      "Training Loss:  0.94828236\n",
      "Validation Loss:  1.2604661\n",
      "\n",
      "Time Elapased:  7.221195436000016\n",
      "\n",
      "Epoch:  104\n",
      "Current Learning Rate 0.01\n",
      "Training Loss:  0.94316906\n",
      "Validation Loss:  1.2629682\n",
      "\n",
      "Time Elapased:  7.085894663999966\n",
      "\n",
      "Epoch:  105\n",
      "Current Learning Rate 0.01\n",
      "Training Loss:  0.942401\n",
      "Validation Loss:  1.2610776\n",
      "\n",
      "Time Elapased:  7.300195271000007\n",
      "\n",
      "Epoch:  106\n",
      "Current Learning Rate 0.01\n",
      "Training Loss:  0.933135\n",
      "Validation Loss:  1.2678754\n",
      "\n",
      "Time Elapased:  7.816351331000078\n",
      "\n",
      "Epoch:  107\n",
      "Current Learning Rate 0.01\n",
      "Training Loss:  0.96017575\n",
      "Validation Loss:  1.2699641\n",
      "\n",
      "Time Elapased:  6.753362431000028\n",
      "\n",
      "Epoch:  108\n",
      "Current Learning Rate 0.01\n",
      "Training Loss:  0.9567103\n",
      "Validation Loss:  1.2599348\n",
      "\n",
      "Time Elapased:  6.990177177000078\n",
      "\n",
      "Epoch:  109\n",
      "Current Learning Rate 0.01\n",
      "Training Loss:  0.9441999\n",
      "Validation Loss:  1.2645304\n",
      "\n",
      "Time Elapased:  7.531070397999997\n",
      "\n",
      "Epoch:  110\n",
      "Current Learning Rate 0.01\n",
      "Training Loss:  0.93683803\n",
      "Validation Loss:  1.2635994\n",
      "Saving Checkpoint for global_step 110\n",
      "\n",
      "Time Elapased:  8.054418794000071\n",
      "\n",
      "Epoch:  111\n",
      "Current Learning Rate 0.01\n",
      "Training Loss:  0.9308397\n",
      "Validation Loss:  1.2636073\n",
      "\n",
      "Time Elapased:  7.603700468000056\n",
      "\n",
      "Epoch:  112\n",
      "Current Learning Rate 0.01\n",
      "Training Loss:  0.92450255\n",
      "Validation Loss:  1.2695904\n",
      "\n",
      "Time Elapased:  7.637659798999948\n",
      "\n",
      "Epoch:  113\n",
      "Current Learning Rate 0.01\n",
      "Training Loss:  0.9222115\n",
      "Validation Loss:  1.2711271\n",
      "\n",
      "Time Elapased:  7.395792625000013\n",
      "\n",
      "Epoch:  114\n",
      "Current Learning Rate 0.01\n",
      "Training Loss:  0.9124625\n",
      "Validation Loss:  1.2705506\n",
      "\n",
      "Time Elapased:  7.395616589000042\n",
      "\n",
      "Epoch:  115\n",
      "Current Learning Rate 0.01\n",
      "Training Loss:  0.9114747\n",
      "Validation Loss:  1.2768797\n",
      "\n",
      "Time Elapased:  7.611214851\n",
      "\n",
      "Epoch:  116\n",
      "Current Learning Rate 0.01\n",
      "Training Loss:  0.90501297\n",
      "Validation Loss:  1.2749915\n",
      "\n",
      "Time Elapased:  7.341113714000016\n",
      "\n",
      "Epoch:  117\n",
      "Current Learning Rate 0.01\n",
      "Training Loss:  0.9054457\n",
      "Validation Loss:  1.2819222\n",
      "\n",
      "Time Elapased:  7.147877283000071\n",
      "\n",
      "Epoch:  118\n",
      "Current Learning Rate 0.01\n",
      "Training Loss:  0.9014165\n",
      "Validation Loss:  1.2824947\n",
      "\n",
      "Time Elapased:  7.001454699999954\n",
      "\n",
      "Epoch:  119\n",
      "Current Learning Rate 0.01\n",
      "Training Loss:  0.902303\n",
      "Validation Loss:  1.2852391\n",
      "\n",
      "Time Elapased:  7.2681319599998915\n",
      "\n",
      "Epoch:  120\n",
      "Current Learning Rate 0.01\n",
      "Training Loss:  0.897553\n",
      "Validation Loss:  1.2858517\n",
      "Saving Checkpoint for global_step 120\n",
      "\n",
      "Time Elapased:  7.842394625999987\n",
      "\n",
      "Epoch:  121\n",
      "Current Learning Rate 0.01\n",
      "Training Loss:  0.8889121\n",
      "Validation Loss:  1.2918884\n",
      "\n",
      "Time Elapased:  7.746756581999989\n",
      "\n",
      "Epoch:  122\n",
      "Current Learning Rate 0.01\n",
      "Training Loss:  0.881994\n",
      "Validation Loss:  1.2914906\n",
      "\n",
      "Time Elapased:  7.340225712999995\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  123\n",
      "Current Learning Rate 0.01\n",
      "Training Loss:  0.87650317\n",
      "Validation Loss:  1.2926106\n",
      "\n",
      "Time Elapased:  7.762984364999966\n",
      "\n",
      "Epoch:  124\n",
      "Current Learning Rate 0.01\n",
      "Training Loss:  0.8712439\n",
      "Validation Loss:  1.296908\n",
      "\n",
      "Time Elapased:  7.290946858000098\n",
      "\n",
      "Epoch:  125\n",
      "Current Learning Rate 0.01\n",
      "Training Loss:  0.8672327\n",
      "Validation Loss:  1.2997916\n",
      "\n",
      "Time Elapased:  7.302713418999929\n",
      "\n",
      "Epoch:  126\n",
      "Current Learning Rate 0.01\n",
      "Training Loss:  0.8657635\n",
      "Validation Loss:  1.3043119\n",
      "\n",
      "Time Elapased:  6.706080159999942\n",
      "\n",
      "Epoch:  127\n",
      "Current Learning Rate 0.01\n",
      "Training Loss:  0.85508376\n",
      "Validation Loss:  1.3080631\n",
      "\n",
      "Time Elapased:  7.4053996979999965\n",
      "\n",
      "Epoch:  128\n",
      "Current Learning Rate 0.01\n",
      "Training Loss:  0.85241455\n",
      "Validation Loss:  1.3086927\n",
      "\n",
      "Time Elapased:  6.991400166000062\n",
      "\n",
      "Epoch:  129\n",
      "Current Learning Rate 0.01\n",
      "Training Loss:  0.84587187\n",
      "Validation Loss:  1.3123738\n",
      "\n",
      "Time Elapased:  7.854033593000054\n",
      "\n",
      "Epoch:  130\n",
      "Current Learning Rate 0.01\n",
      "Training Loss:  0.8413877\n",
      "Validation Loss:  1.3132659\n",
      "Saving Checkpoint for global_step 130\n",
      "\n",
      "Time Elapased:  7.6209555149999915\n",
      "\n",
      "Epoch:  131\n",
      "Current Learning Rate 0.01\n",
      "Training Loss:  0.8397751\n",
      "Validation Loss:  1.3172452\n",
      "\n",
      "Time Elapased:  6.7625987129999885\n",
      "\n",
      "Epoch:  132\n",
      "Current Learning Rate 0.01\n",
      "Training Loss:  0.83756864\n",
      "Validation Loss:  1.3189677\n",
      "\n",
      "Time Elapased:  7.283868754000082\n",
      "\n",
      "Epoch:  133\n",
      "Current Learning Rate 0.01\n",
      "Training Loss:  0.8322861\n",
      "Validation Loss:  1.324429\n",
      "\n",
      "Time Elapased:  7.086015804999988\n",
      "\n",
      "Epoch:  134\n",
      "Current Learning Rate 0.01\n",
      "Training Loss:  0.8335488\n",
      "Validation Loss:  1.3248692\n",
      "\n",
      "Time Elapased:  7.255358056000091\n",
      "\n",
      "Epoch:  135\n",
      "Current Learning Rate 0.01\n",
      "Training Loss:  0.831964\n",
      "Validation Loss:  1.3241954\n",
      "\n",
      "Time Elapased:  6.9946829149999985\n",
      "\n",
      "Epoch:  136\n",
      "Current Learning Rate 0.01\n",
      "Training Loss:  0.8251075\n",
      "Validation Loss:  1.3298732\n",
      "\n",
      "Time Elapased:  6.300456236999935\n",
      "\n",
      "Epoch:  137\n",
      "Current Learning Rate 0.01\n",
      "Training Loss:  0.81965417\n",
      "Validation Loss:  1.3320259\n",
      "\n",
      "Time Elapased:  6.589978200000019\n",
      "\n",
      "Epoch:  138\n",
      "Current Learning Rate 0.01\n",
      "Training Loss:  0.81547964\n",
      "Validation Loss:  1.3315829\n",
      "\n",
      "Time Elapased:  7.4645627699999295\n",
      "\n",
      "Epoch:  139\n",
      "Current Learning Rate 0.01\n",
      "Training Loss:  0.81025815\n",
      "Validation Loss:  1.3384652\n",
      "\n",
      "Time Elapased:  7.07720452600006\n",
      "\n",
      "Epoch:  140\n",
      "Current Learning Rate 0.01\n",
      "Training Loss:  0.807822\n",
      "Validation Loss:  1.342291\n",
      "Saving Checkpoint for global_step 140\n",
      "\n",
      "Time Elapased:  6.7499490870000045\n",
      "\n",
      "Epoch:  141\n",
      "Current Learning Rate 0.01\n",
      "Training Loss:  0.8063015\n",
      "Validation Loss:  1.3440596\n",
      "\n",
      "Time Elapased:  6.604976858999862\n",
      "\n",
      "Epoch:  142\n",
      "Current Learning Rate 0.01\n",
      "Training Loss:  0.79763424\n",
      "Validation Loss:  1.3448424\n",
      "\n",
      "Time Elapased:  7.492308182999977\n",
      "\n",
      "Epoch:  143\n",
      "Current Learning Rate 0.01\n",
      "Training Loss:  0.7937569\n",
      "Validation Loss:  1.3474777\n",
      "\n",
      "Time Elapased:  6.574553038999966\n",
      "\n",
      "Epoch:  144\n",
      "Current Learning Rate 0.01\n",
      "Training Loss:  0.7893459\n",
      "Validation Loss:  1.3508776\n",
      "\n",
      "Time Elapased:  7.05448973700004\n",
      "\n",
      "Epoch:  145\n",
      "Current Learning Rate 0.01\n",
      "Training Loss:  0.78314877\n",
      "Validation Loss:  1.3509109\n",
      "\n",
      "Time Elapased:  7.31976356600012\n",
      "\n",
      "Epoch:  146\n",
      "Current Learning Rate 0.01\n",
      "Training Loss:  0.7808185\n",
      "Validation Loss:  1.3513303\n",
      "\n",
      "Time Elapased:  6.7409968370000115\n",
      "\n",
      "Epoch:  147\n",
      "Current Learning Rate 0.01\n",
      "Training Loss:  0.7808129\n",
      "Validation Loss:  1.3572536\n",
      "\n",
      "Time Elapased:  7.304215018999912\n",
      "\n",
      "Epoch:  148\n",
      "Current Learning Rate 0.01\n",
      "Training Loss:  0.7769292\n",
      "Validation Loss:  1.3581425\n",
      "\n",
      "Time Elapased:  6.720047738999938\n",
      "\n",
      "Epoch:  149\n",
      "Current Learning Rate 0.01\n",
      "Training Loss:  0.7694816\n",
      "Validation Loss:  1.3614233\n",
      "\n",
      "Time Elapased:  6.406396045999827\n",
      "\n",
      "Epoch:  150\n",
      "Current Learning Rate 0.01\n",
      "Training Loss:  0.76956177\n",
      "Validation Loss:  1.3630913\n",
      "Saving Checkpoint for global_step 150\n",
      "\n",
      "Time Elapased:  7.030174377999856\n",
      "\n",
      "Epoch:  151\n",
      "Current Learning Rate 0.01\n",
      "Training Loss:  0.7654625\n",
      "Validation Loss:  1.3657172\n",
      "\n",
      "Time Elapased:  6.945483356000068\n",
      "\n",
      "Epoch:  152\n",
      "Current Learning Rate 0.01\n",
      "Training Loss:  0.7658136\n",
      "Validation Loss:  1.3683653\n",
      "\n",
      "Time Elapased:  7.504743720999841\n",
      "\n",
      "Epoch:  153\n",
      "Current Learning Rate 0.01\n",
      "Training Loss:  0.757025\n",
      "Validation Loss:  1.3723172\n",
      "\n",
      "Time Elapased:  7.647296165000171\n",
      "\n",
      "Epoch:  154\n",
      "Current Learning Rate 0.01\n",
      "Training Loss:  0.7511788\n",
      "Validation Loss:  1.3744371\n",
      "\n",
      "Time Elapased:  6.734622237000167\n",
      "\n",
      "Epoch:  155\n",
      "Current Learning Rate 0.01\n",
      "Training Loss:  0.7500087\n",
      "Validation Loss:  1.3792324\n",
      "\n",
      "Time Elapased:  6.588878420000128\n",
      "\n",
      "Epoch:  156\n",
      "Current Learning Rate 0.01\n",
      "Training Loss:  0.74695826\n",
      "Validation Loss:  1.3792881\n",
      "\n",
      "Time Elapased:  6.758586910000076\n",
      "\n",
      "Epoch:  157\n",
      "Current Learning Rate 0.01\n",
      "Training Loss:  0.7468386\n",
      "Validation Loss:  1.3818326\n",
      "\n",
      "Time Elapased:  7.2313868300000195\n",
      "\n",
      "Epoch:  158\n",
      "Current Learning Rate 0.01\n",
      "Training Loss:  0.73860073\n",
      "Validation Loss:  1.3833228\n",
      "\n",
      "Time Elapased:  6.711698770000112\n",
      "\n",
      "Epoch:  159\n",
      "Current Learning Rate 0.01\n",
      "Training Loss:  0.7331669\n",
      "Validation Loss:  1.3864992\n",
      "\n",
      "Time Elapased:  6.490226874999962\n",
      "\n",
      "Epoch:  160\n",
      "Current Learning Rate 0.01\n",
      "Training Loss:  0.73015106\n",
      "Validation Loss:  1.3880818\n",
      "Saving Checkpoint for global_step 160\n",
      "\n",
      "Time Elapased:  6.758026032999851\n",
      "\n",
      "Epoch:  161\n",
      "Current Learning Rate 0.01\n",
      "Training Loss:  0.72238046\n",
      "Validation Loss:  1.3897171\n",
      "\n",
      "Time Elapased:  7.102259279000009\n",
      "\n",
      "Epoch:  162\n",
      "Current Learning Rate 0.01\n",
      "Training Loss:  0.72423553\n",
      "Validation Loss:  1.3907486\n",
      "\n",
      "Time Elapased:  6.787732814000037\n",
      "\n",
      "Epoch:  163\n",
      "Current Learning Rate 0.01\n",
      "Training Loss:  0.7178667\n",
      "Validation Loss:  1.3938458\n",
      "\n",
      "Time Elapased:  6.840533906000019\n",
      "\n",
      "Epoch:  164\n",
      "Current Learning Rate 0.01\n",
      "Training Loss:  0.7156636\n",
      "Validation Loss:  1.3975389\n",
      "\n",
      "Time Elapased:  6.347888707000038\n",
      "\n",
      "Epoch:  165\n",
      "Current Learning Rate 0.01\n",
      "Training Loss:  0.7101162\n",
      "Validation Loss:  1.4018292\n",
      "\n",
      "Time Elapased:  6.727805731999979\n",
      "\n",
      "Epoch:  166\n",
      "Current Learning Rate 0.01\n",
      "Training Loss:  0.70918506\n",
      "Validation Loss:  1.4041312\n",
      "\n",
      "Time Elapased:  6.746527653000157\n",
      "\n",
      "Epoch:  167\n",
      "Current Learning Rate 0.01\n",
      "Training Loss:  0.708341\n",
      "Validation Loss:  1.4046984\n",
      "\n",
      "Time Elapased:  6.357546232999994\n",
      "\n",
      "Epoch:  168\n",
      "Current Learning Rate 0.01\n",
      "Training Loss:  0.706091\n",
      "Validation Loss:  1.4097387\n",
      "\n",
      "Time Elapased:  7.1243324110000685\n",
      "\n",
      "Epoch:  169\n",
      "Current Learning Rate 0.01\n",
      "Training Loss:  0.700831\n",
      "Validation Loss:  1.4132721\n",
      "\n",
      "Time Elapased:  6.339140902000054\n",
      "\n",
      "Epoch:  170\n",
      "Current Learning Rate 0.01\n",
      "Training Loss:  0.6950518\n",
      "Validation Loss:  1.4159282\n",
      "Saving Checkpoint for global_step 170\n",
      "\n",
      "Time Elapased:  6.878114174000075\n",
      "\n",
      "Epoch:  171\n",
      "Current Learning Rate 0.01\n",
      "Training Loss:  0.69634414\n",
      "Validation Loss:  1.4127387\n",
      "\n",
      "Time Elapased:  6.7477411649999794\n",
      "\n",
      "Epoch:  172\n",
      "Current Learning Rate 0.01\n",
      "Training Loss:  0.7033809\n",
      "Validation Loss:  1.4181551\n",
      "\n",
      "Time Elapased:  6.746795686000041\n",
      "\n",
      "Epoch:  173\n",
      "Current Learning Rate 0.01\n",
      "Training Loss:  0.690374\n",
      "Validation Loss:  1.4224563\n",
      "\n",
      "Time Elapased:  6.879857154000092\n",
      "\n",
      "Epoch:  174\n",
      "Current Learning Rate 0.01\n",
      "Training Loss:  0.6863334\n",
      "Validation Loss:  1.4259303\n",
      "\n",
      "Time Elapased:  6.373059319000049\n",
      "\n",
      "Epoch:  175\n",
      "Current Learning Rate 0.01\n",
      "Training Loss:  0.68673855\n",
      "Validation Loss:  1.426838\n",
      "\n",
      "Time Elapased:  6.368280968999898\n",
      "\n",
      "Epoch:  176\n",
      "Current Learning Rate 0.01\n",
      "Training Loss:  0.6769437\n",
      "Validation Loss:  1.4310685\n",
      "\n",
      "Time Elapased:  6.518211332999954\n",
      "\n",
      "Epoch:  177\n",
      "Current Learning Rate 0.01\n",
      "Training Loss:  0.6698014\n",
      "Validation Loss:  1.4310508\n",
      "\n",
      "Time Elapased:  6.940791667999974\n",
      "\n",
      "Epoch:  178\n",
      "Current Learning Rate 0.01\n",
      "Training Loss:  0.6669153\n",
      "Validation Loss:  1.4345317\n",
      "\n",
      "Time Elapased:  6.7387454940001135\n",
      "\n",
      "Epoch:  179\n",
      "Current Learning Rate 0.01\n",
      "Training Loss:  0.6688901\n",
      "Validation Loss:  1.4387097\n",
      "\n",
      "Time Elapased:  6.429600705999974\n",
      "\n",
      "Epoch:  180\n",
      "Current Learning Rate 0.01\n",
      "Training Loss:  0.66240263\n",
      "Validation Loss:  1.4382187\n",
      "Saving Checkpoint for global_step 180\n",
      "\n",
      "Time Elapased:  7.271766863000039\n",
      "\n",
      "Epoch:  181\n",
      "Current Learning Rate 0.01\n",
      "Training Loss:  0.66057867\n",
      "Validation Loss:  1.4425837\n",
      "\n",
      "Time Elapased:  6.498633569000049\n",
      "\n",
      "Epoch:  182\n",
      "Current Learning Rate 0.01\n",
      "Training Loss:  0.6601188\n",
      "Validation Loss:  1.4434403\n",
      "\n",
      "Time Elapased:  6.329119038000044\n",
      "\n",
      "Epoch:  183\n",
      "Current Learning Rate 0.01\n",
      "Training Loss:  0.6595508\n",
      "Validation Loss:  1.4489119\n",
      "\n",
      "Time Elapased:  6.411682816000166\n",
      "\n",
      "Epoch:  184\n",
      "Current Learning Rate 0.01\n",
      "Training Loss:  0.65478724\n",
      "Validation Loss:  1.4476645\n",
      "\n",
      "Time Elapased:  6.932250716999988\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  185\n",
      "Current Learning Rate 0.01\n",
      "Training Loss:  0.6511092\n",
      "Validation Loss:  1.4536341\n",
      "\n",
      "Time Elapased:  6.337838819999888\n",
      "\n",
      "Epoch:  186\n",
      "Current Learning Rate 0.01\n",
      "Training Loss:  0.64833826\n",
      "Validation Loss:  1.4531169\n",
      "\n",
      "Time Elapased:  6.531160941000053\n",
      "\n",
      "Epoch:  187\n",
      "Current Learning Rate 0.01\n",
      "Training Loss:  0.65348333\n",
      "Validation Loss:  1.4548297\n",
      "\n",
      "Time Elapased:  6.95763916299984\n",
      "\n",
      "Epoch:  188\n",
      "Current Learning Rate 0.01\n",
      "Training Loss:  0.64929307\n",
      "Validation Loss:  1.4576324\n",
      "\n",
      "Time Elapased:  6.542661539999926\n",
      "\n",
      "Epoch:  189\n",
      "Current Learning Rate 0.01\n",
      "Training Loss:  0.6521449\n",
      "Validation Loss:  1.4631648\n",
      "\n",
      "Time Elapased:  6.588754882999865\n",
      "\n",
      "Epoch:  190\n",
      "Current Learning Rate 0.01\n",
      "Training Loss:  0.6426014\n",
      "Validation Loss:  1.4630675\n",
      "Saving Checkpoint for global_step 190\n",
      "\n",
      "Time Elapased:  7.345151144999818\n",
      "\n",
      "Epoch:  191\n",
      "Current Learning Rate 0.01\n",
      "Training Loss:  0.6332511\n",
      "Validation Loss:  1.46661\n",
      "\n",
      "Time Elapased:  6.369020078999938\n",
      "\n",
      "Epoch:  192\n",
      "Current Learning Rate 0.01\n",
      "Training Loss:  0.6314795\n",
      "Validation Loss:  1.4675626\n",
      "\n",
      "Time Elapased:  6.934656469999936\n",
      "\n",
      "Epoch:  193\n",
      "Current Learning Rate 0.01\n",
      "Training Loss:  0.6304457\n",
      "Validation Loss:  1.4713215\n",
      "\n",
      "Time Elapased:  6.372741454999868\n",
      "\n",
      "Epoch:  194\n",
      "Current Learning Rate 0.01\n",
      "Training Loss:  0.6282987\n",
      "Validation Loss:  1.4699961\n",
      "\n",
      "Time Elapased:  6.330877611000005\n",
      "\n",
      "Epoch:  195\n",
      "Current Learning Rate 0.01\n",
      "Training Loss:  0.62157875\n",
      "Validation Loss:  1.4744866\n",
      "\n",
      "Time Elapased:  6.578164773000026\n",
      "\n",
      "Epoch:  196\n",
      "Current Learning Rate 0.01\n",
      "Training Loss:  0.61627865\n",
      "Validation Loss:  1.4781371\n",
      "\n",
      "Time Elapased:  6.589207088999956\n",
      "\n",
      "Epoch:  197\n",
      "Current Learning Rate 0.01\n",
      "Training Loss:  0.61521584\n",
      "Validation Loss:  1.4778144\n",
      "\n",
      "Time Elapased:  6.428123644999914\n",
      "\n",
      "Epoch:  198\n",
      "Current Learning Rate 0.01\n",
      "Training Loss:  0.61504686\n",
      "Validation Loss:  1.4811842\n",
      "\n",
      "Time Elapased:  6.336111460999973\n",
      "\n",
      "Epoch:  199\n",
      "Current Learning Rate 0.01\n",
      "Training Loss:  0.6108893\n",
      "Validation Loss:  1.4807181\n",
      "\n",
      "Time Elapased:  6.320088108999926\n",
      "\n",
      "Epoch:  200\n",
      "Current Learning Rate 0.01\n",
      "Training Loss:  0.60929525\n",
      "Validation Loss:  1.4853009\n",
      "Saving Checkpoint for global_step 200\n",
      "\n",
      "Time Elapased:  6.712225318000037\n",
      "\n",
      "Epoch:  201\n",
      "Current Learning Rate 0.01\n",
      "Training Loss:  0.605723\n",
      "Validation Loss:  1.490171\n",
      "\n",
      "Time Elapased:  6.31286190600008\n",
      "\n",
      "Epoch:  202\n",
      "Current Learning Rate 0.01\n",
      "Training Loss:  0.6050001\n",
      "Validation Loss:  1.4892944\n",
      "\n",
      "Time Elapased:  6.392911558999913\n",
      "\n",
      "Epoch:  203\n",
      "Current Learning Rate 0.01\n",
      "Training Loss:  0.60925573\n",
      "Validation Loss:  1.4947771\n",
      "\n",
      "Time Elapased:  6.4928252140000495\n",
      "\n",
      "Epoch:  204\n",
      "Current Learning Rate 0.01\n",
      "Training Loss:  0.60996616\n",
      "Validation Loss:  1.4901514\n",
      "\n",
      "Time Elapased:  6.657933624000179\n",
      "\n",
      "Epoch:  205\n",
      "Current Learning Rate 0.01\n",
      "Training Loss:  0.6109966\n",
      "Validation Loss:  1.4982374\n",
      "\n",
      "Time Elapased:  6.357286579999936\n",
      "\n",
      "Epoch:  206\n",
      "Current Learning Rate 0.01\n",
      "Training Loss:  0.61050236\n",
      "Validation Loss:  1.5015901\n",
      "\n",
      "Time Elapased:  6.773962142999835\n",
      "\n",
      "Epoch:  207\n",
      "Current Learning Rate 0.01\n",
      "Training Loss:  0.60648847\n",
      "Validation Loss:  1.4980115\n",
      "\n",
      "Time Elapased:  6.957243167000115\n",
      "\n",
      "Epoch:  208\n",
      "Current Learning Rate 0.01\n",
      "Training Loss:  0.62997127\n",
      "Validation Loss:  1.5013435\n",
      "\n",
      "Time Elapased:  6.734626124999977\n",
      "\n",
      "Epoch:  209\n",
      "Current Learning Rate 0.01\n",
      "Training Loss:  0.6250614\n",
      "Validation Loss:  1.5049304\n",
      "\n",
      "Time Elapased:  6.301468489999934\n",
      "\n",
      "Epoch:  210\n",
      "Current Learning Rate 0.01\n",
      "Training Loss:  0.61731017\n",
      "Validation Loss:  1.5115609\n",
      "Saving Checkpoint for global_step 210\n",
      "\n",
      "Time Elapased:  7.438810383999908\n",
      "\n",
      "Epoch:  211\n",
      "Current Learning Rate 0.01\n",
      "Training Loss:  0.61059505\n",
      "Validation Loss:  1.5093218\n",
      "\n",
      "Time Elapased:  6.555389095999999\n",
      "\n",
      "Epoch:  212\n",
      "Current Learning Rate 0.01\n",
      "Training Loss:  0.60061014\n",
      "Validation Loss:  1.5140829\n",
      "\n",
      "Time Elapased:  6.823058311000068\n",
      "\n",
      "Epoch:  213\n",
      "Current Learning Rate 0.01\n",
      "Training Loss:  0.59434867\n",
      "Validation Loss:  1.5153453\n",
      "\n",
      "Time Elapased:  6.918725014000074\n",
      "\n",
      "Epoch:  214\n",
      "Current Learning Rate 0.01\n",
      "Training Loss:  0.5885238\n",
      "Validation Loss:  1.5126073\n",
      "\n",
      "Time Elapased:  7.434241175000125\n",
      "\n",
      "Epoch:  215\n",
      "Current Learning Rate 0.01\n",
      "Training Loss:  0.58191\n",
      "Validation Loss:  1.5195155\n",
      "\n",
      "Time Elapased:  7.703725853000151\n",
      "\n",
      "Epoch:  216\n",
      "Current Learning Rate 0.01\n",
      "Training Loss:  0.57835907\n",
      "Validation Loss:  1.5222301\n",
      "\n",
      "Time Elapased:  7.456870209000044\n",
      "\n",
      "Epoch:  217\n",
      "Current Learning Rate 0.01\n",
      "Training Loss:  0.5723981\n",
      "Validation Loss:  1.5260223\n",
      "\n",
      "Time Elapased:  6.587653519000014\n",
      "\n",
      "Epoch:  218\n",
      "Current Learning Rate 0.01\n",
      "Training Loss:  0.5676101\n",
      "Validation Loss:  1.5276098\n",
      "\n",
      "Time Elapased:  7.611155522999979\n",
      "\n",
      "Epoch:  219\n",
      "Current Learning Rate 0.01\n",
      "Training Loss:  0.56208605\n",
      "Validation Loss:  1.5270631\n",
      "\n",
      "Time Elapased:  6.598791974000051\n",
      "\n",
      "Epoch:  220\n",
      "Current Learning Rate 0.01\n",
      "Training Loss:  0.56141394\n",
      "Validation Loss:  1.5307343\n",
      "Saving Checkpoint for global_step 220\n",
      "\n",
      "Time Elapased:  8.058824565000123\n",
      "\n",
      "Epoch:  221\n",
      "Current Learning Rate 0.01\n",
      "Training Loss:  0.55856144\n",
      "Validation Loss:  1.5322443\n",
      "\n",
      "Time Elapased:  7.252776892000156\n",
      "\n",
      "Epoch:  222\n",
      "Current Learning Rate 0.01\n",
      "Training Loss:  0.55568254\n",
      "Validation Loss:  1.5317005\n",
      "\n",
      "Time Elapased:  7.811584684000081\n",
      "\n",
      "Epoch:  223\n",
      "Current Learning Rate 0.01\n",
      "Training Loss:  0.5560962\n",
      "Validation Loss:  1.5388151\n",
      "\n",
      "Time Elapased:  7.646458750999955\n",
      "\n",
      "Epoch:  224\n",
      "Current Learning Rate 0.01\n",
      "Training Loss:  0.5515508\n",
      "Validation Loss:  1.5389014\n",
      "\n",
      "Time Elapased:  7.577843178999956\n",
      "\n",
      "Epoch:  225\n",
      "Current Learning Rate 0.01\n",
      "Training Loss:  0.5491732\n",
      "Validation Loss:  1.5435662\n",
      "\n",
      "Time Elapased:  7.155780721999918\n",
      "\n",
      "Epoch:  226\n",
      "Current Learning Rate 0.01\n",
      "Training Loss:  0.54547226\n",
      "Validation Loss:  1.5426679\n",
      "\n",
      "Time Elapased:  6.701229149000028\n",
      "\n",
      "Epoch:  227\n",
      "Current Learning Rate 0.01\n",
      "Training Loss:  0.5400606\n",
      "Validation Loss:  1.5431501\n",
      "\n",
      "Time Elapased:  6.937014123999916\n",
      "\n",
      "Epoch:  228\n",
      "Current Learning Rate 0.01\n",
      "Training Loss:  0.53822273\n",
      "Validation Loss:  1.547089\n",
      "\n",
      "Time Elapased:  6.597767062999992\n",
      "\n",
      "Epoch:  229\n",
      "Current Learning Rate 0.01\n",
      "Training Loss:  0.5355421\n",
      "Validation Loss:  1.5493305\n",
      "\n",
      "Time Elapased:  6.789440495000008\n",
      "\n",
      "Epoch:  230\n",
      "Current Learning Rate 0.01\n",
      "Training Loss:  0.53670835\n",
      "Validation Loss:  1.5511036\n",
      "Saving Checkpoint for global_step 230\n",
      "\n",
      "Time Elapased:  6.862865652999972\n",
      "\n",
      "Epoch:  231\n",
      "Current Learning Rate 0.01\n",
      "Training Loss:  0.5362082\n",
      "Validation Loss:  1.5552113\n",
      "\n",
      "Time Elapased:  6.335691341000029\n",
      "\n",
      "Epoch:  232\n",
      "Current Learning Rate 0.01\n",
      "Training Loss:  0.5348347\n",
      "Validation Loss:  1.556247\n",
      "\n",
      "Time Elapased:  6.723106532999964\n",
      "\n",
      "Epoch:  233\n",
      "Current Learning Rate 0.01\n",
      "Training Loss:  0.5384892\n",
      "Validation Loss:  1.557305\n",
      "\n",
      "Time Elapased:  6.6968540279999615\n",
      "\n",
      "Epoch:  234\n",
      "Current Learning Rate 0.01\n",
      "Training Loss:  0.5443395\n",
      "Validation Loss:  1.554944\n",
      "\n",
      "Time Elapased:  6.93917093999994\n",
      "\n",
      "Epoch:  235\n",
      "Current Learning Rate 0.01\n",
      "Training Loss:  0.54642844\n",
      "Validation Loss:  1.5633198\n",
      "\n",
      "Time Elapased:  6.548105892999956\n",
      "\n",
      "Epoch:  236\n",
      "Current Learning Rate 0.01\n",
      "Training Loss:  0.5354482\n",
      "Validation Loss:  1.5623246\n",
      "\n",
      "Time Elapased:  6.52554447600005\n",
      "\n",
      "Epoch:  237\n",
      "Current Learning Rate 0.01\n",
      "Training Loss:  0.5253607\n",
      "Validation Loss:  1.5663956\n",
      "\n",
      "Time Elapased:  6.526069128000017\n",
      "\n",
      "Epoch:  238\n",
      "Current Learning Rate 0.01\n",
      "Training Loss:  0.52089363\n",
      "Validation Loss:  1.5657817\n",
      "\n",
      "Time Elapased:  6.927309648000119\n",
      "\n",
      "Epoch:  239\n",
      "Current Learning Rate 0.01\n",
      "Training Loss:  0.52069867\n",
      "Validation Loss:  1.5712756\n",
      "\n",
      "Time Elapased:  6.313750386000038\n",
      "\n",
      "Epoch:  240\n",
      "Current Learning Rate 0.01\n",
      "Training Loss:  0.5172578\n",
      "Validation Loss:  1.5695759\n",
      "Saving Checkpoint for global_step 240\n",
      "\n",
      "Time Elapased:  6.865823054999964\n",
      "\n",
      "Epoch:  241\n",
      "Current Learning Rate 0.01\n",
      "Training Loss:  0.52291226\n",
      "Validation Loss:  1.5744705\n",
      "\n",
      "Time Elapased:  6.593345123000063\n",
      "\n",
      "Epoch:  242\n",
      "Current Learning Rate 0.01\n",
      "Training Loss:  0.52008855\n",
      "Validation Loss:  1.5740937\n",
      "\n",
      "Time Elapased:  6.334412308000083\n",
      "\n",
      "Epoch:  243\n",
      "Current Learning Rate 0.01\n",
      "Training Loss:  0.52126753\n",
      "Validation Loss:  1.5797452\n",
      "\n",
      "Time Elapased:  7.016035942000144\n",
      "\n",
      "Epoch:  244\n",
      "Current Learning Rate 0.01\n",
      "Training Loss:  0.5277505\n",
      "Validation Loss:  1.5777043\n",
      "\n",
      "Time Elapased:  6.347161513000174\n",
      "\n",
      "Epoch:  245\n",
      "Current Learning Rate 0.01\n",
      "Training Loss:  0.5297171\n",
      "Validation Loss:  1.5811203\n",
      "\n",
      "Time Elapased:  6.767334410999865\n",
      "\n",
      "Epoch:  246\n",
      "Current Learning Rate 0.01\n",
      "Training Loss:  0.5427867\n",
      "Validation Loss:  1.578471\n",
      "\n",
      "Time Elapased:  6.750056438999991\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  247\n",
      "Current Learning Rate 0.01\n",
      "Training Loss:  0.5478114\n",
      "Validation Loss:  1.5814822\n",
      "\n",
      "Time Elapased:  6.345643493999887\n",
      "\n",
      "Epoch:  248\n",
      "Current Learning Rate 0.01\n",
      "Training Loss:  0.56025994\n",
      "Validation Loss:  1.5846112\n",
      "\n",
      "Time Elapased:  6.855654988999959\n",
      "\n",
      "Epoch:  249\n",
      "Current Learning Rate 0.01\n",
      "Training Loss:  0.55289996\n",
      "Validation Loss:  1.5822861\n",
      "\n",
      "Time Elapased:  6.328657638000095\n",
      "\n",
      "Epoch:  250\n",
      "Current Learning Rate 0.01\n",
      "Training Loss:  0.5496601\n",
      "Validation Loss:  1.5849537\n",
      "Saving Checkpoint for global_step 250\n",
      "\n",
      "Time Elapased:  6.711276946999988\n",
      "\n",
      "Epoch:  251\n",
      "Current Learning Rate 0.01\n",
      "Training Loss:  0.54043865\n",
      "Validation Loss:  1.5900167\n",
      "\n",
      "Time Elapased:  6.333158967999907\n",
      "\n",
      "Epoch:  252\n",
      "Current Learning Rate 0.01\n",
      "Training Loss:  0.53372824\n",
      "Validation Loss:  1.5943204\n",
      "\n",
      "Time Elapased:  6.340369767999846\n",
      "\n",
      "Epoch:  253\n",
      "Current Learning Rate 0.01\n",
      "Training Loss:  0.52508706\n",
      "Validation Loss:  1.5956615\n",
      "\n",
      "Time Elapased:  6.331605761999981\n",
      "\n",
      "Epoch:  254\n",
      "Current Learning Rate 0.01\n",
      "Training Loss:  0.5200103\n",
      "Validation Loss:  1.5961584\n",
      "\n",
      "Time Elapased:  6.329649238999991\n",
      "\n",
      "Epoch:  255\n",
      "Current Learning Rate 0.01\n",
      "Training Loss:  0.50923955\n",
      "Validation Loss:  1.6036913\n",
      "\n",
      "Time Elapased:  6.3154429730000174\n",
      "\n",
      "Epoch:  256\n",
      "Current Learning Rate 0.01\n",
      "Training Loss:  0.50205564\n",
      "Validation Loss:  1.600441\n",
      "\n",
      "Time Elapased:  6.329216469999892\n",
      "\n",
      "Epoch:  257\n",
      "Current Learning Rate 0.01\n",
      "Training Loss:  0.49820337\n",
      "Validation Loss:  1.6053702\n",
      "\n",
      "Time Elapased:  6.617316355999947\n",
      "\n",
      "Epoch:  258\n",
      "Current Learning Rate 0.01\n",
      "Training Loss:  0.4946923\n",
      "Validation Loss:  1.6093065\n",
      "\n",
      "Time Elapased:  6.4113293240000075\n",
      "\n",
      "Epoch:  259\n",
      "Current Learning Rate 0.01\n",
      "Training Loss:  0.48904097\n",
      "Validation Loss:  1.6065812\n",
      "\n",
      "Time Elapased:  7.108613814999899\n",
      "\n",
      "Epoch:  260\n",
      "Current Learning Rate 0.01\n",
      "Training Loss:  0.48806563\n",
      "Validation Loss:  1.6131335\n",
      "Saving Checkpoint for global_step 260\n",
      "\n",
      "Time Elapased:  7.062556981999933\n",
      "\n",
      "Epoch:  261\n",
      "Current Learning Rate 0.01\n",
      "Training Loss:  0.4835187\n",
      "Validation Loss:  1.611182\n",
      "\n",
      "Time Elapased:  6.338251424999953\n",
      "\n",
      "Epoch:  262\n",
      "Current Learning Rate 0.01\n",
      "Training Loss:  0.4783234\n",
      "Validation Loss:  1.6124818\n",
      "\n",
      "Time Elapased:  6.33615490599982\n",
      "\n",
      "Epoch:  263\n",
      "Current Learning Rate 0.01\n",
      "Training Loss:  0.47632775\n",
      "Validation Loss:  1.6181358\n",
      "\n",
      "Time Elapased:  6.811946962000093\n",
      "\n",
      "Epoch:  264\n",
      "Current Learning Rate 0.01\n",
      "Training Loss:  0.47218215\n",
      "Validation Loss:  1.6161941\n",
      "\n",
      "Time Elapased:  6.9856488909999825\n",
      "\n",
      "Epoch:  265\n",
      "Current Learning Rate 0.01\n",
      "Training Loss:  0.4721041\n",
      "Validation Loss:  1.6192143\n",
      "\n",
      "Time Elapased:  6.728905643000189\n",
      "\n",
      "Epoch:  266\n",
      "Current Learning Rate 0.01\n",
      "Training Loss:  0.47115213\n",
      "Validation Loss:  1.6215663\n",
      "\n",
      "Time Elapased:  6.634066004000033\n",
      "\n",
      "Epoch:  267\n",
      "Current Learning Rate 0.01\n",
      "Training Loss:  0.47110704\n",
      "Validation Loss:  1.6255596\n",
      "\n",
      "Time Elapased:  6.419694423999999\n",
      "\n",
      "Epoch:  268\n",
      "Current Learning Rate 0.01\n",
      "Training Loss:  0.47148216\n",
      "Validation Loss:  1.6263106\n",
      "\n",
      "Time Elapased:  7.45980326799986\n",
      "\n",
      "Epoch:  269\n",
      "Current Learning Rate 0.01\n",
      "Training Loss:  0.46870363\n",
      "Validation Loss:  1.6302385\n",
      "\n",
      "Time Elapased:  7.350632225000027\n",
      "\n",
      "Epoch:  270\n",
      "Current Learning Rate 0.01\n",
      "Training Loss:  0.4680733\n",
      "Validation Loss:  1.627393\n",
      "Saving Checkpoint for global_step 270\n",
      "\n",
      "Time Elapased:  6.868493766000029\n",
      "\n",
      "Epoch:  271\n",
      "Current Learning Rate 0.01\n",
      "Training Loss:  0.46359408\n",
      "Validation Loss:  1.6321899\n",
      "\n",
      "Time Elapased:  6.547352556000078\n",
      "\n",
      "Epoch:  272\n",
      "Current Learning Rate 0.01\n",
      "Training Loss:  0.47500092\n",
      "Validation Loss:  1.6306423\n",
      "\n",
      "Time Elapased:  6.82857081599991\n",
      "\n",
      "Epoch:  273\n",
      "Current Learning Rate 0.01\n",
      "Training Loss:  0.4721554\n",
      "Validation Loss:  1.637162\n",
      "\n",
      "Time Elapased:  7.546312292999801\n",
      "\n",
      "Epoch:  274\n",
      "Current Learning Rate 0.01\n",
      "Training Loss:  0.47480303\n",
      "Validation Loss:  1.6362162\n",
      "\n",
      "Time Elapased:  7.539055834999999\n",
      "\n",
      "Epoch:  275\n",
      "Current Learning Rate 0.01\n",
      "Training Loss:  0.46821666\n",
      "Validation Loss:  1.6408665\n",
      "\n",
      "Time Elapased:  6.91471419100003\n",
      "\n",
      "Epoch:  276\n",
      "Current Learning Rate 0.01\n",
      "Training Loss:  0.46619588\n",
      "Validation Loss:  1.6430502\n",
      "\n",
      "Time Elapased:  6.761489426000026\n",
      "\n",
      "Epoch:  277\n",
      "Current Learning Rate 0.01\n",
      "Training Loss:  0.46563962\n",
      "Validation Loss:  1.641475\n",
      "\n",
      "Time Elapased:  7.5759533540001485\n",
      "\n",
      "Epoch:  278\n",
      "Current Learning Rate 0.01\n",
      "Training Loss:  0.46716058\n",
      "Validation Loss:  1.6459632\n",
      "\n",
      "Time Elapased:  7.029652574000011\n",
      "\n",
      "Epoch:  279\n",
      "Current Learning Rate 0.01\n",
      "Training Loss:  0.468552\n",
      "Validation Loss:  1.6468594\n",
      "\n",
      "Time Elapased:  6.932586241000081\n",
      "\n",
      "Epoch:  280\n",
      "Current Learning Rate 0.01\n",
      "Training Loss:  0.46462142\n",
      "Validation Loss:  1.6450809\n",
      "Saving Checkpoint for global_step 280\n",
      "\n",
      "Time Elapased:  7.434187863000034\n",
      "\n",
      "Epoch:  281\n",
      "Current Learning Rate 0.01\n",
      "Training Loss:  0.46267563\n",
      "Validation Loss:  1.6455542\n",
      "\n",
      "Time Elapased:  6.430976747999921\n",
      "\n",
      "Epoch:  282\n",
      "Current Learning Rate 0.01\n",
      "Training Loss:  0.46648684\n",
      "Validation Loss:  1.6549269\n",
      "\n",
      "Time Elapased:  6.665889590999996\n",
      "\n",
      "Epoch:  283\n",
      "Current Learning Rate 0.01\n",
      "Training Loss:  0.4691574\n",
      "Validation Loss:  1.6489817\n",
      "\n",
      "Time Elapased:  6.339122355999962\n",
      "\n",
      "Epoch:  284\n",
      "Current Learning Rate 0.01\n",
      "Training Loss:  0.46928254\n",
      "Validation Loss:  1.6529876\n",
      "\n",
      "Time Elapased:  6.813634964999892\n",
      "\n",
      "Epoch:  285\n",
      "Current Learning Rate 0.01\n",
      "Training Loss:  0.45878333\n",
      "Validation Loss:  1.6551981\n",
      "\n",
      "Time Elapased:  6.573362295999914\n",
      "\n",
      "Epoch:  286\n",
      "Current Learning Rate 0.01\n",
      "Training Loss:  0.45121098\n",
      "Validation Loss:  1.6601969\n",
      "\n",
      "Time Elapased:  6.679039590000002\n",
      "\n",
      "Epoch:  287\n",
      "Current Learning Rate 0.01\n",
      "Training Loss:  0.45188946\n",
      "Validation Loss:  1.6601851\n",
      "\n",
      "Time Elapased:  6.494826783999997\n",
      "\n",
      "Epoch:  288\n",
      "Current Learning Rate 0.01\n",
      "Training Loss:  0.44856662\n",
      "Validation Loss:  1.6553218\n",
      "\n",
      "Time Elapased:  6.678173604999984\n",
      "\n",
      "Epoch:  289\n",
      "Current Learning Rate 0.01\n",
      "Training Loss:  0.44177923\n",
      "Validation Loss:  1.6672897\n",
      "\n",
      "Time Elapased:  6.326057339999807\n",
      "\n",
      "Epoch:  290\n",
      "Current Learning Rate 0.01\n",
      "Training Loss:  0.43843248\n",
      "Validation Loss:  1.6675942\n",
      "Saving Checkpoint for global_step 290\n",
      "\n",
      "Time Elapased:  6.6509127180002\n",
      "\n",
      "Epoch:  291\n",
      "Current Learning Rate 0.01\n",
      "Training Loss:  0.43447447\n",
      "Validation Loss:  1.6642147\n",
      "\n",
      "Time Elapased:  6.328029226000126\n",
      "\n",
      "Epoch:  292\n",
      "Current Learning Rate 0.01\n",
      "Training Loss:  0.43215668\n",
      "Validation Loss:  1.6706214\n",
      "\n",
      "Time Elapased:  6.332873237999593\n",
      "\n",
      "Epoch:  293\n",
      "Current Learning Rate 0.01\n",
      "Training Loss:  0.4312308\n",
      "Validation Loss:  1.6707898\n",
      "\n",
      "Time Elapased:  6.318288181000298\n",
      "\n",
      "Epoch:  294\n",
      "Current Learning Rate 0.01\n",
      "Training Loss:  0.43229836\n",
      "Validation Loss:  1.672454\n",
      "\n",
      "Time Elapased:  6.3268642040002305\n",
      "\n",
      "Epoch:  295\n",
      "Current Learning Rate 0.01\n",
      "Training Loss:  0.42885467\n",
      "Validation Loss:  1.6760445\n",
      "\n",
      "Time Elapased:  6.360658353000417\n",
      "\n",
      "Epoch:  296\n",
      "Current Learning Rate 0.01\n",
      "Training Loss:  0.42855543\n",
      "Validation Loss:  1.6761189\n",
      "\n",
      "Time Elapased:  6.493705158000012\n",
      "\n",
      "Epoch:  297\n",
      "Current Learning Rate 0.01\n",
      "Training Loss:  0.42934078\n",
      "Validation Loss:  1.6787825\n",
      "\n",
      "Time Elapased:  6.427521940000133\n",
      "\n",
      "Epoch:  298\n",
      "Current Learning Rate 0.01\n",
      "Training Loss:  0.42702585\n",
      "Validation Loss:  1.6800052\n",
      "\n",
      "Time Elapased:  6.390749465000226\n",
      "\n",
      "Epoch:  299\n",
      "Current Learning Rate 0.01\n",
      "Training Loss:  0.42766303\n",
      "Validation Loss:  1.6829779\n",
      "\n",
      "Time Elapased:  6.4308194670002194\n",
      "\n",
      "Epoch:  300\n",
      "Current Learning Rate 0.01\n",
      "Training Loss:  0.4300711\n",
      "Validation Loss:  1.6825917\n",
      "Saving Checkpoint for global_step 300\n",
      "\n",
      "Time Elapased:  6.678284990000066\n",
      "\n",
      "Epoch:  301\n",
      "Current Learning Rate 0.01\n",
      "Training Loss:  0.42896932\n",
      "Validation Loss:  1.6871018\n",
      "\n",
      "Time Elapased:  6.321666048000225\n",
      "\n",
      "Epoch:  302\n",
      "Current Learning Rate 0.01\n",
      "Training Loss:  0.43337017\n",
      "Validation Loss:  1.6829054\n",
      "\n",
      "Time Elapased:  6.416430466000293\n",
      "\n",
      "Epoch:  303\n",
      "Current Learning Rate 0.01\n",
      "Training Loss:  0.42786303\n",
      "Validation Loss:  1.6854411\n",
      "\n",
      "Time Elapased:  6.39243104299976\n",
      "\n",
      "Epoch:  304\n",
      "Current Learning Rate 0.01\n",
      "Training Loss:  0.42262277\n",
      "Validation Loss:  1.6860739\n",
      "\n",
      "Time Elapased:  6.332649193999714\n",
      "\n",
      "Epoch:  305\n",
      "Current Learning Rate 0.01\n",
      "Training Loss:  0.42205295\n",
      "Validation Loss:  1.6896076\n",
      "\n",
      "Time Elapased:  6.3502389939999375\n",
      "\n",
      "Epoch:  306\n",
      "Current Learning Rate 0.01\n",
      "Training Loss:  0.43305945\n",
      "Validation Loss:  1.688144\n",
      "\n",
      "Time Elapased:  6.321004908000305\n",
      "\n",
      "Epoch:  307\n",
      "Current Learning Rate 0.01\n",
      "Training Loss:  0.4484784\n",
      "Validation Loss:  1.694845\n",
      "\n",
      "Time Elapased:  6.329449185000158\n",
      "\n",
      "Epoch:  308\n",
      "Current Learning Rate 0.01\n",
      "Training Loss:  0.4439166\n",
      "Validation Loss:  1.691973\n",
      "\n",
      "Time Elapased:  6.337812938999832\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  309\n",
      "Current Learning Rate 0.01\n",
      "Training Loss:  0.4492093\n",
      "Validation Loss:  1.6936958\n",
      "\n",
      "Time Elapased:  6.619429752000087\n",
      "\n",
      "Epoch:  310\n",
      "Current Learning Rate 0.01\n",
      "Training Loss:  0.45018148\n",
      "Validation Loss:  1.6929733\n",
      "Saving Checkpoint for global_step 310\n",
      "\n",
      "Time Elapased:  7.163086885999746\n",
      "\n",
      "Epoch:  311\n",
      "Current Learning Rate 0.01\n",
      "Training Loss:  0.45959467\n",
      "Validation Loss:  1.6930547\n",
      "\n",
      "Time Elapased:  6.558638301999963\n",
      "\n",
      "Epoch:  312\n",
      "Current Learning Rate 0.01\n",
      "Training Loss:  0.45406377\n",
      "Validation Loss:  1.697073\n",
      "\n",
      "Time Elapased:  6.3384304209998845\n",
      "\n",
      "Epoch:  313\n",
      "Current Learning Rate 0.01\n",
      "Training Loss:  0.45014104\n",
      "Validation Loss:  1.7008517\n",
      "\n",
      "Time Elapased:  6.544450810999933\n",
      "\n",
      "Epoch:  314\n",
      "Current Learning Rate 0.01\n",
      "Training Loss:  0.45223546\n",
      "Validation Loss:  1.6987327\n",
      "\n",
      "Time Elapased:  6.4339932510001745\n",
      "\n",
      "Epoch:  315\n",
      "Current Learning Rate 0.01\n",
      "Training Loss:  0.44597846\n",
      "Validation Loss:  1.6999589\n",
      "\n",
      "Time Elapased:  6.349566921000132\n",
      "\n",
      "Epoch:  316\n",
      "Current Learning Rate 0.01\n",
      "Training Loss:  0.43929988\n",
      "Validation Loss:  1.7078067\n",
      "\n",
      "Time Elapased:  6.730155976999868\n",
      "\n",
      "Epoch:  317\n",
      "Current Learning Rate 0.01\n",
      "Training Loss:  0.4382346\n",
      "Validation Loss:  1.7045491\n",
      "\n",
      "Time Elapased:  6.565464414999951\n",
      "\n",
      "Epoch:  318\n",
      "Current Learning Rate 0.01\n",
      "Training Loss:  0.43616864\n",
      "Validation Loss:  1.7039837\n",
      "\n",
      "Time Elapased:  6.853147490000083\n",
      "\n",
      "Epoch:  319\n",
      "Current Learning Rate 0.01\n",
      "Training Loss:  0.43517143\n",
      "Validation Loss:  1.7078437\n",
      "\n",
      "Time Elapased:  6.773729957999876\n",
      "\n",
      "Epoch:  320\n",
      "Current Learning Rate 0.01\n",
      "Training Loss:  0.4299545\n",
      "Validation Loss:  1.7104981\n",
      "Saving Checkpoint for global_step 320\n",
      "\n",
      "Time Elapased:  6.927343371999996\n",
      "\n",
      "Epoch:  321\n",
      "Current Learning Rate 0.01\n",
      "Training Loss:  0.4186543\n",
      "Validation Loss:  1.7147492\n",
      "\n",
      "Time Elapased:  6.540329871000267\n",
      "\n",
      "Epoch:  322\n",
      "Current Learning Rate 0.01\n",
      "Training Loss:  0.4175663\n",
      "Validation Loss:  1.7199712\n",
      "\n",
      "Time Elapased:  6.558953263000149\n",
      "\n",
      "Epoch:  323\n",
      "Current Learning Rate 0.01\n",
      "Training Loss:  0.41151482\n",
      "Validation Loss:  1.7212813\n",
      "\n",
      "Time Elapased:  6.8148111310001696\n",
      "\n",
      "Epoch:  324\n",
      "Current Learning Rate 0.01\n",
      "Training Loss:  0.4128362\n",
      "Validation Loss:  1.7192461\n",
      "\n",
      "Time Elapased:  6.62399582199987\n",
      "\n",
      "Epoch:  325\n",
      "Current Learning Rate 0.01\n",
      "Training Loss:  0.4025795\n",
      "Validation Loss:  1.7227689\n",
      "\n",
      "Time Elapased:  6.9711868630001845\n",
      "\n",
      "Epoch:  326\n",
      "Current Learning Rate 0.01\n",
      "Training Loss:  0.40128663\n",
      "Validation Loss:  1.7217325\n",
      "\n",
      "Time Elapased:  6.919939498000076\n",
      "\n",
      "Epoch:  327\n",
      "Current Learning Rate 0.01\n",
      "Training Loss:  0.39559588\n",
      "Validation Loss:  1.7257311\n",
      "\n",
      "Time Elapased:  6.766142872999808\n",
      "\n",
      "Epoch:  328\n",
      "Current Learning Rate 0.01\n",
      "Training Loss:  0.39537278\n",
      "Validation Loss:  1.7259483\n",
      "\n",
      "Time Elapased:  7.300168116999885\n",
      "\n",
      "Epoch:  329\n",
      "Current Learning Rate 0.01\n",
      "Training Loss:  0.3925041\n",
      "Validation Loss:  1.7274156\n",
      "\n",
      "Time Elapased:  6.5621190510000815\n",
      "\n",
      "Epoch:  330\n",
      "Current Learning Rate 0.01\n",
      "Training Loss:  0.39227316\n",
      "Validation Loss:  1.7272313\n",
      "Saving Checkpoint for global_step 330\n",
      "\n",
      "Time Elapased:  6.993482322000091\n",
      "\n",
      "Epoch:  331\n",
      "Current Learning Rate 0.01\n",
      "Training Loss:  0.40082815\n",
      "Validation Loss:  1.73084\n",
      "\n",
      "Time Elapased:  6.339733113999955\n",
      "\n",
      "Epoch:  332\n",
      "Current Learning Rate 0.01\n",
      "Training Loss:  0.40522447\n",
      "Validation Loss:  1.7268093\n",
      "\n",
      "Time Elapased:  6.632787310000367\n",
      "\n",
      "Epoch:  333\n",
      "Current Learning Rate 0.01\n",
      "Training Loss:  0.41629735\n",
      "Validation Loss:  1.7246368\n",
      "\n",
      "Time Elapased:  6.373429846999898\n",
      "\n",
      "Epoch:  334\n",
      "Current Learning Rate 0.01\n",
      "Training Loss:  0.41670325\n",
      "Validation Loss:  1.7333992\n",
      "\n",
      "Time Elapased:  6.3381464219996815\n",
      "\n",
      "Epoch:  335\n",
      "Current Learning Rate 0.01\n",
      "Training Loss:  0.4121376\n",
      "Validation Loss:  1.7336869\n",
      "\n",
      "Time Elapased:  7.076588208999965\n",
      "\n",
      "Epoch:  336\n",
      "Current Learning Rate 0.01\n",
      "Training Loss:  0.40799007\n",
      "Validation Loss:  1.7413031\n",
      "\n",
      "Time Elapased:  6.729684496000118\n",
      "\n",
      "Epoch:  337\n",
      "Current Learning Rate 0.01\n",
      "Training Loss:  0.40203562\n",
      "Validation Loss:  1.7351246\n",
      "\n",
      "Time Elapased:  6.527318876999743\n",
      "\n",
      "Epoch:  338\n",
      "Current Learning Rate 0.01\n",
      "Training Loss:  0.39665565\n",
      "Validation Loss:  1.7416294\n",
      "\n",
      "Time Elapased:  6.623445709999942\n",
      "\n",
      "Epoch:  339\n",
      "Current Learning Rate 0.01\n",
      "Training Loss:  0.39241037\n",
      "Validation Loss:  1.7444259\n",
      "\n",
      "Time Elapased:  6.992904434999673\n",
      "\n",
      "Epoch:  340\n",
      "Current Learning Rate 0.01\n",
      "Training Loss:  0.38682404\n",
      "Validation Loss:  1.745295\n",
      "Saving Checkpoint for global_step 340\n",
      "\n",
      "Time Elapased:  6.880775242999789\n",
      "\n",
      "Epoch:  341\n",
      "Current Learning Rate 0.01\n",
      "Training Loss:  0.38239124\n",
      "Validation Loss:  1.747198\n",
      "\n",
      "Time Elapased:  6.4035813280002\n",
      "\n",
      "Epoch:  342\n",
      "Current Learning Rate 0.01\n",
      "Training Loss:  0.3791161\n",
      "Validation Loss:  1.7527316\n",
      "\n",
      "Time Elapased:  6.3175576969997564\n",
      "\n",
      "Epoch:  343\n",
      "Current Learning Rate 0.01\n",
      "Training Loss:  0.37639835\n",
      "Validation Loss:  1.7496545\n",
      "\n",
      "Time Elapased:  6.324700624999878\n",
      "\n",
      "Epoch:  344\n",
      "Current Learning Rate 0.01\n",
      "Training Loss:  0.37490886\n",
      "Validation Loss:  1.7526942\n",
      "\n",
      "Time Elapased:  6.369517482999981\n",
      "\n",
      "Epoch:  345\n",
      "Current Learning Rate 0.01\n",
      "Training Loss:  0.37477463\n",
      "Validation Loss:  1.7565697\n",
      "\n",
      "Time Elapased:  6.409658725000099\n",
      "\n",
      "Epoch:  346\n",
      "Current Learning Rate 0.01\n",
      "Training Loss:  0.37318707\n",
      "Validation Loss:  1.7557174\n",
      "\n",
      "Time Elapased:  6.559810181000103\n",
      "\n",
      "Epoch:  347\n",
      "Current Learning Rate 0.01\n",
      "Training Loss:  0.37245384\n",
      "Validation Loss:  1.7543795\n",
      "\n",
      "Time Elapased:  6.326691385000231\n",
      "\n",
      "Epoch:  348\n",
      "Current Learning Rate 0.01\n",
      "Training Loss:  0.37170446\n",
      "Validation Loss:  1.7605838\n",
      "\n",
      "Time Elapased:  6.612957772999835\n",
      "\n",
      "Epoch:  349\n",
      "Current Learning Rate 0.01\n",
      "Training Loss:  0.37372458\n",
      "Validation Loss:  1.7565241\n",
      "\n",
      "Time Elapased:  6.587507023999933\n",
      "\n",
      "Epoch:  350\n",
      "Current Learning Rate 0.01\n",
      "Training Loss:  0.37610602\n",
      "Validation Loss:  1.7632815\n",
      "Saving Checkpoint for global_step 350\n",
      "\n",
      "Time Elapased:  7.415467781999723\n",
      "\n",
      "Epoch:  351\n",
      "Current Learning Rate 0.01\n",
      "Training Loss:  0.37749198\n",
      "Validation Loss:  1.7598553\n",
      "\n",
      "Time Elapased:  6.452844284999628\n",
      "\n",
      "Epoch:  352\n",
      "Current Learning Rate 0.01\n",
      "Training Loss:  0.37757647\n",
      "Validation Loss:  1.764814\n",
      "\n",
      "Time Elapased:  6.3495083809998505\n",
      "\n",
      "Epoch:  353\n",
      "Current Learning Rate 0.01\n",
      "Training Loss:  0.37352622\n",
      "Validation Loss:  1.7662194\n",
      "\n",
      "Time Elapased:  6.455182353000055\n",
      "\n",
      "Epoch:  354\n",
      "Current Learning Rate 0.01\n",
      "Training Loss:  0.36914325\n",
      "Validation Loss:  1.7650774\n",
      "\n",
      "Time Elapased:  6.342622115999802\n",
      "\n",
      "Epoch:  355\n",
      "Current Learning Rate 0.01\n",
      "Training Loss:  0.36724237\n",
      "Validation Loss:  1.7692752\n",
      "\n",
      "Time Elapased:  6.332621146000292\n",
      "\n",
      "Epoch:  356\n",
      "Current Learning Rate 0.01\n",
      "Training Loss:  0.3736409\n",
      "Validation Loss:  1.7725487\n",
      "\n",
      "Time Elapased:  6.328732702000252\n",
      "\n",
      "Epoch:  357\n",
      "Current Learning Rate 0.01\n",
      "Training Loss:  0.37210065\n",
      "Validation Loss:  1.7694472\n",
      "\n",
      "Time Elapased:  6.336882287000208\n",
      "\n",
      "Epoch:  358\n",
      "Current Learning Rate 0.01\n",
      "Training Loss:  0.36789018\n",
      "Validation Loss:  1.7743672\n",
      "\n",
      "Time Elapased:  6.340534457000103\n",
      "\n",
      "Epoch:  359\n",
      "Current Learning Rate 0.01\n",
      "Training Loss:  0.3681199\n",
      "Validation Loss:  1.7760333\n",
      "\n",
      "Time Elapased:  6.336181578999913\n",
      "\n",
      "Epoch:  360\n",
      "Current Learning Rate 0.01\n",
      "Training Loss:  0.3660991\n",
      "Validation Loss:  1.7755047\n",
      "Saving Checkpoint for global_step 360\n",
      "\n",
      "Time Elapased:  6.632872032999785\n",
      "\n",
      "Epoch:  361\n",
      "Current Learning Rate 0.01\n",
      "Training Loss:  0.36501914\n",
      "Validation Loss:  1.7753774\n",
      "\n",
      "Time Elapased:  6.408431676999953\n",
      "\n",
      "Epoch:  362\n",
      "Current Learning Rate 0.01\n",
      "Training Loss:  0.37119734\n",
      "Validation Loss:  1.7759844\n",
      "\n",
      "Time Elapased:  6.3367124800001875\n",
      "\n",
      "Epoch:  363\n",
      "Current Learning Rate 0.01\n",
      "Training Loss:  0.368901\n",
      "Validation Loss:  1.7835563\n",
      "\n",
      "Time Elapased:  6.341976868999609\n",
      "\n",
      "Epoch:  364\n",
      "Current Learning Rate 0.01\n",
      "Training Loss:  0.37165493\n",
      "Validation Loss:  1.7821805\n",
      "\n",
      "Time Elapased:  6.328038568000011\n",
      "\n",
      "Epoch:  365\n",
      "Current Learning Rate 0.01\n",
      "Training Loss:  0.37392095\n",
      "Validation Loss:  1.7812465\n",
      "\n",
      "Time Elapased:  6.333802702999947\n",
      "\n",
      "Epoch:  366\n",
      "Current Learning Rate 0.01\n",
      "Training Loss:  0.3722213\n",
      "Validation Loss:  1.7842672\n",
      "\n",
      "Time Elapased:  6.345478814999751\n",
      "\n",
      "Epoch:  367\n",
      "Current Learning Rate 0.01\n",
      "Training Loss:  0.37555635\n",
      "Validation Loss:  1.7775717\n",
      "\n",
      "Time Elapased:  6.408722892000242\n",
      "\n",
      "Epoch:  368\n",
      "Current Learning Rate 0.01\n",
      "Training Loss:  0.37894672\n",
      "Validation Loss:  1.783232\n",
      "\n",
      "Time Elapased:  6.311119007999878\n",
      "\n",
      "Epoch:  369\n",
      "Current Learning Rate 0.01\n",
      "Training Loss:  0.38141388\n",
      "Validation Loss:  1.7842218\n",
      "\n",
      "Time Elapased:  6.387286314999983\n",
      "\n",
      "Epoch:  370\n",
      "Current Learning Rate 0.01\n",
      "Training Loss:  0.37945917\n",
      "Validation Loss:  1.7868861\n",
      "Saving Checkpoint for global_step 370\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Time Elapased:  6.63448127699985\n",
      "\n",
      "Epoch:  371\n",
      "Current Learning Rate 0.01\n",
      "Training Loss:  0.39516258\n",
      "Validation Loss:  1.7812009\n",
      "\n",
      "Time Elapased:  6.334369797000363\n",
      "\n",
      "Epoch:  372\n",
      "Current Learning Rate 0.01\n",
      "Training Loss:  0.391843\n",
      "Validation Loss:  1.7882211\n",
      "\n",
      "Time Elapased:  6.3393315399998755\n",
      "\n",
      "Epoch:  373\n",
      "Current Learning Rate 0.01\n",
      "Training Loss:  0.38008472\n",
      "Validation Loss:  1.7915816\n",
      "\n",
      "Time Elapased:  6.334188180999718\n",
      "\n",
      "Epoch:  374\n",
      "Current Learning Rate 0.01\n",
      "Training Loss:  0.37139755\n",
      "Validation Loss:  1.7897185\n",
      "\n",
      "Time Elapased:  6.4027960989997155\n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-2ae03d334f07>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mall_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mVideoIds_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mFramestamps_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mH_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mIpast_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mIfuture_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mYcaptions_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mXcaptions_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mall_val\u001b[0m \u001b[0;34m=\u001b[0m   \u001b[0;34m(\u001b[0m\u001b[0mVideoIds_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mFramestamps_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mH_val\u001b[0m\u001b[0;34m,\u001b[0m   \u001b[0mIpast_val\u001b[0m\u001b[0;34m,\u001b[0m   \u001b[0mIfuture_val\u001b[0m\u001b[0;34m,\u001b[0m   \u001b[0mYcaptions_val\u001b[0m\u001b[0;34m,\u001b[0m   \u001b[0mXcaptions_val\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mexecute\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mall_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mall_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlearning_rate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeep_prob\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_epochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhome_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mversion\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-10-b121b2956421>\u001b[0m in \u001b[0;36mmodel\u001b[0;34m(all_train, all_val, starter_learning_rate, keep_prob, num_epochs, home_dir, version, print_cost)\u001b[0m\n\u001b[1;32m    141\u001b[0m                       \u001b[0mspj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_keep_prob\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mkeep_prob\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    142\u001b[0m                       spj._reg: 0.0}\n\u001b[0;32m--> 143\u001b[0;31m                 \u001b[0m_\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0mminibatch_train_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mspj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlearning_rate\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrain_feed\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    144\u001b[0m                 \u001b[0mtrain_losses\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mminibatch_train_loss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    145\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/shared/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    903\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    904\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 905\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    906\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    907\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/shared/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1107\u001b[0m             \u001b[0mfeed_handles\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0msubfeed_t\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msubfeed_val\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1108\u001b[0m           \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1109\u001b[0;31m             \u001b[0mnp_val\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msubfeed_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msubfeed_dtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1110\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1111\u001b[0m           if (not is_tensor_handle_feed and\n",
      "\u001b[0;32m/home/shared/anaconda3/lib/python3.6/site-packages/numpy/core/numeric.py\u001b[0m in \u001b[0;36masarray\u001b[0;34m(a, dtype, order)\u001b[0m\n\u001b[1;32m    490\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    491\u001b[0m     \"\"\"\n\u001b[0;32m--> 492\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0morder\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    493\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    494\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Train Model\n",
    "learning_rate = 0.01\n",
    "keep_prob = 1.0\n",
    "num_epochs = 1000\n",
    "all_train = (VideoIds_train, Framestamps_train, H_train, Ipast_train, Ifuture_train, Ycaptions_train, Xcaptions_train)\n",
    "all_val =   (VideoIds_val, Framestamps_val, H_val,   Ipast_val,   Ifuture_val,   Ycaptions_val,   Xcaptions_val)\n",
    "execute = model(all_train, all_val, learning_rate, keep_prob, num_epochs, home_dir, version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Restoring from latest checkpoint...\n",
      "INFO:tensorflow:Restoring parameters from /home/martnzjulio_a/songze/checkpoints_test5/model-370\n",
      "0.3835014\n"
     ]
    }
   ],
   "source": [
    "def setup_graph_and_saver(learning_rate):\n",
    "    tf.reset_default_graph()    \n",
    "    tf.set_random_seed(1)                             \n",
    "    seed = 3 \n",
    "    global_step = tf.Variable(0, name='global_step', trainable=False)\n",
    "    config = Config()\n",
    "    spj = SPJ(config)\n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate = learning_rate).minimize(spj._loss, global_step=global_step)\n",
    "    #optimizer = tf.train.MomentumOptimizer(learning_rate=learning_rate,momentum=0.9).minimize(spj._loss, global_step=global_step)  \n",
    "    init = tf.global_variables_initializer()\n",
    "    saver = tf.train.Saver()\n",
    "    return spj, saver, global_step, optimizer, init, seed\n",
    "\n",
    "def direct_inference(data, learning_rate, minibatch_size,home_dir, version):\n",
    "\n",
    "    # Extract Test Data\n",
    "    (VideoIds, Framestamps, H, Ipast, Ifuture, Ycaptions, Xcaptions) = data\n",
    "    num_data = H.shape[0]\n",
    "    \n",
    "    # Setup Graph\n",
    "    spj, saver, global_step, optimizer, init, seed = setup_graph_and_saver(learning_rate)\n",
    "    # Directory Where Saved Checkpoint\n",
    "    checkpoint_dir = home_dir + \"/checkpoints_\" + str(version) + \"/\"\n",
    "    \n",
    "    # Start Session\n",
    "    with tf.Session() as sess:\n",
    "\n",
    "        # Check for Latest Checkpoint\n",
    "        latest_checkpoint = tf.train.latest_checkpoint(checkpoint_dir)\n",
    "        print(\"Restoring from latest checkpoint...\")\n",
    "        saver.restore(sess, latest_checkpoint)\n",
    "        \n",
    "        # Get minibatches\n",
    "        num_minibatches = num_data // minibatch_size  \n",
    "        seed = seed + 1\n",
    "        minibatches = random_mini_batches(VideoIds, Framestamps, H, Ipast, Ifuture, Ycaptions, Xcaptions, minibatch_size, seed) \n",
    "        \n",
    "        losses = []\n",
    "        \n",
    "        # For all batchs\n",
    "        for counter, minibatch in enumerate(minibatches):\n",
    "            \n",
    "            # Select minibatch\n",
    "            (minibatch_VideoIds, minibatch_Framestamps, minibatch_H, minibatch_Ipast, minibatch_Ifuture, minibatch_Ycaptions, minibatch_Xcaptions) = minibatch\n",
    "            minibatch_Ycaptions = id_2_one_hot_void_padding(minibatch_Ycaptions, spj.config.num_classes, void_dim=0)\n",
    "            \n",
    "            # Feed\n",
    "            feed={spj._H: minibatch_H, \n",
    "                  spj._Ipast: minibatch_Ipast, \n",
    "                  spj._Ifuture: minibatch_Ifuture, \n",
    "                  spj._x: minibatch_Xcaptions, \n",
    "                  spj._y: minibatch_Ycaptions, \n",
    "                  spj._keep_prob: 1.0,\n",
    "                  spj._reg: 0.0\n",
    "                 }\n",
    "            \n",
    "            # Run Predictions\n",
    "            loss, pred, lab = sess.run([spj._loss, spj._predictions, spj._y], feed_dict=feed) \n",
    "            lab = np.argmax(lab,axis=3)\n",
    "            losses.append(loss)\n",
    "            \n",
    "            # Cache Results\n",
    "            if counter == 0:\n",
    "                predictions = pred\n",
    "                labels = lab\n",
    "                ids = minibatch_VideoIds\n",
    "            else:\n",
    "                predictions = np.concatenate((predictions,pred),axis=0)\n",
    "                labels = np.concatenate((labels,lab),axis=0)\n",
    "                ids = np.concatenate((ids, minibatch_VideoIds),axis=0)\n",
    "        avg_loss = np.mean(losses)\n",
    "        print(avg_loss)\n",
    "\n",
    "    return predictions, labels, ids\n",
    "data = (VideoIds_train, Framestamps_train, H_train, Ipast_train, Ifuture_train, Ycaptions_train, Xcaptions_train)\n",
    "#data =   (VideoIds_val, Framestamps_val, H_val,   Ipast_val,   Ifuture_val,   Ycaptions_val,   Xcaptions_val)\n",
    "#data =   (VideoIds_test, Framestamps_test, H_test,   Ipast_test,   Ifuture_test,   Ycaptions_test,   Xcaptions_test)\n",
    "predictions2, labels2, ids2 = direct_inference(data, learning_rate, minibatch_size, home_dir, version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "VIDEO ID             PREDICTION           LABEL               \n",
      "--------             -----                -----               \n",
      "v_jIQFVSymHQs        a                    a                   \n",
      "v_jIQFVSymHQs        young                young               \n",
      "v_jIQFVSymHQs        boy                  boy                 \n",
      "v_jIQFVSymHQs        in                   in                  \n",
      "v_jIQFVSymHQs        blue                 blue                \n",
      "v_jIQFVSymHQs        shirt                shirt               \n",
      "v_jIQFVSymHQs        and                  and                 \n",
      "v_jIQFVSymHQs        jeans                jeans               \n",
      "v_jIQFVSymHQs        retrieves            retrieves           \n",
      "v_jIQFVSymHQs        the                  the                 \n",
      "v_jIQFVSymHQs        ball                 ball                \n",
      "v_jIQFVSymHQs        the                  and                 \n",
      "v_jIQFVSymHQs        runs                 runs                \n",
      "v_jIQFVSymHQs        towards              towards             \n",
      "v_jIQFVSymHQs        b'<end>'             the                 \n",
      "v_jIQFVSymHQs        group                group               \n",
      "v_jIQFVSymHQs        b'<end>'             b'<end>'            \n",
      "v_jIQFVSymHQs        group                b'<pad>'            \n",
      "v_jIQFVSymHQs        b'<end>'             b'<pad>'            \n",
      "v_jIQFVSymHQs        group                b'<pad>'            \n",
      "v_jIQFVSymHQs        b'<end>'             b'<pad>'            \n",
      "v_jIQFVSymHQs        group                b'<pad>'            \n",
      "v_jIQFVSymHQs        b'<end>'             b'<pad>'            \n",
      "v_jIQFVSymHQs        group                b'<pad>'            \n",
      "v_jIQFVSymHQs        b'<end>'             b'<pad>'            \n",
      "v_jIQFVSymHQs        group                b'<pad>'            \n",
      "v_jIQFVSymHQs        b'<end>'             b'<pad>'            \n",
      "v_jIQFVSymHQs        group                b'<pad>'            \n",
      "v_jIQFVSymHQs        b'<end>'             b'<pad>'            \n",
      "v_jIQFVSymHQs        group                b'<pad>'            \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print_pred_and_labels(predictions2, labels2, ids2, id2word, example=16, proposal=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/shared/anaconda3/lib/python3.6/site-packages/nltk/translate/bleu_score.py:490: UserWarning: \n",
      "Corpus/Sentence contains 0 counts of 2-gram overlaps.\n",
      "BLEU scores might be undesirable; use SmoothingFunction().\n",
      "  warnings.warn(_msg)\n",
      "/home/shared/anaconda3/lib/python3.6/site-packages/nltk/translate/bleu_score.py:490: UserWarning: \n",
      "Corpus/Sentence contains 0 counts of 3-gram overlaps.\n",
      "BLEU scores might be undesirable; use SmoothingFunction().\n",
      "  warnings.warn(_msg)\n",
      "/home/shared/anaconda3/lib/python3.6/site-packages/nltk/translate/bleu_score.py:490: UserWarning: \n",
      "Corpus/Sentence contains 0 counts of 5-gram overlaps.\n",
      "BLEU scores might be undesirable; use SmoothingFunction().\n",
      "  warnings.warn(_msg)\n",
      "/home/shared/anaconda3/lib/python3.6/site-packages/nltk/translate/bleu_score.py:490: UserWarning: \n",
      "Corpus/Sentence contains 0 counts of 4-gram overlaps.\n",
      "BLEU scores might be undesirable; use SmoothingFunction().\n",
      "  warnings.warn(_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6072733966884537 0.5981186470927379 0.611003799519507 0.6224274276705752\n"
     ]
    }
   ],
   "source": [
    "bleu1, bleu2, bleu3, bleu4 = compute_bleu_at_1_2_3_4(labels2, predictions2)\n",
    "print(bleu1, bleu2, bleu3, bleu4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
