{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "# As usual, a bit of setup\n",
    "import time\n",
    "from time import process_time\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import LOUPE.WILLOW.loupe as lp\n",
    "import tensorflow as tf\n",
    "import h5py\n",
    "import pandas as pd\n",
    "import csv\n",
    "import copy\n",
    "import math\n",
    "from utils.data_utils import *\n",
    "import sys\n",
    "import re\n",
    "from utils.spj import Config\n",
    "from utils.spj import SPJ\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = (10.0, 8.0) # set default size of plots\n",
    "plt.rcParams['image.interpolation'] = 'nearest'\n",
    "plt.rcParams['image.cmap'] = 'gray'\n",
    "\n",
    "# for auto-reloading external modules\n",
    "# see http://stackoverflow.com/questions/1907993/autoreload-of-modules-in-ipython\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Specify Model Directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "DIRECTORY SET TO:  /home/songzeli\n",
      "VERSION SET TO  :  100_train_attention_in_graph\n"
     ]
    }
   ],
   "source": [
    "# home_dir = \"/home/martnzjulio_a/songze\"\n",
    "home_dir = \"/home/songzeli\"\n",
    "version = \"100_train_attention_in_graph\"\n",
    "minibatch_size = 25\n",
    "\n",
    "print()\n",
    "print(\"DIRECTORY SET TO: \", home_dir)\n",
    "print(\"VERSION SET TO  : \", version)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of words in all captions:  532264\n",
      "Vocabulary Size (Unique):  11125\n"
     ]
    }
   ],
   "source": [
    "# Load Vocabulary\n",
    "embedding_size =512\n",
    "pad_len, num_steps = 30, 30\n",
    "max_num_proposals = 10\n",
    "vocabulary,vocab_size = caption_preprocess(home_dir)\n",
    "emb_matrix,word2id,id2word = get_wordvector(embedding_size,vocab_size,vocabulary)\n",
    "num_classes = len(word2id)\n",
    "\n",
    "# Word Embedding Matrix\n",
    "emb_matrix, word2id, id2word = get_wordvector(embedding_size,vocab_size,vocabulary) #changed by Songze"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load All Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Training Examples: 100\n",
      "\n",
      "VideoIds_train.shape:  (100,)\n",
      "Framestamps_train.shape:  (100, 2, 10)\n",
      "Xcaptions_train.shape:  (100, 10, 30)\n",
      "Ycaptions_train.shape:  (100, 10, 30)\n",
      "H_train.shape:  (100, 500, 10)\n",
      "Ipast_train.shape:  (100, 10, 10)\n",
      "Ifuture_train.shape:  (100, 10, 10)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "num_train = 100\n",
    "\n",
    "# Load Training Data\n",
    "train_file = home_dir + \"/SPJ/train_2400.csv\"\n",
    "train_ids,train_data,train_padded_proposals,train_padded_framestamps = video_preprocess(home_dir, train_file, max_num_proposals)\n",
    "\n",
    "# Train Captions\n",
    "train_padded_sentences,train_padded_sentences_2,train_padded_sentences_id = get_padded_sentences_id(pad_len, train_ids, train_data, word2id, max_num_proposals) \n",
    "Ycaptions_train = np.transpose(copy.deepcopy(train_padded_sentences_2),axes=(0,2,1)).astype(np.int32)[:num_train,:,1:]\n",
    "Xcaptions_train = np.transpose(copy.deepcopy(train_padded_sentences),axes=(0,2,1)).astype(np.int32)[:num_train]\n",
    "\n",
    "Ycaptions_train = truncate_captions(Ycaptions_train)\n",
    "Xcaptions_train = truncate_captions(Xcaptions_train)\n",
    "\n",
    "\n",
    "# Train Features \n",
    "VideoIds_train = train_ids[:num_train]\n",
    "Framestamps_train = train_padded_framestamps[:num_train]\n",
    "H_train = train_padded_proposals.astype(np.float32)[:num_train]\n",
    "Ipast_train = temporal_indicator(train_padded_framestamps, mode=\"past\").astype(np.float32)[:num_train]\n",
    "Ifuture_train = temporal_indicator(train_padded_framestamps, mode=\"future\").astype(np.float32)[:num_train]\n",
    "\n",
    "num_train = len(train_ids[:num_train])\n",
    "print(\"Number of Training Examples:\", num_train)\n",
    "print()\n",
    "print(\"VideoIds_train.shape: \", VideoIds_train.shape)\n",
    "print(\"Framestamps_train.shape: \", Framestamps_train.shape)\n",
    "print(\"Xcaptions_train.shape: \", Xcaptions_train.shape)\n",
    "print(\"Ycaptions_train.shape: \", Ycaptions_train.shape)\n",
    "print(\"H_train.shape: \", H_train.shape)\n",
    "print(\"Ipast_train.shape: \", Ipast_train.shape)\n",
    "print(\"Ifuture_train.shape: \", Ifuture_train.shape)\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Validation Examples: 50\n",
      "\n",
      "VideoIds_val.shape:  (50,)\n",
      "Framestamps_val.shape:  (50, 2, 10)\n",
      "Xcaptions_val.shape:  (50, 10, 30)\n",
      "Ycaptions_val.shape:  (50, 10, 30)\n",
      "H_val.shape:  (50, 500, 10)\n",
      "Ipast_val.shape:  (50, 10, 10)\n",
      "Ifuture_val.shape:  (50, 10, 10)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "num_val = 50\n",
    "\n",
    "# Load Validation Data\n",
    "val_file = home_dir + \"/SPJ/train_val_300.csv\"\n",
    "val_ids,val_data,val_padded_proposals,val_padded_framestamps = video_preprocess(home_dir, val_file, max_num_proposals)\n",
    "\n",
    "# Train Captions\n",
    "val_padded_sentences,val_padded_sentences_2,val_padded_sentences_id = get_padded_sentences_id(pad_len, val_ids, val_data, word2id, max_num_proposals) \n",
    "Ycaptions_val = np.transpose(copy.deepcopy(val_padded_sentences_2),axes=(0,2,1)).astype(np.int32)[:num_val,:,1:]\n",
    "Xcaptions_val = np.transpose(copy.deepcopy(val_padded_sentences),axes=(0,2,1)).astype(np.int32)[:num_val]\n",
    "Ycaptions_val = truncate_captions(Ycaptions_val)\n",
    "Xcaptions_val = truncate_captions(Xcaptions_val)\n",
    "\n",
    "\n",
    "# Train Features \n",
    "VideoIds_val = val_ids[:num_val]\n",
    "Framestamps_val = val_padded_framestamps[:num_val]\n",
    "H_val = val_padded_proposals.astype(np.float32)[:num_val]\n",
    "Ipast_val = temporal_indicator(val_padded_framestamps, mode=\"past\").astype(np.float32)[:num_val]\n",
    "Ifuture_val = temporal_indicator(val_padded_framestamps, mode=\"future\").astype(np.float32)[:num_val]\n",
    "\n",
    "num_val = len(val_ids[:num_val])\n",
    "print(\"Number of Validation Examples:\", num_val)\n",
    "print()\n",
    "print(\"VideoIds_val.shape: \", VideoIds_val.shape)\n",
    "print(\"Framestamps_val.shape: \", Framestamps_val.shape)\n",
    "print(\"Xcaptions_val.shape: \", Xcaptions_val.shape)\n",
    "print(\"Ycaptions_val.shape: \", Ycaptions_val.shape)\n",
    "print(\"H_val.shape: \", H_val.shape)\n",
    "print(\"Ipast_val.shape: \", Ipast_val.shape)\n",
    "print(\"Ifuture_val.shape: \", Ifuture_val.shape)\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model(all_train, all_val, starter_learning_rate, keep_prob, num_epochs, home_dir, version, print_cost = True):\n",
    "    \"\"\"\n",
    "    Implements a tensorflow neural network: C3D->ATTENTION->CAPTIONING\n",
    "    \n",
    "    Arguments:\n",
    "    H_train -- training set, of shape = [n_train,num_c3d_features,num_proposals]\n",
    "    Y_train -- caption labels, of shape = [n_train,num_proposals,num_steps+1]\n",
    "    H_test -- training set, of shape = [n_test,num_c3d_features,num_proposals]\n",
    "    Y_test -- caption labels, of shape = [n_test,num_proposals,num_steps+1]\n",
    "    learning_rate -- learning rate of the optimization\n",
    "    num_epochs -- number of epochs of the optimization loop\n",
    "    minibatch_size -- size of a minibatch\n",
    "    print_cost -- True to print the cost every 100 epochs\n",
    "    \n",
    "    Returns:\n",
    "    parameters -- parameters learnt by the model. They can then be used to predict.\n",
    "    \"\"\"\n",
    "    (VideoIds_train, Framestamps_train, H_train, Ipast_train, Ifuture_train, Ycaptions_train, Xcaptions_train) = all_train\n",
    "    (VideoIds_val, Framestamps_val, H_val,   Ipast_val,   Ifuture_val,   Ycaptions_val,   Xcaptions_val)   = all_val\n",
    "    \n",
    "    # Directory to Save Checkpoint\n",
    "    checkpoint_dir = home_dir + \"/checkpoints_\" + str(version) + \"/\"\n",
    "    tensorboard_dir =  home_dir + \"/tensorboard_\" + str(version) + \"/\"\n",
    "    print(\"Checkpoint directory: \", checkpoint_dir)\n",
    "    print(\"Tensorboard directory: \", tensorboard_dir)\n",
    "    \n",
    "    # Reset Graph\n",
    "    tf.reset_default_graph()    \n",
    "    \n",
    "    # For Consistency\n",
    "    tf.set_random_seed(1)                             \n",
    "    seed = 3                                         \n",
    "    \n",
    "    # Number of Training Examples\n",
    "    num_train = H_train.shape[0] \n",
    "    num_val = H_val.shape[0] \n",
    "    \n",
    "    # to keep track of costs\n",
    "    costs = []\n",
    "    \n",
    "    \n",
    "    # Model\n",
    "    config = Config()\n",
    "    spj = SPJ(config)\n",
    "    \n",
    "    # Print Hyperparameters\n",
    "    print()\n",
    "    print(\"Hyperparameters:\")\n",
    "    print(\"----------------\")\n",
    "    print(\"Starter Learning Rate: \", starter_learning_rate)\n",
    "    print(\"Number of Proposals: \", spj.config.num_proposals)\n",
    "    print(\"C3D Features Dim: \", spj.config.num_c3d_features )\n",
    "    print(\"Batch Size: \", spj.config.batch_size)\n",
    "    print(\"Dropout Keep Prob: \", keep_prob)\n",
    "    print(\"Vocab Size: \", spj.config.num_classes)\n",
    "    print(\"Number of LSTM Time Steps: \", spj.config.num_steps)\n",
    "    print(\"Word Embedding Size: \" , spj.config.hidden_dim)\n",
    "    print(\"LSTM Hidden Dim: \" , spj.config.hidden_dim)\n",
    "    print(\"LSTM Num Layers: \" , spj.config.num_layers)\n",
    "    \n",
    "    # Global Epoch Number\n",
    "    global_step = tf.Variable(0, name='global_step', trainable=False)\n",
    "\n",
    "    # Learning Rate Decay\n",
    "    learning_rate = tf.train.exponential_decay(\n",
    "        learning_rate = starter_learning_rate, \n",
    "        global_step = global_step,\n",
    "        decay_steps = 100000, \n",
    "        decay_rate = 0.96, \n",
    "        staircase=True)\n",
    "    \n",
    "    # Backpropagation: Define the tensorflow optimizer. Use an AdamOptimizer.\n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate = learning_rate).minimize(spj._loss, global_step=global_step)\n",
    "#     optimizer = tf.train.MomentumOptimizer(learning_rate=learning_rate,momentum=0.9).minimize(spj._loss, global_step=global_step)  \n",
    "    \n",
    "    # Passing global_step to minimize() will increment it at each step.\n",
    "    learning_step = (optimizer)\n",
    "    \n",
    "    # Initialize all the variables\n",
    "    init = tf.global_variables_initializer()\n",
    "    \n",
    "    # Add ops to save and restore all the variables.\n",
    "    saver = tf.train.Saver(max_to_keep=5)\n",
    "    \n",
    "    # Tensorboard Loss\n",
    "    #training_summary = tf.summary.scalar(\"training_loss\", spj.loss)\n",
    "    #validation_summary = tf.summary.scalar(\"validation_loss\", spj.loss)\n",
    "    #writer = tf.train.SummaryWriter(...)\n",
    "    \n",
    "\n",
    "    # Start the session to compute the tensorflow graph\n",
    "    with tf.Session() as sess:\n",
    "    \n",
    "        # check for latest checkpoint\n",
    "        latest_checkpoint = tf.train.latest_checkpoint(checkpoint_dir)\n",
    "        if latest_checkpoint == None:\n",
    "            # If no check point run the initialization\n",
    "            print()\n",
    "            print(\"No checkpoint exists, initializing parameters...\")\n",
    "            sess.run(init)\n",
    "        else:\n",
    "            print()\n",
    "            print(\"Restoring from latest checkpoint...\")\n",
    "            saver.restore(sess, latest_checkpoint)\n",
    "            sess.run(init)\n",
    "        \n",
    "        #Tensorboard\n",
    "        summary_writer = tf.summary.FileWriter(tensorboard_dir,sess.graph)\n",
    "        \n",
    "        # Training Loop\n",
    "        for epoch in range(num_epochs):\n",
    "            \n",
    "            # Record start time\n",
    "            print()\n",
    "            start = process_time() \n",
    "            \n",
    "            # Variable to store cost\n",
    "            epoch_train_loss = 0.0\n",
    "            epoch_val_loss = 0.0\n",
    "            train_losses = []\n",
    "            val_losses = []\n",
    "            \n",
    "            # Get minibatches\n",
    "            num_train_minibatches = num_train // spj.config.batch_size \n",
    "            num_val_minibatches = num_val // spj.config.batch_size \n",
    "            seed = seed + 1\n",
    "            train_minibatches = random_mini_batches(VideoIds_train, Framestamps_train, H_train, Ipast_train, Ifuture_train, Ycaptions_train, Xcaptions_train, spj.config.batch_size , seed)\n",
    "            val_minibatches = random_mini_batches(VideoIds_val, Framestamps_val, H_val, Ipast_val,   Ifuture_val,   Ycaptions_val,   Xcaptions_val,   spj.config.batch_size , seed)\n",
    "            \n",
    "            for counter, train_minibatch in enumerate(train_minibatches):\n",
    "                \n",
    "                # Select minibatch\n",
    "                (minibatch_VideoIds_train, minibatch_Framestamps_train, minibatch_H_train, minibatch_Ipast_train, minibatch_Ifuture_train, minibatch_Ycaptions_train, minibatch_Xcaptions_train) = train_minibatch\n",
    "                minibatch_Ycaptions_train = id_2_one_hot_void_padding(minibatch_Ycaptions_train, spj.config.num_classes, void_dim=0)\n",
    "                \n",
    "                # Run Train Session\n",
    "                train_feed={spj._H: minibatch_H_train, \n",
    "                      spj._Ipast: minibatch_Ipast_train, \n",
    "                      spj._Ifuture: minibatch_Ifuture_train, \n",
    "                      spj._x: minibatch_Xcaptions_train, \n",
    "                      spj._y: minibatch_Ycaptions_train, \n",
    "                      spj._keep_prob: keep_prob,\n",
    "                      spj._reg: 0.0}\n",
    "                _ , minibatch_train_loss = sess.run([optimizer, spj._loss], feed_dict=train_feed)\n",
    "                train_losses.append(minibatch_train_loss)\n",
    "                \n",
    "            for counter, val_minibatch in enumerate(val_minibatches):\n",
    "                \n",
    "                # Select minibatch\n",
    "                (minibatch_VideoIds_val, minibatch_Framestamps_val, minibatch_H_val, minibatch_Ipast_val, minibatch_Ifuture_val, minibatch_Ycaptions_val, minibatch_Xcaptions_val) = val_minibatch\n",
    "                minibatch_Ycaptions_val = id_2_one_hot_void_padding(minibatch_Ycaptions_val, spj.config.num_classes, void_dim=0)\n",
    "                \n",
    "                # Run Validation Session\n",
    "                val_feed={spj._H: minibatch_H_val, \n",
    "                          spj._Ipast: minibatch_Ipast_val, \n",
    "                          spj._Ifuture: minibatch_Ifuture_val, \n",
    "                          spj._x: minibatch_Xcaptions_val, \n",
    "                          spj._y: minibatch_Ycaptions_val, \n",
    "                          spj._keep_prob: 1.0,\n",
    "                          spj._reg: 0.0}\n",
    "                minibatch_val_loss = sess.run([spj._loss], feed_dict=val_feed) #\n",
    "                val_losses.append(minibatch_val_loss)\n",
    "            \n",
    "            epoch_train_loss = np.mean(train_losses)\n",
    "            epoch_val_loss = np.mean(val_losses)\n",
    "            \n",
    "            # Print cost\n",
    "            if print_cost == True:\n",
    "                global_epoch = tf.train.global_step(sess, global_step)//num_train_minibatches\n",
    "                print(\"Epoch: \", global_epoch)\n",
    "                print (\"Training Loss: \", epoch_train_loss)\n",
    "                print (\"Validation Loss: \", epoch_val_loss)\n",
    "                # Add and Write to Tensorboard\n",
    "                train_summary = tf.Summary()\n",
    "                val_summary = tf.Summary()\n",
    "                train_summary.value.add(tag=\"train_losss\", simple_value=epoch_train_loss)\n",
    "                train_summary.value.add(tag=\"val_losss\", simple_value=epoch_val_loss)\n",
    "                summary_writer.add_summary(train_summary, global_epoch)\n",
    "                summary_writer.add_summary(val_summary, global_epoch)\n",
    "\n",
    "            \n",
    "            # Save Model (every 20 epochs)\n",
    "            if global_epoch % 10 == 0:\n",
    "                print(\"Saving Checkpoint for global_step \" + str(global_epoch))\n",
    "                saver.save(sess, checkpoint_dir + 'model', global_step = global_epoch)\n",
    "        \n",
    "            # Save and Print Processed Time\n",
    "            end = process_time() \n",
    "            print()\n",
    "            print(\"Time Elapased: \", end - start)\n",
    "        \n",
    "        return 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoint directory:  /home/songzeli/checkpoints_100_train_attention_in_graph/\n",
      "Tensorboard directory:  /home/songzeli/tensorboard_100_train_attention_in_graph/\n",
      "\n",
      "Hyperparameters:\n",
      "----------------\n",
      "Starter Learning Rate:  0.01\n",
      "Number of Proposals:  10\n",
      "C3D Features Dim:  500\n",
      "Batch Size:  25\n",
      "Dropout Keep Prob:  0.8\n",
      "Vocab Size:  10999\n",
      "Number of LSTM Time Steps:  30\n",
      "Word Embedding Size:  512\n",
      "LSTM Hidden Dim:  512\n",
      "LSTM Num Layers:  2\n",
      "\n",
      "No checkpoint exists, initializing parameters...\n",
      "\n",
      "Epoch:  1\n",
      "Training Loss:  2.1979616\n",
      "Validation Loss:  1.075989\n",
      "\n",
      "Time Elapased:  6.552239633999761\n",
      "\n",
      "Epoch:  2\n",
      "Training Loss:  1.70839\n",
      "Validation Loss:  1.0904464\n",
      "\n",
      "Time Elapased:  6.067095675000019\n",
      "\n",
      "Epoch:  3\n",
      "Training Loss:  1.5933272\n",
      "Validation Loss:  1.0959735\n",
      "\n",
      "Time Elapased:  5.9491887409999435\n",
      "\n",
      "Epoch:  4\n",
      "Training Loss:  1.5524871\n",
      "Validation Loss:  1.0952923\n",
      "\n",
      "Time Elapased:  5.716236482000113\n",
      "\n",
      "Epoch:  5\n",
      "Training Loss:  1.5170884\n",
      "Validation Loss:  1.0897216\n",
      "\n",
      "Time Elapased:  6.025244570999803\n",
      "\n",
      "Epoch:  6\n",
      "Training Loss:  1.4944913\n",
      "Validation Loss:  1.0938183\n",
      "\n",
      "Time Elapased:  6.014078504999816\n",
      "\n",
      "Epoch:  7\n",
      "Training Loss:  1.4742583\n",
      "Validation Loss:  1.0967882\n",
      "\n",
      "Time Elapased:  5.826724529999865\n",
      "\n",
      "Epoch:  8\n",
      "Training Loss:  1.4645114\n",
      "Validation Loss:  1.1064076\n",
      "\n",
      "Time Elapased:  6.000407174999964\n",
      "\n",
      "Epoch:  9\n",
      "Training Loss:  1.4468155\n",
      "Validation Loss:  1.1071641\n",
      "\n",
      "Time Elapased:  5.994372138000017\n",
      "\n",
      "Epoch:  10\n",
      "Training Loss:  1.4356365\n",
      "Validation Loss:  1.1087747\n",
      "Saving Checkpoint for global_step 10\n",
      "\n",
      "Time Elapased:  6.121817792000002\n",
      "\n",
      "Epoch:  11\n",
      "Training Loss:  1.4272547\n",
      "Validation Loss:  1.0986655\n",
      "\n",
      "Time Elapased:  5.892768262000118\n",
      "\n",
      "Epoch:  12\n",
      "Training Loss:  1.4100894\n",
      "Validation Loss:  1.1032898\n",
      "\n",
      "Time Elapased:  5.951588548000018\n",
      "\n",
      "Epoch:  13\n",
      "Training Loss:  1.4007521\n",
      "Validation Loss:  1.1050117\n",
      "\n",
      "Time Elapased:  6.000446738999926\n",
      "\n",
      "Epoch:  14\n",
      "Training Loss:  1.3918449\n",
      "Validation Loss:  1.0999051\n",
      "\n",
      "Time Elapased:  5.879866643000241\n",
      "\n",
      "Epoch:  15\n",
      "Training Loss:  1.3802345\n",
      "Validation Loss:  1.0992761\n",
      "\n",
      "Time Elapased:  5.77519856500021\n",
      "\n",
      "Epoch:  16\n",
      "Training Loss:  1.3739331\n",
      "Validation Loss:  1.1031665\n",
      "\n",
      "Time Elapased:  5.924983191999672\n",
      "\n",
      "Epoch:  17\n",
      "Training Loss:  1.3690625\n",
      "Validation Loss:  1.1049298\n",
      "\n",
      "Time Elapased:  5.890016041000308\n",
      "\n",
      "Epoch:  18\n",
      "Training Loss:  1.3607082\n",
      "Validation Loss:  1.1095581\n",
      "\n",
      "Time Elapased:  5.903052951000063\n",
      "\n",
      "Epoch:  19\n",
      "Training Loss:  1.3535424\n",
      "Validation Loss:  1.1107061\n",
      "\n",
      "Time Elapased:  5.99991080899963\n",
      "\n",
      "Epoch:  20\n",
      "Training Loss:  1.3432\n",
      "Validation Loss:  1.1086142\n",
      "Saving Checkpoint for global_step 20\n",
      "\n",
      "Time Elapased:  6.399019638000027\n",
      "\n",
      "Epoch:  21\n",
      "Training Loss:  1.340735\n",
      "Validation Loss:  1.1102104\n",
      "\n",
      "Time Elapased:  5.913085891000264\n",
      "\n",
      "Epoch:  22\n",
      "Training Loss:  1.3363682\n",
      "Validation Loss:  1.1124538\n",
      "\n",
      "Time Elapased:  5.94821293699988\n",
      "\n",
      "Epoch:  23\n",
      "Training Loss:  1.3266953\n",
      "Validation Loss:  1.1161253\n",
      "\n",
      "Time Elapased:  6.017094652000196\n",
      "\n",
      "Epoch:  24\n",
      "Training Loss:  1.3245873\n",
      "Validation Loss:  1.1189358\n",
      "\n",
      "Time Elapased:  6.074768855000002\n",
      "\n",
      "Epoch:  25\n",
      "Training Loss:  1.3176088\n",
      "Validation Loss:  1.1182176\n",
      "\n",
      "Time Elapased:  6.046655381999699\n",
      "\n",
      "Epoch:  26\n",
      "Training Loss:  1.3104066\n",
      "Validation Loss:  1.1217383\n",
      "\n",
      "Time Elapased:  6.001599393000106\n",
      "\n",
      "Epoch:  27\n",
      "Training Loss:  1.3065014\n",
      "Validation Loss:  1.1219211\n",
      "\n",
      "Time Elapased:  5.988118313000086\n",
      "\n",
      "Epoch:  28\n",
      "Training Loss:  1.3003501\n",
      "Validation Loss:  1.1216456\n",
      "\n",
      "Time Elapased:  5.924914945999262\n",
      "\n",
      "Epoch:  29\n",
      "Training Loss:  1.3006896\n",
      "Validation Loss:  1.1241746\n",
      "\n",
      "Time Elapased:  6.039537604000543\n",
      "\n",
      "Epoch:  30\n",
      "Training Loss:  1.2990793\n",
      "Validation Loss:  1.1272873\n",
      "Saving Checkpoint for global_step 30\n",
      "\n",
      "Time Elapased:  6.2634147469998425\n",
      "\n",
      "Epoch:  31\n",
      "Training Loss:  1.2925493\n",
      "Validation Loss:  1.1271746\n",
      "\n",
      "Time Elapased:  5.992935473000216\n",
      "\n",
      "Epoch:  32\n",
      "Training Loss:  1.2845496\n",
      "Validation Loss:  1.1298499\n",
      "\n",
      "Time Elapased:  5.8745383030000085\n",
      "\n",
      "Epoch:  33\n",
      "Training Loss:  1.2817311\n",
      "Validation Loss:  1.1271651\n",
      "\n",
      "Time Elapased:  5.870465531000264\n",
      "\n",
      "Epoch:  34\n",
      "Training Loss:  1.2809302\n",
      "Validation Loss:  1.1293085\n",
      "\n",
      "Time Elapased:  5.995089844999711\n",
      "\n",
      "Epoch:  35\n",
      "Training Loss:  1.2745469\n",
      "Validation Loss:  1.1295422\n",
      "\n",
      "Time Elapased:  5.990898289999677\n",
      "\n",
      "Epoch:  36\n",
      "Training Loss:  1.2716908\n",
      "Validation Loss:  1.1288843\n",
      "\n",
      "Time Elapased:  5.9995049210001525\n",
      "\n",
      "Epoch:  37\n",
      "Training Loss:  1.2678508\n",
      "Validation Loss:  1.1313325\n",
      "\n",
      "Time Elapased:  5.924237145000006\n",
      "\n",
      "Epoch:  38\n",
      "Training Loss:  1.2707572\n",
      "Validation Loss:  1.1279012\n",
      "\n",
      "Time Elapased:  5.866330361000109\n",
      "\n",
      "Epoch:  39\n",
      "Training Loss:  1.2709675\n",
      "Validation Loss:  1.1330464\n",
      "\n",
      "Time Elapased:  5.921659037000609\n",
      "\n",
      "Epoch:  40\n",
      "Training Loss:  1.271351\n",
      "Validation Loss:  1.1307036\n",
      "Saving Checkpoint for global_step 40\n",
      "\n",
      "Time Elapased:  6.403855308999482\n",
      "\n",
      "Epoch:  41\n",
      "Training Loss:  1.2735353\n",
      "Validation Loss:  1.1282566\n",
      "\n",
      "Time Elapased:  5.844967107999764\n",
      "\n",
      "Epoch:  42\n",
      "Training Loss:  1.2631401\n",
      "Validation Loss:  1.1338787\n",
      "\n",
      "Time Elapased:  6.097680453999601\n",
      "\n",
      "Epoch:  43\n",
      "Training Loss:  1.2583202\n",
      "Validation Loss:  1.1378611\n",
      "\n",
      "Time Elapased:  5.899239662000582\n",
      "\n",
      "Epoch:  44\n",
      "Training Loss:  1.258012\n",
      "Validation Loss:  1.1361971\n",
      "\n",
      "Time Elapased:  5.898656686000322\n",
      "\n",
      "Epoch:  45\n",
      "Training Loss:  1.2560712\n",
      "Validation Loss:  1.1414325\n",
      "\n",
      "Time Elapased:  5.916351744999702\n",
      "\n",
      "Epoch:  46\n",
      "Training Loss:  1.2572633\n",
      "Validation Loss:  1.1378791\n",
      "\n",
      "Time Elapased:  5.888858543000424\n",
      "\n",
      "Epoch:  47\n",
      "Training Loss:  1.2431431\n",
      "Validation Loss:  1.1401136\n",
      "\n",
      "Time Elapased:  5.916195783999683\n",
      "\n",
      "Epoch:  48\n",
      "Training Loss:  1.2453656\n",
      "Validation Loss:  1.1415128\n",
      "\n",
      "Time Elapased:  5.965260730999944\n",
      "\n",
      "Epoch:  49\n",
      "Training Loss:  1.240574\n",
      "Validation Loss:  1.1402586\n",
      "\n",
      "Time Elapased:  5.953361108000536\n",
      "\n",
      "Epoch:  50\n",
      "Training Loss:  1.2316453\n",
      "Validation Loss:  1.1421366\n",
      "Saving Checkpoint for global_step 50\n",
      "\n",
      "Time Elapased:  6.319490662000135\n",
      "\n",
      "Epoch:  51\n",
      "Training Loss:  1.2296362\n",
      "Validation Loss:  1.1435091\n",
      "\n",
      "Time Elapased:  6.05166126199947\n",
      "\n",
      "Epoch:  52\n",
      "Training Loss:  1.2299834\n",
      "Validation Loss:  1.1445413\n",
      "\n",
      "Time Elapased:  5.999669478999749\n",
      "\n",
      "Epoch:  53\n",
      "Training Loss:  1.2270827\n",
      "Validation Loss:  1.1424892\n",
      "\n",
      "Time Elapased:  6.016671243000019\n",
      "\n",
      "Epoch:  54\n",
      "Training Loss:  1.227495\n",
      "Validation Loss:  1.1468601\n",
      "\n",
      "Time Elapased:  5.922199405000356\n",
      "\n",
      "Epoch:  55\n",
      "Training Loss:  1.2211106\n",
      "Validation Loss:  1.1440198\n",
      "\n",
      "Time Elapased:  6.1063758650007\n",
      "\n",
      "Epoch:  56\n",
      "Training Loss:  1.2179449\n",
      "Validation Loss:  1.1479079\n",
      "\n",
      "Time Elapased:  5.912764590999359\n",
      "\n",
      "Epoch:  57\n",
      "Training Loss:  1.2183015\n",
      "Validation Loss:  1.1447852\n",
      "\n",
      "Time Elapased:  5.825403535000078\n",
      "\n",
      "Epoch:  58\n",
      "Training Loss:  1.2191291\n",
      "Validation Loss:  1.1452866\n",
      "\n",
      "Time Elapased:  5.7585171269993225\n",
      "\n",
      "Epoch:  59\n",
      "Training Loss:  1.2178768\n",
      "Validation Loss:  1.1453555\n",
      "\n",
      "Time Elapased:  5.916791543999352\n",
      "\n",
      "Epoch:  60\n",
      "Training Loss:  1.2108806\n",
      "Validation Loss:  1.1463579\n",
      "Saving Checkpoint for global_step 60\n",
      "\n",
      "Time Elapased:  6.235097799999494\n",
      "\n",
      "Epoch:  61\n",
      "Training Loss:  1.2050612\n",
      "Validation Loss:  1.1481769\n",
      "\n",
      "Time Elapased:  5.958639727000445\n",
      "\n",
      "Epoch:  62\n",
      "Training Loss:  1.2073883\n",
      "Validation Loss:  1.1492883\n",
      "\n",
      "Time Elapased:  5.867425050999373\n",
      "\n",
      "Epoch:  63\n",
      "Training Loss:  1.2025714\n",
      "Validation Loss:  1.1453193\n",
      "\n",
      "Time Elapased:  5.949489899000582\n",
      "\n",
      "Epoch:  64\n",
      "Training Loss:  1.2035509\n",
      "Validation Loss:  1.1448522\n",
      "\n",
      "Time Elapased:  5.884451109999645\n",
      "\n",
      "Epoch:  65\n",
      "Training Loss:  1.2046599\n",
      "Validation Loss:  1.151364\n",
      "\n",
      "Time Elapased:  6.016208623000239\n",
      "\n",
      "Epoch:  66\n",
      "Training Loss:  1.2060168\n",
      "Validation Loss:  1.1426606\n",
      "\n",
      "Time Elapased:  6.011262683000496\n",
      "\n",
      "Epoch:  67\n",
      "Training Loss:  1.2013456\n",
      "Validation Loss:  1.1405993\n",
      "\n",
      "Time Elapased:  5.945024419999754\n",
      "\n",
      "Epoch:  68\n",
      "Training Loss:  1.1951699\n",
      "Validation Loss:  1.1465191\n",
      "\n",
      "Time Elapased:  5.939022143000329\n",
      "\n",
      "Epoch:  69\n",
      "Training Loss:  1.1897502\n",
      "Validation Loss:  1.142557\n",
      "\n",
      "Time Elapased:  5.946592866999708\n",
      "\n",
      "Epoch:  70\n",
      "Training Loss:  1.1811395\n",
      "Validation Loss:  1.137577\n",
      "Saving Checkpoint for global_step 70\n",
      "\n",
      "Time Elapased:  6.426409507999779\n",
      "\n",
      "Epoch:  71\n",
      "Training Loss:  1.1784394\n",
      "Validation Loss:  1.1378869\n",
      "\n",
      "Time Elapased:  5.925777745000232\n",
      "\n",
      "Epoch:  72\n",
      "Training Loss:  1.1742661\n",
      "Validation Loss:  1.1326618\n",
      "\n",
      "Time Elapased:  6.078235611000309\n",
      "\n",
      "Epoch:  73\n",
      "Training Loss:  1.1703386\n",
      "Validation Loss:  1.1316257\n",
      "\n",
      "Time Elapased:  6.035866835999514\n",
      "\n",
      "Epoch:  74\n",
      "Training Loss:  1.169101\n",
      "Validation Loss:  1.1367657\n",
      "\n",
      "Time Elapased:  5.840176339000209\n",
      "\n",
      "Epoch:  75\n",
      "Training Loss:  1.1649971\n",
      "Validation Loss:  1.1358349\n",
      "\n",
      "Time Elapased:  5.885344348000217\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  76\n",
      "Training Loss:  1.1646026\n",
      "Validation Loss:  1.1335552\n",
      "\n",
      "Time Elapased:  5.922767295000085\n",
      "\n",
      "Epoch:  77\n",
      "Training Loss:  1.1621168\n",
      "Validation Loss:  1.1332083\n",
      "\n",
      "Time Elapased:  5.923474907999662\n",
      "\n",
      "Epoch:  78\n",
      "Training Loss:  1.1618859\n",
      "Validation Loss:  1.1387823\n",
      "\n",
      "Time Elapased:  5.893849670000236\n",
      "\n",
      "Epoch:  79\n",
      "Training Loss:  1.1607641\n",
      "Validation Loss:  1.1281638\n",
      "\n",
      "Time Elapased:  5.952097919999687\n",
      "\n",
      "Epoch:  80\n",
      "Training Loss:  1.1629307\n",
      "Validation Loss:  1.1292666\n",
      "Saving Checkpoint for global_step 80\n",
      "\n",
      "Time Elapased:  6.288596659000177\n",
      "\n",
      "Epoch:  81\n",
      "Training Loss:  1.1583257\n",
      "Validation Loss:  1.1373034\n",
      "\n",
      "Time Elapased:  5.935546414999408\n",
      "\n",
      "Epoch:  82\n",
      "Training Loss:  1.1556821\n",
      "Validation Loss:  1.1282922\n",
      "\n",
      "Time Elapased:  5.97681093600022\n",
      "\n",
      "Epoch:  83\n",
      "Training Loss:  1.152647\n",
      "Validation Loss:  1.1343815\n",
      "\n",
      "Time Elapased:  5.788444332999461\n",
      "\n",
      "Epoch:  84\n",
      "Training Loss:  1.1458685\n",
      "Validation Loss:  1.1363485\n",
      "\n",
      "Time Elapased:  5.971957251000276\n",
      "\n",
      "Epoch:  85\n",
      "Training Loss:  1.1412892\n",
      "Validation Loss:  1.1307662\n",
      "\n",
      "Time Elapased:  5.965430293999816\n",
      "\n",
      "Epoch:  86\n",
      "Training Loss:  1.1365571\n",
      "Validation Loss:  1.131124\n",
      "\n",
      "Time Elapased:  5.9515167030003795\n",
      "\n",
      "Epoch:  87\n",
      "Training Loss:  1.1302941\n",
      "Validation Loss:  1.1323674\n",
      "\n",
      "Time Elapased:  6.036565024999618\n",
      "\n",
      "Epoch:  88\n",
      "Training Loss:  1.1294212\n",
      "Validation Loss:  1.1308935\n",
      "\n",
      "Time Elapased:  5.941251057000045\n",
      "\n",
      "Epoch:  89\n",
      "Training Loss:  1.1242607\n",
      "Validation Loss:  1.1351645\n",
      "\n",
      "Time Elapased:  5.855977598000209\n",
      "\n",
      "Epoch:  90\n",
      "Training Loss:  1.1201956\n",
      "Validation Loss:  1.1297154\n",
      "Saving Checkpoint for global_step 90\n",
      "\n",
      "Time Elapased:  6.239204893000533\n",
      "\n",
      "Epoch:  91\n",
      "Training Loss:  1.1178925\n",
      "Validation Loss:  1.1320292\n",
      "\n",
      "Time Elapased:  6.109936771999855\n",
      "\n",
      "Epoch:  92\n",
      "Training Loss:  1.1114037\n",
      "Validation Loss:  1.1309543\n",
      "\n",
      "Time Elapased:  5.822204131000035\n",
      "\n",
      "Epoch:  93\n",
      "Training Loss:  1.109707\n",
      "Validation Loss:  1.1307726\n",
      "\n",
      "Time Elapased:  5.872471436000524\n",
      "\n",
      "Epoch:  94\n",
      "Training Loss:  1.10616\n",
      "Validation Loss:  1.1356602\n",
      "\n",
      "Time Elapased:  5.917522504000772\n",
      "\n",
      "Epoch:  95\n",
      "Training Loss:  1.102526\n",
      "Validation Loss:  1.1362708\n",
      "\n",
      "Time Elapased:  6.007838331999665\n",
      "\n",
      "Epoch:  96\n",
      "Training Loss:  1.0946054\n",
      "Validation Loss:  1.1357635\n",
      "\n",
      "Time Elapased:  5.9501359329997285\n",
      "\n",
      "Epoch:  97\n",
      "Training Loss:  1.0923593\n",
      "Validation Loss:  1.1378689\n",
      "\n",
      "Time Elapased:  5.879378699000881\n",
      "\n",
      "Epoch:  98\n",
      "Training Loss:  1.0895834\n",
      "Validation Loss:  1.1337025\n",
      "\n",
      "Time Elapased:  5.915878776000682\n",
      "\n",
      "Epoch:  99\n",
      "Training Loss:  1.0879593\n",
      "Validation Loss:  1.13848\n",
      "\n",
      "Time Elapased:  5.958874817000833\n",
      "\n",
      "Epoch:  100\n",
      "Training Loss:  1.0926024\n",
      "Validation Loss:  1.1396217\n",
      "Saving Checkpoint for global_step 100\n",
      "\n",
      "Time Elapased:  6.31544743700033\n",
      "\n",
      "Epoch:  101\n",
      "Training Loss:  1.0890558\n",
      "Validation Loss:  1.1429386\n",
      "\n",
      "Time Elapased:  5.947577114000524\n",
      "\n",
      "Epoch:  102\n",
      "Training Loss:  1.088172\n",
      "Validation Loss:  1.1460559\n",
      "\n",
      "Time Elapased:  5.968196359000103\n",
      "\n",
      "Epoch:  103\n",
      "Training Loss:  1.0876179\n",
      "Validation Loss:  1.1464539\n",
      "\n",
      "Time Elapased:  5.835737884000082\n",
      "\n",
      "Epoch:  104\n",
      "Training Loss:  1.0738704\n",
      "Validation Loss:  1.1514962\n",
      "\n",
      "Time Elapased:  5.995602801000132\n",
      "\n",
      "Epoch:  105\n",
      "Training Loss:  1.0695574\n",
      "Validation Loss:  1.1491091\n",
      "\n",
      "Time Elapased:  5.908799826000177\n",
      "\n",
      "Epoch:  106\n",
      "Training Loss:  1.0640066\n",
      "Validation Loss:  1.1485083\n",
      "\n",
      "Time Elapased:  6.03333954899972\n",
      "\n",
      "Epoch:  107\n",
      "Training Loss:  1.0612121\n",
      "Validation Loss:  1.1518581\n",
      "\n",
      "Time Elapased:  6.039464762000534\n",
      "\n",
      "Epoch:  108\n",
      "Training Loss:  1.056907\n",
      "Validation Loss:  1.1490115\n",
      "\n",
      "Time Elapased:  5.901140498999666\n",
      "\n",
      "Epoch:  109\n",
      "Training Loss:  1.050732\n",
      "Validation Loss:  1.1497539\n",
      "\n",
      "Time Elapased:  6.037185368999417\n",
      "\n",
      "Epoch:  110\n",
      "Training Loss:  1.0478737\n",
      "Validation Loss:  1.1543245\n",
      "Saving Checkpoint for global_step 110\n",
      "\n",
      "Time Elapased:  6.216509037999458\n",
      "\n",
      "Epoch:  111\n",
      "Training Loss:  1.0509533\n",
      "Validation Loss:  1.1521858\n",
      "\n",
      "Time Elapased:  5.86869919000037\n",
      "\n",
      "Epoch:  112\n",
      "Training Loss:  1.0535841\n",
      "Validation Loss:  1.1487852\n",
      "\n",
      "Time Elapased:  5.962138650999805\n",
      "\n",
      "Epoch:  113\n",
      "Training Loss:  1.1048639\n",
      "Validation Loss:  1.1446983\n",
      "\n",
      "Time Elapased:  5.965645672999926\n",
      "\n",
      "Epoch:  114\n",
      "Training Loss:  1.1687318\n",
      "Validation Loss:  1.1469955\n",
      "\n",
      "Time Elapased:  5.858191481000176\n",
      "\n",
      "Epoch:  115\n",
      "Training Loss:  1.2469411\n",
      "Validation Loss:  1.1406442\n",
      "\n",
      "Time Elapased:  5.901384929999949\n",
      "\n",
      "Epoch:  116\n",
      "Training Loss:  1.2553027\n",
      "Validation Loss:  1.1260289\n",
      "\n",
      "Time Elapased:  5.87769114799994\n",
      "\n",
      "Epoch:  117\n",
      "Training Loss:  1.3705149\n",
      "Validation Loss:  1.0335641\n",
      "\n",
      "Time Elapased:  5.839969267000015\n",
      "\n",
      "Epoch:  118\n",
      "Training Loss:  1.5103594\n",
      "Validation Loss:  1.0088876\n",
      "\n",
      "Time Elapased:  5.996699839999565\n",
      "\n",
      "Epoch:  119\n",
      "Training Loss:  1.4226384\n",
      "Validation Loss:  1.0191916\n",
      "\n",
      "Time Elapased:  5.892556005000188\n",
      "\n",
      "Epoch:  120\n",
      "Training Loss:  1.3797275\n",
      "Validation Loss:  1.0146923\n",
      "Saving Checkpoint for global_step 120\n",
      "\n",
      "Time Elapased:  6.613473101000636\n",
      "\n",
      "Epoch:  121\n",
      "Training Loss:  1.3371149\n",
      "Validation Loss:  1.0216881\n",
      "\n",
      "Time Elapased:  5.876302899999246\n",
      "\n",
      "Epoch:  122\n",
      "Training Loss:  1.3192767\n",
      "Validation Loss:  1.021771\n",
      "\n",
      "Time Elapased:  5.946091820999754\n",
      "\n",
      "Epoch:  123\n",
      "Training Loss:  1.2989655\n",
      "Validation Loss:  1.0319583\n",
      "\n",
      "Time Elapased:  5.87700531199971\n",
      "\n",
      "Epoch:  124\n",
      "Training Loss:  1.2854698\n",
      "Validation Loss:  1.0407009\n",
      "\n",
      "Time Elapased:  5.885524895999879\n",
      "\n",
      "Epoch:  125\n",
      "Training Loss:  1.2579994\n",
      "Validation Loss:  1.0485545\n",
      "\n",
      "Time Elapased:  5.830063463000442\n",
      "\n",
      "Epoch:  126\n",
      "Training Loss:  1.2418919\n",
      "Validation Loss:  1.0522231\n",
      "\n",
      "Time Elapased:  5.922151439999652\n",
      "\n",
      "Epoch:  127\n",
      "Training Loss:  1.2318423\n",
      "Validation Loss:  1.0556061\n",
      "\n",
      "Time Elapased:  6.015050753000651\n",
      "\n",
      "Epoch:  128\n",
      "Training Loss:  1.2194345\n",
      "Validation Loss:  1.0531912\n",
      "\n",
      "Time Elapased:  5.912587568000163\n",
      "\n",
      "Epoch:  129\n",
      "Training Loss:  1.2060508\n",
      "Validation Loss:  1.0624058\n",
      "\n",
      "Time Elapased:  5.942716551000558\n",
      "\n",
      "Epoch:  130\n",
      "Training Loss:  1.1956936\n",
      "Validation Loss:  1.0655252\n",
      "Saving Checkpoint for global_step 130\n",
      "\n",
      "Time Elapased:  6.223205018000044\n",
      "\n",
      "Epoch:  131\n",
      "Training Loss:  1.1846312\n",
      "Validation Loss:  1.0716835\n",
      "\n",
      "Time Elapased:  5.997885685000256\n",
      "\n",
      "Epoch:  132\n",
      "Training Loss:  1.1766435\n",
      "Validation Loss:  1.071866\n",
      "\n",
      "Time Elapased:  6.009483293000812\n",
      "\n",
      "Epoch:  133\n",
      "Training Loss:  1.1701438\n",
      "Validation Loss:  1.0783376\n",
      "\n",
      "Time Elapased:  5.921167857\n",
      "\n",
      "Epoch:  134\n",
      "Training Loss:  1.1647953\n",
      "Validation Loss:  1.0791464\n",
      "\n",
      "Time Elapased:  5.87459232799938\n",
      "\n",
      "Epoch:  135\n",
      "Training Loss:  1.154422\n",
      "Validation Loss:  1.085926\n",
      "\n",
      "Time Elapased:  5.859405205999792\n",
      "\n",
      "Epoch:  136\n",
      "Training Loss:  1.1442204\n",
      "Validation Loss:  1.0909967\n",
      "\n",
      "Time Elapased:  5.9345028400002775\n",
      "\n",
      "Epoch:  137\n",
      "Training Loss:  1.1321357\n",
      "Validation Loss:  1.0931706\n",
      "\n",
      "Time Elapased:  5.903353314999549\n",
      "\n",
      "Epoch:  138\n",
      "Training Loss:  1.126659\n",
      "Validation Loss:  1.0936451\n",
      "\n",
      "Time Elapased:  6.103387547999773\n",
      "\n",
      "Epoch:  139\n",
      "Training Loss:  1.1174492\n",
      "Validation Loss:  1.0991824\n",
      "\n",
      "Time Elapased:  5.972147327999664\n",
      "\n",
      "Epoch:  140\n",
      "Training Loss:  1.1109416\n",
      "Validation Loss:  1.1032424\n",
      "Saving Checkpoint for global_step 140\n",
      "\n",
      "Time Elapased:  6.05481311099993\n",
      "\n",
      "Epoch:  141\n",
      "Training Loss:  1.1032223\n",
      "Validation Loss:  1.1034234\n",
      "\n",
      "Time Elapased:  5.801724603999901\n",
      "\n",
      "Epoch:  142\n",
      "Training Loss:  1.1018268\n",
      "Validation Loss:  1.1078625\n",
      "\n",
      "Time Elapased:  5.847439260999636\n",
      "\n",
      "Epoch:  143\n",
      "Training Loss:  1.0967355\n",
      "Validation Loss:  1.1113921\n",
      "\n",
      "Time Elapased:  5.947161201000199\n",
      "\n",
      "Epoch:  144\n",
      "Training Loss:  1.0909164\n",
      "Validation Loss:  1.1187136\n",
      "\n",
      "Time Elapased:  5.959509326999978\n",
      "\n",
      "Epoch:  145\n",
      "Training Loss:  1.0836656\n",
      "Validation Loss:  1.1213522\n",
      "\n",
      "Time Elapased:  5.949319113000456\n",
      "\n",
      "Epoch:  146\n",
      "Training Loss:  1.0774268\n",
      "Validation Loss:  1.1191642\n",
      "\n",
      "Time Elapased:  5.844384741000795\n",
      "\n",
      "Epoch:  147\n",
      "Training Loss:  1.0713761\n",
      "Validation Loss:  1.1245844\n",
      "\n",
      "Time Elapased:  5.769003849000001\n",
      "\n",
      "Epoch:  148\n",
      "Training Loss:  1.0707848\n",
      "Validation Loss:  1.128033\n",
      "\n",
      "Time Elapased:  5.88697478100039\n",
      "\n",
      "Epoch:  149\n",
      "Training Loss:  1.0703982\n",
      "Validation Loss:  1.1292713\n",
      "\n",
      "Time Elapased:  5.902032237999265\n",
      "\n",
      "Epoch:  150\n",
      "Training Loss:  1.0775588\n",
      "Validation Loss:  1.1296484\n",
      "Saving Checkpoint for global_step 150\n",
      "\n",
      "Time Elapased:  6.406836498999837\n",
      "\n",
      "Epoch:  151\n",
      "Training Loss:  1.0758159\n",
      "Validation Loss:  1.1331267\n",
      "\n",
      "Time Elapased:  5.978688716999386\n",
      "\n",
      "Epoch:  152\n",
      "Training Loss:  1.076862\n",
      "Validation Loss:  1.1403139\n",
      "\n",
      "Time Elapased:  6.148586995999722\n",
      "\n",
      "Epoch:  153\n",
      "Training Loss:  1.07732\n",
      "Validation Loss:  1.1371231\n",
      "\n",
      "Time Elapased:  5.9094292890004\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  154\n",
      "Training Loss:  1.0699996\n",
      "Validation Loss:  1.1453427\n",
      "\n",
      "Time Elapased:  5.8646460750005645\n",
      "\n",
      "Epoch:  155\n",
      "Training Loss:  1.0706296\n",
      "Validation Loss:  1.1396618\n",
      "\n",
      "Time Elapased:  5.895059309000317\n",
      "\n",
      "Epoch:  156\n",
      "Training Loss:  1.0674427\n",
      "Validation Loss:  1.1443784\n",
      "\n",
      "Time Elapased:  6.043234819000645\n",
      "\n",
      "Epoch:  157\n",
      "Training Loss:  1.0627346\n",
      "Validation Loss:  1.1453336\n",
      "\n",
      "Time Elapased:  5.924158310999701\n",
      "\n",
      "Epoch:  158\n",
      "Training Loss:  1.0627292\n",
      "Validation Loss:  1.1494328\n",
      "\n",
      "Time Elapased:  5.8751688259999355\n",
      "\n",
      "Epoch:  159\n",
      "Training Loss:  1.0538237\n",
      "Validation Loss:  1.1444428\n",
      "\n",
      "Time Elapased:  5.9975899969995226\n",
      "\n",
      "Epoch:  160\n",
      "Training Loss:  1.0507228\n",
      "Validation Loss:  1.153449\n",
      "Saving Checkpoint for global_step 160\n",
      "\n",
      "Time Elapased:  6.474693729999672\n",
      "\n",
      "Epoch:  161\n",
      "Training Loss:  1.044446\n",
      "Validation Loss:  1.1534194\n",
      "\n",
      "Time Elapased:  5.960681148000731\n",
      "\n",
      "Epoch:  162\n",
      "Training Loss:  1.0335805\n",
      "Validation Loss:  1.155788\n",
      "\n",
      "Time Elapased:  5.9549776939993535\n",
      "\n",
      "Epoch:  163\n",
      "Training Loss:  1.0338544\n",
      "Validation Loss:  1.1562214\n",
      "\n",
      "Time Elapased:  5.937059474000307\n",
      "\n",
      "Epoch:  164\n",
      "Training Loss:  1.0275275\n",
      "Validation Loss:  1.1586211\n",
      "\n",
      "Time Elapased:  5.88953576699987\n",
      "\n",
      "Epoch:  165\n",
      "Training Loss:  1.0194591\n",
      "Validation Loss:  1.1633143\n",
      "\n",
      "Time Elapased:  5.991021597999861\n",
      "\n",
      "Epoch:  166\n",
      "Training Loss:  1.0073183\n",
      "Validation Loss:  1.1666455\n",
      "\n",
      "Time Elapased:  5.928525440999692\n",
      "\n",
      "Epoch:  167\n",
      "Training Loss:  1.0035205\n",
      "Validation Loss:  1.1661068\n",
      "\n",
      "Time Elapased:  6.002449456999784\n",
      "\n",
      "Epoch:  168\n",
      "Training Loss:  0.99666446\n",
      "Validation Loss:  1.1692482\n",
      "\n",
      "Time Elapased:  5.9012315549998675\n",
      "\n",
      "Epoch:  169\n",
      "Training Loss:  0.9955318\n",
      "Validation Loss:  1.175405\n",
      "\n",
      "Time Elapased:  5.864296578999529\n",
      "\n",
      "Epoch:  170\n",
      "Training Loss:  0.9917048\n",
      "Validation Loss:  1.1713835\n",
      "Saving Checkpoint for global_step 170\n",
      "\n",
      "Time Elapased:  6.301025476000177\n",
      "\n",
      "Epoch:  171\n",
      "Training Loss:  0.9866148\n",
      "Validation Loss:  1.1684003\n",
      "\n",
      "Time Elapased:  6.050124495000091\n",
      "\n",
      "Epoch:  172\n",
      "Training Loss:  0.99868745\n",
      "Validation Loss:  1.1808587\n",
      "\n",
      "Time Elapased:  6.047060989999409\n",
      "\n",
      "Epoch:  173\n",
      "Training Loss:  0.99482614\n",
      "Validation Loss:  1.1786579\n",
      "\n",
      "Time Elapased:  5.941219106999597\n",
      "\n",
      "Epoch:  174\n",
      "Training Loss:  0.9882843\n",
      "Validation Loss:  1.1777627\n",
      "\n",
      "Time Elapased:  5.924050761000217\n",
      "\n",
      "Epoch:  175\n",
      "Training Loss:  0.98440367\n",
      "Validation Loss:  1.1778128\n",
      "\n",
      "Time Elapased:  5.948250170000392\n",
      "\n",
      "Epoch:  176\n",
      "Training Loss:  0.9790642\n",
      "Validation Loss:  1.1807954\n",
      "\n",
      "Time Elapased:  5.866072503999931\n",
      "\n",
      "Epoch:  177\n",
      "Training Loss:  0.9822425\n",
      "Validation Loss:  1.1832081\n",
      "\n",
      "Time Elapased:  5.921930248000535\n",
      "\n",
      "Epoch:  178\n",
      "Training Loss:  0.9758577\n",
      "Validation Loss:  1.1858461\n",
      "\n",
      "Time Elapased:  6.071227540000109\n",
      "\n",
      "Epoch:  179\n",
      "Training Loss:  0.97040343\n",
      "Validation Loss:  1.1905693\n",
      "\n",
      "Time Elapased:  5.906848572999479\n",
      "\n",
      "Epoch:  180\n",
      "Training Loss:  0.9624319\n",
      "Validation Loss:  1.1893244\n",
      "Saving Checkpoint for global_step 180\n",
      "\n",
      "Time Elapased:  6.354668402000243\n",
      "\n",
      "Epoch:  181\n",
      "Training Loss:  0.9531231\n",
      "Validation Loss:  1.1930792\n",
      "\n",
      "Time Elapased:  5.9870996370000285\n",
      "\n",
      "Epoch:  182\n",
      "Training Loss:  0.9496036\n",
      "Validation Loss:  1.1954081\n",
      "\n",
      "Time Elapased:  5.89807885800019\n",
      "\n",
      "Epoch:  183\n",
      "Training Loss:  0.9435644\n",
      "Validation Loss:  1.197476\n",
      "\n",
      "Time Elapased:  6.0760552039992035\n",
      "\n",
      "Epoch:  184\n",
      "Training Loss:  0.9375732\n",
      "Validation Loss:  1.2036028\n",
      "\n",
      "Time Elapased:  5.856578513999921\n",
      "\n",
      "Epoch:  185\n",
      "Training Loss:  0.934612\n",
      "Validation Loss:  1.2029837\n",
      "\n",
      "Time Elapased:  5.713448051999876\n",
      "\n",
      "Epoch:  186\n",
      "Training Loss:  0.93377525\n",
      "Validation Loss:  1.205766\n",
      "\n",
      "Time Elapased:  5.844995415000085\n",
      "\n",
      "Epoch:  187\n",
      "Training Loss:  0.92684597\n",
      "Validation Loss:  1.20732\n",
      "\n",
      "Time Elapased:  5.889976845999627\n",
      "\n",
      "Epoch:  188\n",
      "Training Loss:  0.9321354\n",
      "Validation Loss:  1.2089205\n",
      "\n",
      "Time Elapased:  5.892049234999831\n",
      "\n",
      "Epoch:  189\n",
      "Training Loss:  0.93430215\n",
      "Validation Loss:  1.2146001\n",
      "\n",
      "Time Elapased:  5.762373554000078\n",
      "\n",
      "Epoch:  190\n",
      "Training Loss:  0.9279872\n",
      "Validation Loss:  1.2225997\n",
      "Saving Checkpoint for global_step 190\n",
      "\n",
      "Time Elapased:  6.176494577000085\n",
      "\n",
      "Epoch:  191\n",
      "Training Loss:  0.92078805\n",
      "Validation Loss:  1.2152147\n",
      "\n",
      "Time Elapased:  5.868489665999732\n",
      "\n",
      "Epoch:  192\n",
      "Training Loss:  0.916343\n",
      "Validation Loss:  1.2198007\n",
      "\n",
      "Time Elapased:  5.908819686000243\n",
      "\n",
      "Epoch:  193\n",
      "Training Loss:  0.91169643\n",
      "Validation Loss:  1.2254186\n",
      "\n",
      "Time Elapased:  5.941517747000034\n",
      "\n",
      "Epoch:  194\n",
      "Training Loss:  0.90950245\n",
      "Validation Loss:  1.2283595\n",
      "\n",
      "Time Elapased:  5.96296890099984\n",
      "\n",
      "Epoch:  195\n",
      "Training Loss:  0.9030476\n",
      "Validation Loss:  1.2268856\n",
      "\n",
      "Time Elapased:  5.860493023999879\n",
      "\n",
      "Epoch:  196\n",
      "Training Loss:  0.9230492\n",
      "Validation Loss:  1.2280788\n",
      "\n",
      "Time Elapased:  5.8361971180002\n",
      "\n",
      "Epoch:  197\n",
      "Training Loss:  0.9292068\n",
      "Validation Loss:  1.2332487\n",
      "\n",
      "Time Elapased:  5.897047399999792\n",
      "\n",
      "Epoch:  198\n",
      "Training Loss:  0.9289104\n",
      "Validation Loss:  1.2277668\n",
      "\n",
      "Time Elapased:  5.7938458039998295\n",
      "\n",
      "Epoch:  199\n",
      "Training Loss:  0.9183185\n",
      "Validation Loss:  1.2247157\n",
      "\n",
      "Time Elapased:  5.862139367000054\n",
      "\n",
      "Epoch:  200\n",
      "Training Loss:  0.9256711\n",
      "Validation Loss:  1.2263428\n",
      "Saving Checkpoint for global_step 200\n",
      "\n",
      "Time Elapased:  6.309385432000454\n",
      "\n",
      "Epoch:  201\n",
      "Training Loss:  0.910509\n",
      "Validation Loss:  1.2321515\n",
      "\n",
      "Time Elapased:  5.863072465000187\n",
      "\n",
      "Epoch:  202\n",
      "Training Loss:  0.9073133\n",
      "Validation Loss:  1.2348261\n",
      "\n",
      "Time Elapased:  5.933540752000226\n",
      "\n",
      "Epoch:  203\n",
      "Training Loss:  0.90220296\n",
      "Validation Loss:  1.2278512\n",
      "\n",
      "Time Elapased:  6.07092268300039\n",
      "\n",
      "Epoch:  204\n",
      "Training Loss:  0.905791\n",
      "Validation Loss:  1.2338898\n",
      "\n",
      "Time Elapased:  6.012351162999948\n",
      "\n",
      "Epoch:  205\n",
      "Training Loss:  0.9026155\n",
      "Validation Loss:  1.2373967\n",
      "\n",
      "Time Elapased:  5.828222557999652\n",
      "\n",
      "Epoch:  206\n",
      "Training Loss:  0.89633393\n",
      "Validation Loss:  1.2375435\n",
      "\n",
      "Time Elapased:  5.980780912999762\n",
      "\n",
      "Epoch:  207\n",
      "Training Loss:  0.8859619\n",
      "Validation Loss:  1.2412429\n",
      "\n",
      "Time Elapased:  5.904017038999882\n",
      "\n",
      "Epoch:  208\n",
      "Training Loss:  0.8830682\n",
      "Validation Loss:  1.2398784\n",
      "\n",
      "Time Elapased:  5.719479972000045\n",
      "\n",
      "Epoch:  209\n",
      "Training Loss:  0.88106686\n",
      "Validation Loss:  1.2444701\n",
      "\n",
      "Time Elapased:  6.016154740000275\n",
      "\n",
      "Epoch:  210\n",
      "Training Loss:  0.8818009\n",
      "Validation Loss:  1.2433262\n",
      "Saving Checkpoint for global_step 210\n",
      "\n",
      "Time Elapased:  6.155024564000087\n",
      "\n",
      "Epoch:  211\n",
      "Training Loss:  0.8774014\n",
      "Validation Loss:  1.2424297\n",
      "\n",
      "Time Elapased:  5.7871601520000695\n",
      "\n",
      "Epoch:  212\n",
      "Training Loss:  0.8721419\n",
      "Validation Loss:  1.2470766\n",
      "\n",
      "Time Elapased:  5.8404269679995195\n",
      "\n",
      "Epoch:  213\n",
      "Training Loss:  0.86668235\n",
      "Validation Loss:  1.2474523\n",
      "\n",
      "Time Elapased:  5.861789355999463\n",
      "\n",
      "Epoch:  214\n",
      "Training Loss:  0.8628377\n",
      "Validation Loss:  1.2528348\n",
      "\n",
      "Time Elapased:  5.778377751999869\n",
      "\n",
      "Epoch:  215\n",
      "Training Loss:  0.8642226\n",
      "Validation Loss:  1.2494769\n",
      "\n",
      "Time Elapased:  5.728829078000672\n",
      "\n",
      "Epoch:  216\n",
      "Training Loss:  0.86907244\n",
      "Validation Loss:  1.2544823\n",
      "\n",
      "Time Elapased:  5.846675853999841\n",
      "\n",
      "Epoch:  217\n",
      "Training Loss:  0.87083125\n",
      "Validation Loss:  1.2571609\n",
      "\n",
      "Time Elapased:  5.87909879200015\n",
      "\n",
      "Epoch:  218\n",
      "Training Loss:  0.937461\n",
      "Validation Loss:  1.2595994\n",
      "\n",
      "Time Elapased:  5.8940598360004515\n",
      "\n",
      "Epoch:  219\n",
      "Training Loss:  0.96895707\n",
      "Validation Loss:  1.2483975\n",
      "\n",
      "Time Elapased:  5.729456185000345\n",
      "\n",
      "Epoch:  220\n",
      "Training Loss:  0.99391943\n",
      "Validation Loss:  1.2519971\n",
      "Saving Checkpoint for global_step 220\n",
      "\n",
      "Time Elapased:  6.198752518999754\n",
      "\n",
      "Epoch:  221\n",
      "Training Loss:  1.0177636\n",
      "Validation Loss:  1.2512474\n",
      "\n",
      "Time Elapased:  5.8512750710006\n",
      "\n",
      "Epoch:  222\n",
      "Training Loss:  1.0130256\n",
      "Validation Loss:  1.2465942\n",
      "\n",
      "Time Elapased:  5.98669096499998\n",
      "\n",
      "Epoch:  223\n",
      "Training Loss:  1.1059008\n",
      "Validation Loss:  1.2587962\n",
      "\n",
      "Time Elapased:  5.860450829999536\n",
      "\n",
      "Epoch:  224\n",
      "Training Loss:  1.1714425\n",
      "Validation Loss:  1.2250942\n",
      "\n",
      "Time Elapased:  5.953002629999901\n",
      "\n",
      "Epoch:  225\n",
      "Training Loss:  1.0960382\n",
      "Validation Loss:  1.2300704\n",
      "\n",
      "Time Elapased:  6.051905983000324\n",
      "\n",
      "Epoch:  226\n",
      "Training Loss:  1.0848615\n",
      "Validation Loss:  1.2242879\n",
      "\n",
      "Time Elapased:  5.907273036000333\n",
      "\n",
      "Epoch:  227\n",
      "Training Loss:  1.0992012\n",
      "Validation Loss:  1.22295\n",
      "\n",
      "Time Elapased:  6.06945102499958\n",
      "\n",
      "Epoch:  228\n",
      "Training Loss:  1.0819433\n",
      "Validation Loss:  1.2091217\n",
      "\n",
      "Time Elapased:  5.895506460999968\n",
      "\n",
      "Epoch:  229\n",
      "Training Loss:  1.063528\n",
      "Validation Loss:  1.2100818\n",
      "\n",
      "Time Elapased:  5.798959200999889\n",
      "\n",
      "Epoch:  230\n",
      "Training Loss:  1.0543277\n",
      "Validation Loss:  1.2085671\n",
      "Saving Checkpoint for global_step 230\n",
      "\n",
      "Time Elapased:  6.211995134999597\n",
      "\n",
      "Epoch:  231\n",
      "Training Loss:  1.040235\n",
      "Validation Loss:  1.2073907\n",
      "\n",
      "Time Elapased:  5.619806594999318\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  232\n",
      "Training Loss:  1.0246015\n",
      "Validation Loss:  1.203646\n",
      "\n",
      "Time Elapased:  5.835512875999484\n",
      "\n",
      "Epoch:  233\n",
      "Training Loss:  1.0110682\n",
      "Validation Loss:  1.2141509\n",
      "\n",
      "Time Elapased:  5.872164028999578\n",
      "\n",
      "Epoch:  234\n",
      "Training Loss:  0.9942828\n",
      "Validation Loss:  1.2052824\n",
      "\n",
      "Time Elapased:  5.891425026000434\n",
      "\n",
      "Epoch:  235\n",
      "Training Loss:  0.98151875\n",
      "Validation Loss:  1.2111766\n",
      "\n",
      "Time Elapased:  5.867962274000092\n",
      "\n",
      "Epoch:  236\n",
      "Training Loss:  0.96865124\n",
      "Validation Loss:  1.212888\n",
      "\n",
      "Time Elapased:  5.922088010000152\n",
      "\n",
      "Epoch:  237\n",
      "Training Loss:  0.9599912\n",
      "Validation Loss:  1.2158647\n",
      "\n",
      "Time Elapased:  5.9794999309997365\n",
      "\n",
      "Epoch:  238\n",
      "Training Loss:  0.9511332\n",
      "Validation Loss:  1.2154579\n",
      "\n",
      "Time Elapased:  5.9658316340000965\n",
      "\n",
      "Epoch:  239\n",
      "Training Loss:  0.94206417\n",
      "Validation Loss:  1.2174399\n",
      "\n",
      "Time Elapased:  5.946629297999607\n",
      "\n",
      "Epoch:  240\n",
      "Training Loss:  0.93573165\n",
      "Validation Loss:  1.2209558\n",
      "Saving Checkpoint for global_step 240\n",
      "\n",
      "Time Elapased:  6.23345393999989\n",
      "\n",
      "Epoch:  241\n",
      "Training Loss:  0.9292364\n",
      "Validation Loss:  1.2216618\n",
      "\n",
      "Time Elapased:  6.026601981000567\n",
      "\n",
      "Epoch:  242\n",
      "Training Loss:  0.9218487\n",
      "Validation Loss:  1.2248099\n",
      "\n",
      "Time Elapased:  5.966093289999662\n",
      "\n",
      "Epoch:  243\n",
      "Training Loss:  0.9201651\n",
      "Validation Loss:  1.2257428\n",
      "\n",
      "Time Elapased:  5.925687244999608\n",
      "\n",
      "Epoch:  244\n",
      "Training Loss:  0.9164806\n",
      "Validation Loss:  1.229254\n",
      "\n",
      "Time Elapased:  6.082005102000039\n",
      "\n",
      "Epoch:  245\n",
      "Training Loss:  0.9164487\n",
      "Validation Loss:  1.2302899\n",
      "\n",
      "Time Elapased:  6.075313948000257\n",
      "\n",
      "Epoch:  246\n",
      "Training Loss:  0.90968376\n",
      "Validation Loss:  1.2365047\n",
      "\n",
      "Time Elapased:  5.956825683999341\n",
      "\n",
      "Epoch:  247\n",
      "Training Loss:  0.9023973\n",
      "Validation Loss:  1.2402161\n",
      "\n",
      "Time Elapased:  5.8871529110001575\n",
      "\n",
      "Epoch:  248\n",
      "Training Loss:  0.90469784\n",
      "Validation Loss:  1.2360444\n",
      "\n",
      "Time Elapased:  5.942766879000374\n",
      "\n",
      "Epoch:  249\n",
      "Training Loss:  0.90073746\n",
      "Validation Loss:  1.2392963\n",
      "\n",
      "Time Elapased:  5.974820878000173\n",
      "\n",
      "Epoch:  250\n",
      "Training Loss:  0.8948536\n",
      "Validation Loss:  1.2394217\n",
      "Saving Checkpoint for global_step 250\n",
      "\n",
      "Time Elapased:  6.233133814000212\n",
      "\n",
      "Epoch:  251\n",
      "Training Loss:  0.88862574\n",
      "Validation Loss:  1.2478985\n",
      "\n",
      "Time Elapased:  5.92513473300005\n",
      "\n",
      "Epoch:  252\n",
      "Training Loss:  0.88526183\n",
      "Validation Loss:  1.24439\n",
      "\n",
      "Time Elapased:  5.958005440999841\n",
      "\n",
      "Epoch:  253\n",
      "Training Loss:  0.8717229\n",
      "Validation Loss:  1.2527311\n",
      "\n",
      "Time Elapased:  5.870090231000177\n",
      "\n",
      "Epoch:  254\n",
      "Training Loss:  0.8650628\n",
      "Validation Loss:  1.2507938\n",
      "\n",
      "Time Elapased:  5.820300829000189\n",
      "\n",
      "Epoch:  255\n",
      "Training Loss:  0.86067545\n",
      "Validation Loss:  1.2535696\n",
      "\n",
      "Time Elapased:  5.872780362999947\n",
      "\n",
      "Epoch:  256\n",
      "Training Loss:  0.85296595\n",
      "Validation Loss:  1.25976\n",
      "\n",
      "Time Elapased:  5.929150771999957\n",
      "\n",
      "Epoch:  257\n",
      "Training Loss:  0.8511165\n",
      "Validation Loss:  1.2563239\n",
      "\n",
      "Time Elapased:  5.898291507999602\n",
      "\n",
      "Epoch:  258\n",
      "Training Loss:  0.84611136\n",
      "Validation Loss:  1.2603384\n",
      "\n",
      "Time Elapased:  5.965899489000549\n",
      "\n",
      "Epoch:  259\n",
      "Training Loss:  0.8397695\n",
      "Validation Loss:  1.2664399\n",
      "\n",
      "Time Elapased:  6.000129729000037\n",
      "\n",
      "Epoch:  260\n",
      "Training Loss:  0.83414865\n",
      "Validation Loss:  1.2632926\n",
      "Saving Checkpoint for global_step 260\n",
      "\n",
      "Time Elapased:  6.354155695999907\n",
      "\n",
      "Epoch:  261\n",
      "Training Loss:  0.8293811\n",
      "Validation Loss:  1.2649179\n",
      "\n",
      "Time Elapased:  5.97504859000037\n",
      "\n",
      "Epoch:  262\n",
      "Training Loss:  0.82828313\n",
      "Validation Loss:  1.2640364\n",
      "\n",
      "Time Elapased:  5.971694195000055\n",
      "\n",
      "Epoch:  263\n",
      "Training Loss:  0.82692164\n",
      "Validation Loss:  1.2705957\n",
      "\n",
      "Time Elapased:  5.981637023000076\n",
      "\n",
      "Epoch:  264\n",
      "Training Loss:  0.8189366\n",
      "Validation Loss:  1.2707901\n",
      "\n",
      "Time Elapased:  5.914602918000128\n",
      "\n",
      "Epoch:  265\n",
      "Training Loss:  0.8196879\n",
      "Validation Loss:  1.2716875\n",
      "\n",
      "Time Elapased:  6.108586336000371\n",
      "\n",
      "Epoch:  266\n",
      "Training Loss:  0.8164676\n",
      "Validation Loss:  1.2743101\n",
      "\n",
      "Time Elapased:  5.991715872999521\n",
      "\n",
      "Epoch:  267\n",
      "Training Loss:  0.81758904\n",
      "Validation Loss:  1.2729106\n",
      "\n",
      "Time Elapased:  5.862257133000639\n",
      "\n",
      "Epoch:  268\n",
      "Training Loss:  0.8147895\n",
      "Validation Loss:  1.2785645\n",
      "\n",
      "Time Elapased:  6.033981837000283\n",
      "\n",
      "Epoch:  269\n",
      "Training Loss:  0.8076631\n",
      "Validation Loss:  1.2808478\n",
      "\n",
      "Time Elapased:  5.895567348999975\n",
      "\n",
      "Epoch:  270\n",
      "Training Loss:  0.807431\n",
      "Validation Loss:  1.2789528\n",
      "Saving Checkpoint for global_step 270\n",
      "\n",
      "Time Elapased:  6.342862245000106\n",
      "\n",
      "Epoch:  271\n",
      "Training Loss:  0.79947734\n",
      "Validation Loss:  1.2847621\n",
      "\n",
      "Time Elapased:  5.998988588999964\n",
      "\n",
      "Epoch:  272\n",
      "Training Loss:  0.79836065\n",
      "Validation Loss:  1.2838824\n",
      "\n",
      "Time Elapased:  5.837456897000266\n",
      "\n",
      "Epoch:  273\n",
      "Training Loss:  0.7937675\n",
      "Validation Loss:  1.2893407\n",
      "\n",
      "Time Elapased:  5.677258119999351\n",
      "\n",
      "Epoch:  274\n",
      "Training Loss:  0.7922646\n",
      "Validation Loss:  1.2878506\n",
      "\n",
      "Time Elapased:  5.845885931999874\n",
      "\n",
      "Epoch:  275\n",
      "Training Loss:  0.78352606\n",
      "Validation Loss:  1.2876892\n",
      "\n",
      "Time Elapased:  5.8771060750004835\n",
      "\n",
      "Epoch:  276\n",
      "Training Loss:  0.7809943\n",
      "Validation Loss:  1.2933877\n",
      "\n",
      "Time Elapased:  6.0927713029996085\n",
      "\n",
      "Epoch:  277\n",
      "Training Loss:  0.77764934\n",
      "Validation Loss:  1.2942352\n",
      "\n",
      "Time Elapased:  5.9160598789994765\n",
      "\n",
      "Epoch:  278\n",
      "Training Loss:  0.7759653\n",
      "Validation Loss:  1.292227\n",
      "\n",
      "Time Elapased:  5.9480455909997545\n",
      "\n",
      "Epoch:  279\n",
      "Training Loss:  0.7721373\n",
      "Validation Loss:  1.3003569\n",
      "\n",
      "Time Elapased:  5.847961808000036\n",
      "\n",
      "Epoch:  280\n",
      "Training Loss:  0.77391857\n",
      "Validation Loss:  1.2989088\n",
      "Saving Checkpoint for global_step 280\n",
      "\n",
      "Time Elapased:  6.304138548000083\n",
      "\n",
      "Epoch:  281\n",
      "Training Loss:  0.769112\n",
      "Validation Loss:  1.2974508\n",
      "\n",
      "Time Elapased:  5.94856038800026\n",
      "\n",
      "Epoch:  282\n",
      "Training Loss:  0.76804954\n",
      "Validation Loss:  1.3058949\n",
      "\n",
      "Time Elapased:  5.953108553000675\n",
      "\n",
      "Epoch:  283\n",
      "Training Loss:  0.7633802\n",
      "Validation Loss:  1.3042663\n",
      "\n",
      "Time Elapased:  5.925008709999929\n",
      "\n",
      "Epoch:  284\n",
      "Training Loss:  0.7602349\n",
      "Validation Loss:  1.3017623\n",
      "\n",
      "Time Elapased:  5.979125615000157\n",
      "\n",
      "Epoch:  285\n",
      "Training Loss:  0.75544333\n",
      "Validation Loss:  1.3089304\n",
      "\n",
      "Time Elapased:  5.907721892000154\n",
      "\n",
      "Epoch:  286\n",
      "Training Loss:  0.75424564\n",
      "Validation Loss:  1.3099234\n",
      "\n",
      "Time Elapased:  5.928584430999763\n",
      "\n",
      "Epoch:  287\n",
      "Training Loss:  0.75324583\n",
      "Validation Loss:  1.3110709\n",
      "\n",
      "Time Elapased:  6.026384185000097\n",
      "\n",
      "Epoch:  288\n",
      "Training Loss:  0.7467843\n",
      "Validation Loss:  1.3125082\n",
      "\n",
      "Time Elapased:  6.021914947999903\n",
      "\n",
      "Epoch:  289\n",
      "Training Loss:  0.7429817\n",
      "Validation Loss:  1.3144636\n",
      "\n",
      "Time Elapased:  5.959856477999892\n",
      "\n",
      "Epoch:  290\n",
      "Training Loss:  0.7388408\n",
      "Validation Loss:  1.3152617\n",
      "Saving Checkpoint for global_step 290\n",
      "\n",
      "Time Elapased:  6.318487986999571\n",
      "\n",
      "Epoch:  291\n",
      "Training Loss:  0.7368893\n",
      "Validation Loss:  1.3189193\n",
      "\n",
      "Time Elapased:  5.901505339999858\n",
      "\n",
      "Epoch:  292\n",
      "Training Loss:  0.7358994\n",
      "Validation Loss:  1.3195955\n",
      "\n",
      "Time Elapased:  5.85945652800001\n",
      "\n",
      "Epoch:  293\n",
      "Training Loss:  0.73373026\n",
      "Validation Loss:  1.3208072\n",
      "\n",
      "Time Elapased:  5.795599408999806\n",
      "\n",
      "Epoch:  294\n",
      "Training Loss:  0.73297954\n",
      "Validation Loss:  1.3234088\n",
      "\n",
      "Time Elapased:  5.75267877799979\n",
      "\n",
      "Epoch:  295\n",
      "Training Loss:  0.7306917\n",
      "Validation Loss:  1.3255641\n",
      "\n",
      "Time Elapased:  6.007384763000118\n",
      "\n",
      "Epoch:  296\n",
      "Training Loss:  0.72657\n",
      "Validation Loss:  1.3239026\n",
      "\n",
      "Time Elapased:  5.953874444000576\n",
      "\n",
      "Epoch:  297\n",
      "Training Loss:  0.72393775\n",
      "Validation Loss:  1.3293204\n",
      "\n",
      "Time Elapased:  6.122386030000598\n",
      "\n",
      "Epoch:  298\n",
      "Training Loss:  0.7181534\n",
      "Validation Loss:  1.3321531\n",
      "\n",
      "Time Elapased:  5.904643926999597\n",
      "\n",
      "Epoch:  299\n",
      "Training Loss:  0.71868324\n",
      "Validation Loss:  1.3286529\n",
      "\n",
      "Time Elapased:  5.869700102999559\n",
      "\n",
      "Epoch:  300\n",
      "Training Loss:  0.7123804\n",
      "Validation Loss:  1.3354124\n",
      "Saving Checkpoint for global_step 300\n",
      "\n",
      "Time Elapased:  6.407809203999932\n",
      "\n",
      "Epoch:  301\n",
      "Training Loss:  0.7122066\n",
      "Validation Loss:  1.3330021\n",
      "\n",
      "Time Elapased:  5.899057253000137\n",
      "\n",
      "Epoch:  302\n",
      "Training Loss:  0.7094617\n",
      "Validation Loss:  1.3391883\n",
      "\n",
      "Time Elapased:  5.889305264000541\n",
      "\n",
      "Epoch:  303\n",
      "Training Loss:  0.7082635\n",
      "Validation Loss:  1.3386269\n",
      "\n",
      "Time Elapased:  5.971086096999898\n",
      "\n",
      "Epoch:  304\n",
      "Training Loss:  0.7041085\n",
      "Validation Loss:  1.3374183\n",
      "\n",
      "Time Elapased:  6.056426430000101\n",
      "\n",
      "Epoch:  305\n",
      "Training Loss:  0.70144\n",
      "Validation Loss:  1.3439219\n",
      "\n",
      "Time Elapased:  5.9986501000003045\n",
      "\n",
      "Epoch:  306\n",
      "Training Loss:  0.7005372\n",
      "Validation Loss:  1.3470757\n",
      "\n",
      "Time Elapased:  5.951436174000264\n",
      "\n",
      "Epoch:  307\n",
      "Training Loss:  0.6990292\n",
      "Validation Loss:  1.3449583\n",
      "\n",
      "Time Elapased:  5.9690399889996115\n",
      "\n",
      "Epoch:  308\n",
      "Training Loss:  0.696334\n",
      "Validation Loss:  1.3460486\n",
      "\n",
      "Time Elapased:  6.00816601599945\n",
      "\n",
      "Epoch:  309\n",
      "Training Loss:  0.6967125\n",
      "Validation Loss:  1.3493824\n",
      "\n",
      "Time Elapased:  6.024024970999562\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  310\n",
      "Training Loss:  0.69915956\n",
      "Validation Loss:  1.3529732\n",
      "Saving Checkpoint for global_step 310\n",
      "\n",
      "Time Elapased:  6.353069824000158\n",
      "\n",
      "Epoch:  311\n",
      "Training Loss:  0.689918\n",
      "Validation Loss:  1.3514082\n",
      "\n",
      "Time Elapased:  5.929295946999446\n",
      "\n",
      "Epoch:  312\n",
      "Training Loss:  0.6894162\n",
      "Validation Loss:  1.3536363\n",
      "\n",
      "Time Elapased:  5.913882948999344\n",
      "\n",
      "Epoch:  313\n",
      "Training Loss:  0.6928287\n",
      "Validation Loss:  1.3565347\n",
      "\n",
      "Time Elapased:  5.935443428000326\n",
      "\n",
      "Epoch:  314\n",
      "Training Loss:  0.6922823\n",
      "Validation Loss:  1.3583763\n",
      "\n",
      "Time Elapased:  5.7334762859991315\n",
      "\n",
      "Epoch:  315\n",
      "Training Loss:  0.68502915\n",
      "Validation Loss:  1.3570921\n",
      "\n",
      "Time Elapased:  5.884842737999861\n",
      "\n",
      "Epoch:  316\n",
      "Training Loss:  0.6853718\n",
      "Validation Loss:  1.3620443\n",
      "\n",
      "Time Elapased:  5.916462743999546\n",
      "\n",
      "Epoch:  317\n",
      "Training Loss:  0.6797564\n",
      "Validation Loss:  1.3613493\n",
      "\n",
      "Time Elapased:  6.138159335999262\n",
      "\n",
      "Epoch:  318\n",
      "Training Loss:  0.67884004\n",
      "Validation Loss:  1.3664856\n",
      "\n",
      "Time Elapased:  5.970263565000096\n",
      "\n",
      "Epoch:  319\n",
      "Training Loss:  0.6776858\n",
      "Validation Loss:  1.3653755\n",
      "\n",
      "Time Elapased:  6.065486308999425\n",
      "\n",
      "Epoch:  320\n",
      "Training Loss:  0.681439\n",
      "Validation Loss:  1.3694347\n",
      "Saving Checkpoint for global_step 320\n",
      "\n",
      "Time Elapased:  6.442426152000735\n",
      "\n",
      "Epoch:  321\n",
      "Training Loss:  0.67649055\n",
      "Validation Loss:  1.3681244\n",
      "\n",
      "Time Elapased:  6.061644532000173\n",
      "\n",
      "Epoch:  322\n",
      "Training Loss:  0.6713762\n",
      "Validation Loss:  1.3696147\n",
      "\n",
      "Time Elapased:  6.0490214129995366\n",
      "\n",
      "Epoch:  323\n",
      "Training Loss:  0.6684413\n",
      "Validation Loss:  1.3754421\n",
      "\n",
      "Time Elapased:  6.057535218000339\n",
      "\n",
      "Epoch:  324\n",
      "Training Loss:  0.66706675\n",
      "Validation Loss:  1.3735136\n",
      "\n",
      "Time Elapased:  5.968075581000448\n",
      "\n",
      "Epoch:  325\n",
      "Training Loss:  0.666688\n",
      "Validation Loss:  1.3728662\n",
      "\n",
      "Time Elapased:  6.004486227999223\n",
      "\n",
      "Epoch:  326\n",
      "Training Loss:  0.66741174\n",
      "Validation Loss:  1.3813143\n",
      "\n",
      "Time Elapased:  6.1033394709993445\n",
      "\n",
      "Epoch:  327\n",
      "Training Loss:  0.6659679\n",
      "Validation Loss:  1.3796825\n",
      "\n",
      "Time Elapased:  6.009689489999801\n",
      "\n",
      "Epoch:  328\n",
      "Training Loss:  0.66086674\n",
      "Validation Loss:  1.3795502\n",
      "\n",
      "Time Elapased:  6.035292990999551\n",
      "\n",
      "Epoch:  329\n",
      "Training Loss:  0.6599649\n",
      "Validation Loss:  1.3836687\n",
      "\n",
      "Time Elapased:  6.211625457000082\n",
      "\n",
      "Epoch:  330\n",
      "Training Loss:  0.659186\n",
      "Validation Loss:  1.386404\n",
      "Saving Checkpoint for global_step 330\n",
      "\n",
      "Time Elapased:  6.373450553999646\n",
      "\n",
      "Epoch:  331\n",
      "Training Loss:  0.65748817\n",
      "Validation Loss:  1.387917\n",
      "\n",
      "Time Elapased:  5.896830254000633\n",
      "\n",
      "Epoch:  332\n",
      "Training Loss:  0.65702814\n",
      "Validation Loss:  1.3876226\n",
      "\n",
      "Time Elapased:  6.040026416999353\n",
      "\n",
      "Epoch:  333\n",
      "Training Loss:  0.65365016\n",
      "Validation Loss:  1.3893509\n",
      "\n",
      "Time Elapased:  6.117179102000591\n",
      "\n",
      "Epoch:  334\n",
      "Training Loss:  0.6495056\n",
      "Validation Loss:  1.392942\n",
      "\n",
      "Time Elapased:  6.068989123999927\n",
      "\n",
      "Epoch:  335\n",
      "Training Loss:  0.6506588\n",
      "Validation Loss:  1.3942372\n",
      "\n",
      "Time Elapased:  5.9790183040004194\n",
      "\n",
      "Epoch:  336\n",
      "Training Loss:  0.64544874\n",
      "Validation Loss:  1.3927038\n",
      "\n",
      "Time Elapased:  5.994032646999585\n",
      "\n",
      "Epoch:  337\n",
      "Training Loss:  0.63953686\n",
      "Validation Loss:  1.3952159\n",
      "\n",
      "Time Elapased:  5.992335425999954\n",
      "\n",
      "Epoch:  338\n",
      "Training Loss:  0.63505065\n",
      "Validation Loss:  1.3996625\n",
      "\n",
      "Time Elapased:  5.968857855999886\n",
      "\n",
      "Epoch:  339\n",
      "Training Loss:  0.6349865\n",
      "Validation Loss:  1.3993828\n",
      "\n",
      "Time Elapased:  6.060534475000168\n",
      "\n",
      "Epoch:  340\n",
      "Training Loss:  0.63403404\n",
      "Validation Loss:  1.40042\n",
      "Saving Checkpoint for global_step 340\n",
      "\n",
      "Time Elapased:  6.414347937999992\n",
      "\n",
      "Epoch:  341\n",
      "Training Loss:  0.6289989\n",
      "Validation Loss:  1.4021043\n",
      "\n",
      "Time Elapased:  6.026499497000259\n",
      "\n",
      "Epoch:  342\n",
      "Training Loss:  0.62923\n",
      "Validation Loss:  1.401963\n",
      "\n",
      "Time Elapased:  6.011238482999943\n",
      "\n",
      "Epoch:  343\n",
      "Training Loss:  0.6255933\n",
      "Validation Loss:  1.4066043\n",
      "\n",
      "Time Elapased:  5.993596532000083\n",
      "\n",
      "Epoch:  344\n",
      "Training Loss:  0.6258539\n",
      "Validation Loss:  1.4071631\n",
      "\n",
      "Time Elapased:  5.917568015999677\n",
      "\n",
      "Epoch:  345\n",
      "Training Loss:  0.62452245\n",
      "Validation Loss:  1.4079666\n",
      "\n",
      "Time Elapased:  5.909853905000091\n",
      "\n",
      "Epoch:  346\n",
      "Training Loss:  0.6250195\n",
      "Validation Loss:  1.4105945\n",
      "\n",
      "Time Elapased:  5.999358840000241\n",
      "\n",
      "Epoch:  347\n",
      "Training Loss:  0.62361324\n",
      "Validation Loss:  1.4136202\n",
      "\n",
      "Time Elapased:  5.904556989999946\n",
      "\n",
      "Epoch:  348\n",
      "Training Loss:  0.6224108\n",
      "Validation Loss:  1.4111726\n",
      "\n",
      "Time Elapased:  5.908872816999974\n",
      "\n",
      "Epoch:  349\n",
      "Training Loss:  0.61956304\n",
      "Validation Loss:  1.4145935\n",
      "\n",
      "Time Elapased:  6.011050566000449\n",
      "\n",
      "Epoch:  350\n",
      "Training Loss:  0.6181451\n",
      "Validation Loss:  1.4147615\n",
      "Saving Checkpoint for global_step 350\n",
      "\n",
      "Time Elapased:  6.295419510000102\n",
      "\n",
      "Epoch:  351\n",
      "Training Loss:  0.616089\n",
      "Validation Loss:  1.4193659\n",
      "\n",
      "Time Elapased:  6.008860666999681\n",
      "\n",
      "Epoch:  352\n",
      "Training Loss:  0.6156958\n",
      "Validation Loss:  1.4205878\n",
      "\n",
      "Time Elapased:  5.959926271999393\n",
      "\n",
      "Epoch:  353\n",
      "Training Loss:  0.61503386\n",
      "Validation Loss:  1.4174633\n",
      "\n",
      "Time Elapased:  6.04869072099973\n",
      "\n",
      "Epoch:  354\n",
      "Training Loss:  0.6118734\n",
      "Validation Loss:  1.4281723\n",
      "\n",
      "Time Elapased:  5.926544938000006\n",
      "\n",
      "Epoch:  355\n",
      "Training Loss:  0.6117456\n",
      "Validation Loss:  1.4261621\n",
      "\n",
      "Time Elapased:  6.070294082000146\n",
      "\n",
      "Epoch:  356\n",
      "Training Loss:  0.6132349\n",
      "Validation Loss:  1.421294\n",
      "\n",
      "Time Elapased:  5.961432791999869\n",
      "\n",
      "Epoch:  357\n",
      "Training Loss:  0.61405516\n",
      "Validation Loss:  1.4261128\n",
      "\n",
      "Time Elapased:  5.974927059000038\n",
      "\n",
      "Epoch:  358\n",
      "Training Loss:  0.61666733\n",
      "Validation Loss:  1.4285191\n",
      "\n",
      "Time Elapased:  5.995276564999585\n",
      "\n",
      "Epoch:  359\n",
      "Training Loss:  0.6123927\n",
      "Validation Loss:  1.4296432\n",
      "\n",
      "Time Elapased:  5.94650757799991\n",
      "\n",
      "Epoch:  360\n",
      "Training Loss:  0.61060095\n",
      "Validation Loss:  1.4340091\n",
      "Saving Checkpoint for global_step 360\n",
      "\n",
      "Time Elapased:  6.381418504000067\n",
      "\n",
      "Epoch:  361\n",
      "Training Loss:  0.604791\n",
      "Validation Loss:  1.4327085\n",
      "\n",
      "Time Elapased:  6.05004546300006\n",
      "\n",
      "Epoch:  362\n",
      "Training Loss:  0.600566\n",
      "Validation Loss:  1.4356072\n",
      "\n",
      "Time Elapased:  5.939515018999373\n",
      "\n",
      "Epoch:  363\n",
      "Training Loss:  0.5992991\n",
      "Validation Loss:  1.4390452\n",
      "\n",
      "Time Elapased:  5.956662438999956\n",
      "\n",
      "Epoch:  364\n",
      "Training Loss:  0.5993623\n",
      "Validation Loss:  1.4392254\n",
      "\n",
      "Time Elapased:  5.882711444999586\n",
      "\n",
      "Epoch:  365\n",
      "Training Loss:  0.5980283\n",
      "Validation Loss:  1.4398123\n",
      "\n",
      "Time Elapased:  5.920677697999963\n",
      "\n",
      "Epoch:  366\n",
      "Training Loss:  0.59205437\n",
      "Validation Loss:  1.4368224\n",
      "\n",
      "Time Elapased:  5.918207194999923\n",
      "\n",
      "Epoch:  367\n",
      "Training Loss:  0.5918771\n",
      "Validation Loss:  1.4433765\n",
      "\n",
      "Time Elapased:  6.069742202999805\n",
      "\n",
      "Epoch:  368\n",
      "Training Loss:  0.5893481\n",
      "Validation Loss:  1.4450024\n",
      "\n",
      "Time Elapased:  6.043529268000384\n",
      "\n",
      "Epoch:  369\n",
      "Training Loss:  0.5897712\n",
      "Validation Loss:  1.4439932\n",
      "\n",
      "Time Elapased:  5.916357451999829\n",
      "\n",
      "Epoch:  370\n",
      "Training Loss:  0.5878465\n",
      "Validation Loss:  1.4466138\n",
      "Saving Checkpoint for global_step 370\n",
      "\n",
      "Time Elapased:  6.314691587999732\n",
      "\n",
      "Epoch:  371\n",
      "Training Loss:  0.59002554\n",
      "Validation Loss:  1.4531407\n",
      "\n",
      "Time Elapased:  5.913063181000325\n",
      "\n",
      "Epoch:  372\n",
      "Training Loss:  0.5920037\n",
      "Validation Loss:  1.4465387\n",
      "\n",
      "Time Elapased:  6.081772616999842\n",
      "\n",
      "Epoch:  373\n",
      "Training Loss:  0.60234916\n",
      "Validation Loss:  1.4526407\n",
      "\n",
      "Time Elapased:  5.956609985999421\n",
      "\n",
      "Epoch:  374\n",
      "Training Loss:  0.6005641\n",
      "Validation Loss:  1.4543958\n",
      "\n",
      "Time Elapased:  5.934985294999933\n",
      "\n",
      "Epoch:  375\n",
      "Training Loss:  0.6000874\n",
      "Validation Loss:  1.4480305\n",
      "\n",
      "Time Elapased:  5.909855565999351\n",
      "\n",
      "Epoch:  376\n",
      "Training Loss:  0.5984652\n",
      "Validation Loss:  1.4551718\n",
      "\n",
      "Time Elapased:  5.9229378019999785\n",
      "\n",
      "Epoch:  377\n",
      "Training Loss:  0.5908998\n",
      "Validation Loss:  1.4569069\n",
      "\n",
      "Time Elapased:  6.016323569000633\n",
      "\n",
      "Epoch:  378\n",
      "Training Loss:  0.58784235\n",
      "Validation Loss:  1.4510546\n",
      "\n",
      "Time Elapased:  5.933874411999568\n",
      "\n",
      "Epoch:  379\n",
      "Training Loss:  0.58787537\n",
      "Validation Loss:  1.4629655\n",
      "\n",
      "Time Elapased:  6.021502491000319\n",
      "\n",
      "Epoch:  380\n",
      "Training Loss:  0.585564\n",
      "Validation Loss:  1.4579899\n",
      "Saving Checkpoint for global_step 380\n",
      "\n",
      "Time Elapased:  6.467279943999529\n",
      "\n",
      "Epoch:  381\n",
      "Training Loss:  0.57676303\n",
      "Validation Loss:  1.463062\n",
      "\n",
      "Time Elapased:  6.248497432000477\n",
      "\n",
      "Epoch:  382\n",
      "Training Loss:  0.57961214\n",
      "Validation Loss:  1.4618423\n",
      "\n",
      "Time Elapased:  6.005209506000028\n",
      "\n",
      "Epoch:  383\n",
      "Training Loss:  0.575641\n",
      "Validation Loss:  1.4683292\n",
      "\n",
      "Time Elapased:  6.139208757000233\n",
      "\n",
      "Epoch:  384\n",
      "Training Loss:  0.5738737\n",
      "Validation Loss:  1.4671893\n",
      "\n",
      "Time Elapased:  6.04852262500026\n",
      "\n",
      "Epoch:  385\n",
      "Training Loss:  0.5702095\n",
      "Validation Loss:  1.4688733\n",
      "\n",
      "Time Elapased:  5.951272577000054\n",
      "\n",
      "Epoch:  386\n",
      "Training Loss:  0.5702446\n",
      "Validation Loss:  1.4686741\n",
      "\n",
      "Time Elapased:  5.911578718999408\n",
      "\n",
      "Epoch:  387\n",
      "Training Loss:  0.56801826\n",
      "Validation Loss:  1.4727578\n",
      "\n",
      "Time Elapased:  5.922846690999904\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  388\n",
      "Training Loss:  0.5658481\n",
      "Validation Loss:  1.4720277\n",
      "\n",
      "Time Elapased:  6.029130785000234\n",
      "\n",
      "Epoch:  389\n",
      "Training Loss:  0.56542826\n",
      "Validation Loss:  1.4734447\n",
      "\n",
      "Time Elapased:  5.910397575000388\n",
      "\n",
      "Epoch:  390\n",
      "Training Loss:  0.5684037\n",
      "Validation Loss:  1.4767586\n",
      "Saving Checkpoint for global_step 390\n",
      "\n",
      "Time Elapased:  6.298459900000125\n",
      "\n",
      "Epoch:  391\n",
      "Training Loss:  0.5633911\n",
      "Validation Loss:  1.4774044\n",
      "\n",
      "Time Elapased:  5.881966282999201\n",
      "\n",
      "Epoch:  392\n",
      "Training Loss:  0.5590972\n",
      "Validation Loss:  1.4796481\n",
      "\n",
      "Time Elapased:  6.026799240999935\n",
      "\n",
      "Epoch:  393\n",
      "Training Loss:  0.5591944\n",
      "Validation Loss:  1.4811873\n",
      "\n",
      "Time Elapased:  5.9910072510001555\n",
      "\n",
      "Epoch:  394\n",
      "Training Loss:  0.56127626\n",
      "Validation Loss:  1.4825389\n",
      "\n",
      "Time Elapased:  6.071819582999524\n",
      "\n",
      "Epoch:  395\n",
      "Training Loss:  0.5634375\n",
      "Validation Loss:  1.4816172\n",
      "\n",
      "Time Elapased:  5.953082380999149\n",
      "\n",
      "Epoch:  396\n",
      "Training Loss:  0.56078583\n",
      "Validation Loss:  1.4815663\n",
      "\n",
      "Time Elapased:  6.028373017999911\n",
      "\n",
      "Epoch:  397\n",
      "Training Loss:  0.5591532\n",
      "Validation Loss:  1.4871804\n",
      "\n",
      "Time Elapased:  5.821822871999757\n",
      "\n",
      "Epoch:  398\n",
      "Training Loss:  0.5613936\n",
      "Validation Loss:  1.4816655\n",
      "\n",
      "Time Elapased:  6.077582574999724\n",
      "\n",
      "Epoch:  399\n",
      "Training Loss:  0.55776733\n",
      "Validation Loss:  1.4874499\n",
      "\n",
      "Time Elapased:  5.975294436000695\n",
      "\n",
      "Epoch:  400\n",
      "Training Loss:  0.5553145\n",
      "Validation Loss:  1.4900074\n",
      "Saving Checkpoint for global_step 400\n",
      "\n",
      "Time Elapased:  6.322925243999634\n",
      "\n",
      "Epoch:  401\n",
      "Training Loss:  0.5600176\n",
      "Validation Loss:  1.4920692\n",
      "\n",
      "Time Elapased:  5.948993880999296\n",
      "\n",
      "Epoch:  402\n",
      "Training Loss:  0.55836123\n",
      "Validation Loss:  1.4923465\n",
      "\n",
      "Time Elapased:  5.957448397999542\n",
      "\n",
      "Epoch:  403\n",
      "Training Loss:  0.5552848\n",
      "Validation Loss:  1.4925625\n",
      "\n",
      "Time Elapased:  6.054523066000002\n",
      "\n",
      "Epoch:  404\n",
      "Training Loss:  0.55293113\n",
      "Validation Loss:  1.4953806\n",
      "\n",
      "Time Elapased:  6.006651431999671\n",
      "\n",
      "Epoch:  405\n",
      "Training Loss:  0.5497358\n",
      "Validation Loss:  1.495826\n",
      "\n",
      "Time Elapased:  5.922017581999171\n",
      "\n",
      "Epoch:  406\n",
      "Training Loss:  0.55170393\n",
      "Validation Loss:  1.5021074\n",
      "\n",
      "Time Elapased:  6.057748676000301\n",
      "\n",
      "Epoch:  407\n",
      "Training Loss:  0.5477112\n",
      "Validation Loss:  1.5004554\n",
      "\n",
      "Time Elapased:  5.996274337000614\n",
      "\n",
      "Epoch:  408\n",
      "Training Loss:  0.54911786\n",
      "Validation Loss:  1.5003064\n",
      "\n",
      "Time Elapased:  5.990745768999659\n",
      "\n",
      "Epoch:  409\n",
      "Training Loss:  0.55359447\n",
      "Validation Loss:  1.498024\n",
      "\n",
      "Time Elapased:  6.026494625999476\n",
      "\n",
      "Epoch:  410\n",
      "Training Loss:  0.555071\n",
      "Validation Loss:  1.5074725\n",
      "Saving Checkpoint for global_step 410\n",
      "\n",
      "Time Elapased:  6.201104507999844\n",
      "\n",
      "Epoch:  411\n",
      "Training Loss:  0.56328666\n",
      "Validation Loss:  1.5009903\n",
      "\n",
      "Time Elapased:  5.9141104240006825\n",
      "\n",
      "Epoch:  412\n",
      "Training Loss:  0.55984676\n",
      "Validation Loss:  1.5094094\n",
      "\n",
      "Time Elapased:  5.929133419999744\n",
      "\n",
      "Epoch:  413\n",
      "Training Loss:  0.5515232\n",
      "Validation Loss:  1.506994\n",
      "\n",
      "Time Elapased:  5.878068177999921\n",
      "\n",
      "Epoch:  414\n",
      "Training Loss:  0.5579222\n",
      "Validation Loss:  1.5078555\n",
      "\n",
      "Time Elapased:  5.985735452999506\n",
      "\n",
      "Epoch:  415\n",
      "Training Loss:  0.5544951\n",
      "Validation Loss:  1.5079635\n",
      "\n",
      "Time Elapased:  6.183937126999808\n",
      "\n",
      "Epoch:  416\n",
      "Training Loss:  0.5512694\n",
      "Validation Loss:  1.5114908\n",
      "\n",
      "Time Elapased:  5.91263091699966\n",
      "\n",
      "Epoch:  417\n",
      "Training Loss:  0.54813385\n",
      "Validation Loss:  1.5114323\n",
      "\n",
      "Time Elapased:  5.868189808999887\n",
      "\n",
      "Epoch:  418\n",
      "Training Loss:  0.54697275\n",
      "Validation Loss:  1.5106834\n",
      "\n",
      "Time Elapased:  6.0051165209997635\n",
      "\n",
      "Epoch:  419\n",
      "Training Loss:  0.5510201\n",
      "Validation Loss:  1.5203058\n",
      "\n",
      "Time Elapased:  5.996924982000564\n",
      "\n",
      "Epoch:  420\n",
      "Training Loss:  0.5418447\n",
      "Validation Loss:  1.5124719\n",
      "Saving Checkpoint for global_step 420\n",
      "\n",
      "Time Elapased:  6.374929998999505\n",
      "\n",
      "Epoch:  421\n",
      "Training Loss:  0.5393778\n",
      "Validation Loss:  1.5180418\n",
      "\n",
      "Time Elapased:  6.128401515000405\n",
      "\n",
      "Epoch:  422\n",
      "Training Loss:  0.5409263\n",
      "Validation Loss:  1.5199409\n",
      "\n",
      "Time Elapased:  5.919959203000872\n",
      "\n",
      "Epoch:  423\n",
      "Training Loss:  0.53600717\n",
      "Validation Loss:  1.5198169\n",
      "\n",
      "Time Elapased:  6.035840669999743\n",
      "\n",
      "Epoch:  424\n",
      "Training Loss:  0.5340924\n",
      "Validation Loss:  1.5217559\n",
      "\n",
      "Time Elapased:  5.93866616699961\n",
      "\n",
      "Epoch:  425\n",
      "Training Loss:  0.5333288\n",
      "Validation Loss:  1.5222507\n",
      "\n",
      "Time Elapased:  5.956345392000003\n",
      "\n",
      "Epoch:  426\n",
      "Training Loss:  0.53285366\n",
      "Validation Loss:  1.5236914\n",
      "\n",
      "Time Elapased:  6.021353522000027\n",
      "\n",
      "Epoch:  427\n",
      "Training Loss:  0.5287512\n",
      "Validation Loss:  1.5237896\n",
      "\n",
      "Time Elapased:  5.994387490999543\n",
      "\n",
      "Epoch:  428\n",
      "Training Loss:  0.52939177\n",
      "Validation Loss:  1.52637\n",
      "\n",
      "Time Elapased:  5.823530665999897\n",
      "\n",
      "Epoch:  429\n",
      "Training Loss:  0.52855986\n",
      "Validation Loss:  1.5298896\n",
      "\n",
      "Time Elapased:  6.027086295000117\n",
      "\n",
      "Epoch:  430\n",
      "Training Loss:  0.52494776\n",
      "Validation Loss:  1.5266198\n",
      "Saving Checkpoint for global_step 430\n",
      "\n",
      "Time Elapased:  6.264687503999994\n",
      "\n",
      "Epoch:  431\n",
      "Training Loss:  0.5243292\n",
      "Validation Loss:  1.5266902\n",
      "\n",
      "Time Elapased:  6.199277347000134\n",
      "\n",
      "Epoch:  432\n",
      "Training Loss:  0.52456677\n",
      "Validation Loss:  1.5327053\n",
      "\n",
      "Time Elapased:  6.0828572919999715\n",
      "\n",
      "Epoch:  433\n",
      "Training Loss:  0.52478945\n",
      "Validation Loss:  1.5304904\n",
      "\n",
      "Time Elapased:  6.112428356000237\n",
      "\n",
      "Epoch:  434\n",
      "Training Loss:  0.5243468\n",
      "Validation Loss:  1.5344977\n",
      "\n",
      "Time Elapased:  5.820818674999828\n",
      "\n",
      "Epoch:  435\n",
      "Training Loss:  0.52729917\n",
      "Validation Loss:  1.5342088\n",
      "\n",
      "Time Elapased:  5.736909153999477\n",
      "\n",
      "Epoch:  436\n",
      "Training Loss:  0.5229429\n",
      "Validation Loss:  1.5337578\n",
      "\n",
      "Time Elapased:  6.084348411999599\n",
      "\n",
      "Epoch:  437\n",
      "Training Loss:  0.5433587\n",
      "Validation Loss:  1.5388441\n",
      "\n",
      "Time Elapased:  5.896408854000583\n",
      "\n",
      "Epoch:  438\n",
      "Training Loss:  0.53620285\n",
      "Validation Loss:  1.5343764\n",
      "\n",
      "Time Elapased:  6.006554685000083\n",
      "\n",
      "Epoch:  439\n",
      "Training Loss:  0.53630644\n",
      "Validation Loss:  1.5398209\n",
      "\n",
      "Time Elapased:  5.942355187999965\n",
      "\n",
      "Epoch:  440\n",
      "Training Loss:  0.5328553\n",
      "Validation Loss:  1.5404456\n",
      "Saving Checkpoint for global_step 440\n",
      "\n",
      "Time Elapased:  6.502556381999966\n",
      "\n",
      "Epoch:  441\n",
      "Training Loss:  0.52893716\n",
      "Validation Loss:  1.5399194\n",
      "\n",
      "Time Elapased:  5.862891656999636\n",
      "\n",
      "Epoch:  442\n",
      "Training Loss:  0.5234035\n",
      "Validation Loss:  1.5424\n",
      "\n",
      "Time Elapased:  6.043637069000397\n",
      "\n",
      "Epoch:  443\n",
      "Training Loss:  0.5216956\n",
      "Validation Loss:  1.5397487\n",
      "\n",
      "Time Elapased:  6.082737941999767\n",
      "\n",
      "Epoch:  444\n",
      "Training Loss:  0.5224249\n",
      "Validation Loss:  1.5452877\n",
      "\n",
      "Time Elapased:  5.969155103999583\n",
      "\n",
      "Epoch:  445\n",
      "Training Loss:  0.52291596\n",
      "Validation Loss:  1.5433247\n",
      "\n",
      "Time Elapased:  5.96466761100055\n",
      "\n",
      "Epoch:  446\n",
      "Training Loss:  0.51718307\n",
      "Validation Loss:  1.5486195\n",
      "\n",
      "Time Elapased:  6.001809916000639\n",
      "\n",
      "Epoch:  447\n",
      "Training Loss:  0.518145\n",
      "Validation Loss:  1.5480833\n",
      "\n",
      "Time Elapased:  5.9862335150000945\n",
      "\n",
      "Epoch:  448\n",
      "Training Loss:  0.52024984\n",
      "Validation Loss:  1.5497625\n",
      "\n",
      "Time Elapased:  5.913716925999324\n",
      "\n",
      "Epoch:  449\n",
      "Training Loss:  0.5217181\n",
      "Validation Loss:  1.5519283\n",
      "\n",
      "Time Elapased:  5.766670709999744\n",
      "\n",
      "Epoch:  450\n",
      "Training Loss:  0.519246\n",
      "Validation Loss:  1.551826\n",
      "Saving Checkpoint for global_step 450\n",
      "\n",
      "Time Elapased:  6.301251824000246\n",
      "\n",
      "Epoch:  451\n",
      "Training Loss:  0.5219001\n",
      "Validation Loss:  1.548495\n",
      "\n",
      "Time Elapased:  5.9629978249995474\n",
      "\n",
      "Epoch:  452\n",
      "Training Loss:  0.52442014\n",
      "Validation Loss:  1.5554864\n",
      "\n",
      "Time Elapased:  5.960332362999907\n",
      "\n",
      "Epoch:  453\n",
      "Training Loss:  0.5214285\n",
      "Validation Loss:  1.5550704\n",
      "\n",
      "Time Elapased:  6.012193398000818\n",
      "\n",
      "Epoch:  454\n",
      "Training Loss:  0.51719725\n",
      "Validation Loss:  1.5592326\n",
      "\n",
      "Time Elapased:  5.9058575029994245\n",
      "\n",
      "Epoch:  455\n",
      "Training Loss:  0.5108939\n",
      "Validation Loss:  1.555094\n",
      "\n",
      "Time Elapased:  5.929632831000163\n",
      "\n",
      "Epoch:  456\n",
      "Training Loss:  0.51279867\n",
      "Validation Loss:  1.562747\n",
      "\n",
      "Time Elapased:  5.964318666999134\n",
      "\n",
      "Epoch:  457\n",
      "Training Loss:  0.5103358\n",
      "Validation Loss:  1.5590992\n",
      "\n",
      "Time Elapased:  6.120668870999907\n",
      "\n",
      "Epoch:  458\n",
      "Training Loss:  0.5052581\n",
      "Validation Loss:  1.5604746\n",
      "\n",
      "Time Elapased:  5.916525282000293\n",
      "\n",
      "Epoch:  459\n",
      "Training Loss:  0.5041315\n",
      "Validation Loss:  1.5652946\n",
      "\n",
      "Time Elapased:  6.044524174999424\n",
      "\n",
      "Epoch:  460\n",
      "Training Loss:  0.5034783\n",
      "Validation Loss:  1.5614653\n",
      "Saving Checkpoint for global_step 460\n",
      "\n",
      "Time Elapased:  6.4691059690003385\n",
      "\n",
      "Epoch:  461\n",
      "Training Loss:  0.4999071\n",
      "Validation Loss:  1.5677555\n",
      "\n",
      "Time Elapased:  6.000969558000179\n",
      "\n",
      "Epoch:  462\n",
      "Training Loss:  0.49792522\n",
      "Validation Loss:  1.56531\n",
      "\n",
      "Time Elapased:  5.948144101999787\n",
      "\n",
      "Epoch:  463\n",
      "Training Loss:  0.49526647\n",
      "Validation Loss:  1.5670457\n",
      "\n",
      "Time Elapased:  6.032849102\n",
      "\n",
      "Epoch:  464\n",
      "Training Loss:  0.49494186\n",
      "Validation Loss:  1.5705097\n",
      "\n",
      "Time Elapased:  5.970338461999745\n",
      "\n",
      "Epoch:  465\n",
      "Training Loss:  0.49358976\n",
      "Validation Loss:  1.5686263\n",
      "\n",
      "Time Elapased:  6.037636646999999\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  466\n",
      "Training Loss:  0.4957801\n",
      "Validation Loss:  1.5718012\n",
      "\n",
      "Time Elapased:  5.991114006000316\n",
      "\n",
      "Epoch:  467\n",
      "Training Loss:  0.49593532\n",
      "Validation Loss:  1.5719974\n",
      "\n",
      "Time Elapased:  5.882351249000749\n",
      "\n",
      "Epoch:  468\n",
      "Training Loss:  0.4941706\n",
      "Validation Loss:  1.5711542\n",
      "\n",
      "Time Elapased:  5.7840890970001055\n",
      "\n",
      "Epoch:  469\n",
      "Training Loss:  0.49043295\n",
      "Validation Loss:  1.5760565\n",
      "\n",
      "Time Elapased:  5.925197380999634\n",
      "\n",
      "Epoch:  470\n",
      "Training Loss:  0.49177653\n",
      "Validation Loss:  1.5744491\n",
      "Saving Checkpoint for global_step 470\n",
      "\n",
      "Time Elapased:  6.415263044999847\n",
      "\n",
      "Epoch:  471\n",
      "Training Loss:  0.49537516\n",
      "Validation Loss:  1.5751716\n",
      "\n",
      "Time Elapased:  6.142953067000235\n",
      "\n",
      "Epoch:  472\n",
      "Training Loss:  0.48929465\n",
      "Validation Loss:  1.5772574\n",
      "\n",
      "Time Elapased:  5.984634774999904\n",
      "\n",
      "Epoch:  473\n",
      "Training Loss:  0.49005035\n",
      "Validation Loss:  1.5810537\n",
      "\n",
      "Time Elapased:  6.0907010359997\n",
      "\n",
      "Epoch:  474\n",
      "Training Loss:  0.4901415\n",
      "Validation Loss:  1.5833912\n",
      "\n",
      "Time Elapased:  5.986985335000099\n",
      "\n",
      "Epoch:  475\n",
      "Training Loss:  0.48912024\n",
      "Validation Loss:  1.5756962\n",
      "\n",
      "Time Elapased:  5.9628415010001845\n",
      "\n",
      "Epoch:  476\n",
      "Training Loss:  0.48716903\n",
      "Validation Loss:  1.5852315\n",
      "\n",
      "Time Elapased:  5.964266152000164\n",
      "\n",
      "Epoch:  477\n",
      "Training Loss:  0.48368785\n",
      "Validation Loss:  1.5808926\n",
      "\n",
      "Time Elapased:  5.892960857999242\n",
      "\n",
      "Epoch:  478\n",
      "Training Loss:  0.4835058\n",
      "Validation Loss:  1.585454\n",
      "\n",
      "Time Elapased:  5.992921930000193\n",
      "\n",
      "Epoch:  479\n",
      "Training Loss:  0.48160943\n",
      "Validation Loss:  1.5844212\n",
      "\n",
      "Time Elapased:  6.041212762000214\n",
      "\n",
      "Epoch:  480\n",
      "Training Loss:  0.48144525\n",
      "Validation Loss:  1.5857468\n",
      "Saving Checkpoint for global_step 480\n",
      "\n",
      "Time Elapased:  6.309481049999704\n",
      "\n",
      "Epoch:  481\n",
      "Training Loss:  0.48080313\n",
      "Validation Loss:  1.5898283\n",
      "\n",
      "Time Elapased:  5.990757710999787\n",
      "\n",
      "Epoch:  482\n",
      "Training Loss:  0.48268038\n",
      "Validation Loss:  1.5917201\n",
      "\n",
      "Time Elapased:  6.037987232999512\n",
      "\n",
      "Epoch:  483\n",
      "Training Loss:  0.48609477\n",
      "Validation Loss:  1.5952611\n",
      "\n",
      "Time Elapased:  5.954823042000498\n",
      "\n",
      "Epoch:  484\n",
      "Training Loss:  0.48400754\n",
      "Validation Loss:  1.589478\n",
      "\n",
      "Time Elapased:  6.0148415020003085\n",
      "\n",
      "Epoch:  485\n",
      "Training Loss:  0.479838\n",
      "Validation Loss:  1.5899451\n",
      "\n",
      "Time Elapased:  5.942722597000284\n",
      "\n",
      "Epoch:  486\n",
      "Training Loss:  0.47843355\n",
      "Validation Loss:  1.598357\n",
      "\n",
      "Time Elapased:  5.853223934999733\n",
      "\n",
      "Epoch:  487\n",
      "Training Loss:  0.47665793\n",
      "Validation Loss:  1.5947344\n",
      "\n",
      "Time Elapased:  5.850484090999998\n",
      "\n",
      "Epoch:  488\n",
      "Training Loss:  0.4761207\n",
      "Validation Loss:  1.5965695\n",
      "\n",
      "Time Elapased:  5.984702732000187\n",
      "\n",
      "Epoch:  489\n",
      "Training Loss:  0.48625302\n",
      "Validation Loss:  1.5935874\n",
      "\n",
      "Time Elapased:  6.017412418999811\n",
      "\n",
      "Epoch:  490\n",
      "Training Loss:  0.48892018\n",
      "Validation Loss:  1.5947644\n",
      "Saving Checkpoint for global_step 490\n",
      "\n",
      "Time Elapased:  6.368224443999679\n",
      "\n",
      "Epoch:  491\n",
      "Training Loss:  0.48992196\n",
      "Validation Loss:  1.597069\n",
      "\n",
      "Time Elapased:  5.926740293999501\n",
      "\n",
      "Epoch:  492\n",
      "Training Loss:  0.4892229\n",
      "Validation Loss:  1.59798\n",
      "\n",
      "Time Elapased:  5.91311407899957\n",
      "\n",
      "Epoch:  493\n",
      "Training Loss:  0.4843102\n",
      "Validation Loss:  1.6032732\n",
      "\n",
      "Time Elapased:  6.104571761999978\n",
      "\n",
      "Epoch:  494\n",
      "Training Loss:  0.48370013\n",
      "Validation Loss:  1.6001849\n",
      "\n",
      "Time Elapased:  5.9354868820000775\n",
      "\n",
      "Epoch:  495\n",
      "Training Loss:  0.48613358\n",
      "Validation Loss:  1.6009179\n",
      "\n",
      "Time Elapased:  5.888780272000076\n",
      "\n",
      "Epoch:  496\n",
      "Training Loss:  0.4863245\n",
      "Validation Loss:  1.6063819\n",
      "\n",
      "Time Elapased:  5.817463046000739\n",
      "\n",
      "Epoch:  497\n",
      "Training Loss:  0.48059925\n",
      "Validation Loss:  1.605131\n",
      "\n",
      "Time Elapased:  5.951738879999539\n",
      "\n",
      "Epoch:  498\n",
      "Training Loss:  0.47749302\n",
      "Validation Loss:  1.6042712\n",
      "\n",
      "Time Elapased:  5.770745281999552\n",
      "\n",
      "Epoch:  499\n",
      "Training Loss:  0.47571608\n",
      "Validation Loss:  1.6090279\n",
      "\n",
      "Time Elapased:  5.826076782999735\n",
      "\n",
      "Epoch:  500\n",
      "Training Loss:  0.47503394\n",
      "Validation Loss:  1.6067755\n",
      "Saving Checkpoint for global_step 500\n",
      "\n",
      "Time Elapased:  6.320797385000333\n"
     ]
    }
   ],
   "source": [
    "# Train Model\n",
    "learning_rate = 0.01\n",
    "keep_prob = 0.8\n",
    "num_epochs = 500\n",
    "all_train = (VideoIds_train, Framestamps_train, H_train, Ipast_train, Ifuture_train, Ycaptions_train, Xcaptions_train)\n",
    "all_val =   (VideoIds_val, Framestamps_val, H_val,   Ipast_val,   Ifuture_val,   Ycaptions_val,   Xcaptions_val)\n",
    "execute = model(all_train, all_val, learning_rate, keep_prob, num_epochs, home_dir, version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Restoring from latest checkpoint...\n",
      "INFO:tensorflow:Restoring parameters from /home/songzeli/checkpoints_100_train_attention_in_graph/model-500\n",
      "0.4731813\n"
     ]
    }
   ],
   "source": [
    "def setup_graph_and_saver(learning_rate):\n",
    "    tf.reset_default_graph()    \n",
    "    tf.set_random_seed(1)                             \n",
    "    seed = 3 \n",
    "    global_step = tf.Variable(0, name='global_step', trainable=False)\n",
    "    config = Config()\n",
    "    spj = SPJ(config)\n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate = learning_rate).minimize(spj._loss, global_step=global_step)\n",
    "    #optimizer = tf.train.MomentumOptimizer(learning_rate=learning_rate,momentum=0.9).minimize(spj._loss, global_step=global_step)  \n",
    "    init = tf.global_variables_initializer()\n",
    "    saver = tf.train.Saver()\n",
    "    return spj, saver, global_step, optimizer, init, seed\n",
    "\n",
    "def direct_inference(data, learning_rate, minibatch_size,home_dir, version):\n",
    "\n",
    "    # Extract Test Data\n",
    "    (VideoIds, Framestamps, H, Ipast, Ifuture, Ycaptions, Xcaptions) = data\n",
    "    num_data = H.shape[0]\n",
    "    \n",
    "    # Setup Graph\n",
    "    spj, saver, global_step, optimizer, init, seed = setup_graph_and_saver(learning_rate)\n",
    "    # Directory Where Saved Checkpoint\n",
    "    checkpoint_dir = home_dir + \"/checkpoints_\" + str(version) + \"/\"\n",
    "    \n",
    "    # Start Session\n",
    "    with tf.Session() as sess:\n",
    "\n",
    "        # Check for Latest Checkpoint\n",
    "        latest_checkpoint = tf.train.latest_checkpoint(checkpoint_dir)\n",
    "        print(\"Restoring from latest checkpoint...\")\n",
    "        saver.restore(sess, latest_checkpoint)\n",
    "        \n",
    "        # Get minibatches\n",
    "        num_minibatches = num_data // minibatch_size  \n",
    "        seed = seed + 1\n",
    "        minibatches = random_mini_batches(VideoIds, Framestamps, H, Ipast, Ifuture, Ycaptions, Xcaptions, minibatch_size, seed) \n",
    "        \n",
    "        losses = []\n",
    "        \n",
    "        # For all batchs\n",
    "        for counter, minibatch in enumerate(minibatches):\n",
    "            \n",
    "            # Select minibatch\n",
    "            (minibatch_VideoIds, minibatch_Framestamps, minibatch_H, minibatch_Ipast, minibatch_Ifuture, minibatch_Ycaptions, minibatch_Xcaptions) = minibatch\n",
    "            minibatch_Ycaptions = id_2_one_hot_void_padding(minibatch_Ycaptions, spj.config.num_classes, void_dim=0)\n",
    "            \n",
    "            # Feed\n",
    "            feed={spj._H: minibatch_H, \n",
    "                  spj._Ipast: minibatch_Ipast, \n",
    "                  spj._Ifuture: minibatch_Ifuture, \n",
    "                  spj._x: minibatch_Xcaptions, \n",
    "                  spj._y: minibatch_Ycaptions, \n",
    "                  spj._keep_prob: 1.0,\n",
    "                  spj._reg: 0.0\n",
    "                 }\n",
    "            \n",
    "            # Run Predictions\n",
    "            loss, pred, lab = sess.run([spj._loss, spj._predictions, spj._y], feed_dict=feed) \n",
    "            lab = np.argmax(lab,axis=3)\n",
    "            losses.append(loss)\n",
    "            \n",
    "            # Cache Results\n",
    "            if counter == 0:\n",
    "                predictions = pred\n",
    "                labels = lab\n",
    "                ids = minibatch_VideoIds\n",
    "            else:\n",
    "                predictions = np.concatenate((predictions,pred),axis=0)\n",
    "                labels = np.concatenate((labels,lab),axis=0)\n",
    "                ids = np.concatenate((ids, minibatch_VideoIds),axis=0)\n",
    "        avg_loss = np.mean(losses)\n",
    "        print(avg_loss)\n",
    "\n",
    "    return predictions, labels, ids\n",
    "data = (VideoIds_train, Framestamps_train, H_train, Ipast_train, Ifuture_train, Ycaptions_train, Xcaptions_train)\n",
    "#data =   (VideoIds_val, Framestamps_val, H_val,   Ipast_val,   Ifuture_val,   Ycaptions_val,   Xcaptions_val)\n",
    "#data =   (VideoIds_test, Framestamps_test, H_test,   Ipast_test,   Ifuture_test,   Ycaptions_test,   Xcaptions_test)\n",
    "predictions2, labels2, ids2 = direct_inference(data, learning_rate, minibatch_size, home_dir, version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "VIDEO ID             PREDICTION           LABEL               \n",
      "--------             -----                -----               \n",
      "v_sjyZWmvTGA4        he                   b'<pad>'            \n",
      "v_sjyZWmvTGA4        also                 b'<pad>'            \n",
      "v_sjyZWmvTGA4        out                  b'<pad>'            \n",
      "v_sjyZWmvTGA4        out                  b'<pad>'            \n",
      "v_sjyZWmvTGA4        front                b'<pad>'            \n",
      "v_sjyZWmvTGA4        the                  b'<pad>'            \n",
      "v_sjyZWmvTGA4        shed                 b'<pad>'            \n",
      "v_sjyZWmvTGA4        home                 b'<pad>'            \n",
      "v_sjyZWmvTGA4        b'<end>'             b'<pad>'            \n",
      "v_sjyZWmvTGA4        b'<end>'             b'<pad>'            \n",
      "v_sjyZWmvTGA4        b'<end>'             b'<pad>'            \n",
      "v_sjyZWmvTGA4        b'<end>'             b'<pad>'            \n",
      "v_sjyZWmvTGA4        a                    b'<pad>'            \n",
      "v_sjyZWmvTGA4        b'<end>'             b'<pad>'            \n",
      "v_sjyZWmvTGA4        the                  b'<pad>'            \n",
      "v_sjyZWmvTGA4        b'<end>'             b'<pad>'            \n",
      "v_sjyZWmvTGA4        b'<end>'             b'<pad>'            \n",
      "v_sjyZWmvTGA4        head                 b'<pad>'            \n",
      "v_sjyZWmvTGA4        bottom               b'<pad>'            \n",
      "v_sjyZWmvTGA4        left                 b'<pad>'            \n",
      "v_sjyZWmvTGA4        b'<end>'             b'<pad>'            \n",
      "v_sjyZWmvTGA4        b'<end>'             b'<pad>'            \n",
      "v_sjyZWmvTGA4        b'<end>'             b'<pad>'            \n",
      "v_sjyZWmvTGA4        b'<end>'             b'<pad>'            \n",
      "v_sjyZWmvTGA4        b'<end>'             b'<pad>'            \n",
      "v_sjyZWmvTGA4        b'<end>'             b'<pad>'            \n",
      "v_sjyZWmvTGA4        b'<end>'             b'<pad>'            \n",
      "v_sjyZWmvTGA4        b'<end>'             b'<pad>'            \n",
      "v_sjyZWmvTGA4        b'<end>'             b'<pad>'            \n",
      "v_sjyZWmvTGA4        b'<end>'             b'<pad>'            \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print_pred_and_labels(predictions2, labels2, ids2, id2word, example=1, proposal=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/shared/anaconda3/lib/python3.6/site-packages/nltk/translate/bleu_score.py:490: UserWarning: \n",
      "Corpus/Sentence contains 0 counts of 2-gram overlaps.\n",
      "BLEU scores might be undesirable; use SmoothingFunction().\n",
      "  warnings.warn(_msg)\n",
      "/home/shared/anaconda3/lib/python3.6/site-packages/nltk/translate/bleu_score.py:490: UserWarning: \n",
      "Corpus/Sentence contains 0 counts of 3-gram overlaps.\n",
      "BLEU scores might be undesirable; use SmoothingFunction().\n",
      "  warnings.warn(_msg)\n",
      "/home/shared/anaconda3/lib/python3.6/site-packages/nltk/translate/bleu_score.py:490: UserWarning: \n",
      "Corpus/Sentence contains 0 counts of 4-gram overlaps.\n",
      "BLEU scores might be undesirable; use SmoothingFunction().\n",
      "  warnings.warn(_msg)\n",
      "/home/shared/anaconda3/lib/python3.6/site-packages/nltk/translate/bleu_score.py:490: UserWarning: \n",
      "Corpus/Sentence contains 0 counts of 5-gram overlaps.\n",
      "BLEU scores might be undesirable; use SmoothingFunction().\n",
      "  warnings.warn(_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5314450908821954 0.5159225729973744 0.5282469538767958 0.5400661911257377\n"
     ]
    }
   ],
   "source": [
    "bleu1, bleu2, bleu3, bleu4 = compute_bleu_at_1_2_3_4(labels2, predictions2)\n",
    "print(bleu1, bleu2, bleu3, bleu4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/songzeli\n"
     ]
    }
   ],
   "source": [
    "print(home_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from /home/songzeli/checkpoints_100_train_attention_in_graph/model-500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/songzeli/SPJ/utils/data_utils.py:415: RuntimeWarning: divide by zero encountered in log\n",
      "  a = np.log(a) / (temperature)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "word_id:  [[[2.0000e+00 2.7440e+03 8.8500e+02 ... 1.4570e+03 3.5800e+02 5.6400e+03]\n",
      "  [2.0000e+00 1.7000e+02 4.7210e+03 ... 4.3320e+03 3.2540e+03 1.0947e+04]\n",
      "  [2.0000e+00 4.9280e+03 6.0040e+03 ... 4.7980e+03 1.0270e+04 4.7210e+03]\n",
      "  ...\n",
      "  [2.0000e+00 8.4150e+03 2.3670e+03 ... 8.7480e+03 1.1840e+03 9.7890e+03]\n",
      "  [2.0000e+00 4.7210e+03 6.7670e+03 ... 4.2680e+03 3.1160e+03 1.8770e+03]\n",
      "  [2.0000e+00 9.9000e+02 3.2540e+03 ... 3.1960e+03 3.3300e+03 1.0140e+03]]\n",
      "\n",
      " [[2.0000e+00 1.4570e+03 5.0130e+03 ... 4.8790e+03 6.3080e+03 7.8510e+03]\n",
      "  [2.0000e+00 9.2180e+03 8.2450e+03 ... 6.3010e+03 9.1590e+03 6.4860e+03]\n",
      "  [2.0000e+00 1.7000e+02 9.9890e+03 ... 3.0000e+00 9.7150e+03 6.6210e+03]\n",
      "  ...\n",
      "  [2.0000e+00 9.7610e+03 5.5340e+03 ... 8.9230e+03 5.6400e+03 1.6200e+02]\n",
      "  [2.0000e+00 5.8070e+03 1.5810e+03 ... 3.1960e+03 9.7610e+03 1.0875e+04]\n",
      "  [2.0000e+00 2.7440e+03 9.9240e+03 ... 3.6890e+03 9.1160e+03 3.1160e+03]]\n",
      "\n",
      " [[2.0000e+00 9.6130e+03 5.1000e+03 ... 7.7260e+03 6.2390e+03 2.8580e+03]\n",
      "  [2.0000e+00 1.0348e+04 2.0430e+03 ... 2.8580e+03 1.7000e+02 1.0528e+04]\n",
      "  [2.0000e+00 3.5800e+02 3.8720e+03 ... 2.8730e+03 6.7490e+03 5.4220e+03]\n",
      "  ...\n",
      "  [2.0000e+00 6.4050e+03 9.7250e+03 ... 1.0578e+04 1.9030e+03 6.2390e+03]\n",
      "  [2.0000e+00 4.7590e+03 2.2340e+03 ... 6.9700e+02 1.6200e+02 1.0838e+04]\n",
      "  [2.0000e+00 9.7660e+03 2.3670e+03 ... 2.8580e+03 1.0815e+04 8.8500e+02]]\n",
      "\n",
      " ...\n",
      "\n",
      " [[2.0000e+00 9.3130e+03 1.7000e+02 ... 7.7610e+03 6.3010e+03 3.0000e+00]\n",
      "  [2.0000e+00 5.7310e+03 8.8500e+02 ... 9.7890e+03 9.7660e+03 3.6500e+02]\n",
      "  [2.0000e+00 5.5340e+03 9.9240e+03 ... 7.3970e+03 4.1090e+03 2.1730e+03]\n",
      "  ...\n",
      "  [2.0000e+00 3.4690e+03 6.7510e+03 ... 6.6210e+03 8.2450e+03 7.7610e+03]\n",
      "  [2.0000e+00 2.7440e+03 2.2340e+03 ... 1.0874e+04 4.7210e+03 3.1160e+03]\n",
      "  [2.0000e+00 5.7140e+03 8.8950e+03 ... 1.0838e+04 3.0000e+00 1.0838e+04]]\n",
      "\n",
      " [[2.0000e+00 2.2550e+03 7.0840e+03 ... 1.4570e+03 2.8580e+03 2.9150e+03]\n",
      "  [2.0000e+00 3.0610e+03 1.8600e+02 ... 1.7620e+03 1.0875e+04 7.9240e+03]\n",
      "  [2.0000e+00 9.7660e+03 3.1700e+03 ... 5.3510e+03 5.4220e+03 2.9150e+03]\n",
      "  ...\n",
      "  [2.0000e+00 9.7610e+03 2.1810e+03 ... 8.4770e+03 2.0510e+03 9.0460e+03]\n",
      "  [2.0000e+00 5.8070e+03 6.3290e+03 ... 6.4860e+03 3.3300e+03 2.8580e+03]\n",
      "  [2.0000e+00 7.2360e+03 2.2400e+02 ... 5.9040e+03 9.9970e+03 6.1940e+03]]\n",
      "\n",
      " [[2.0000e+00 7.3680e+03 5.5340e+03 ... 6.4050e+03 9.1810e+03 8.8690e+03]\n",
      "  [2.0000e+00 4.7590e+03 7.1570e+03 ... 3.0000e+00 6.4860e+03 9.2180e+03]\n",
      "  [2.0000e+00 4.9280e+03 7.3700e+03 ... 6.7500e+03 5.9040e+03 2.0510e+03]\n",
      "  ...\n",
      "  [2.0000e+00 8.8830e+03 1.0329e+04 ... 1.7860e+03 9.1810e+03 8.6280e+03]\n",
      "  [2.0000e+00 4.7210e+03 5.1600e+02 ... 6.1440e+03 2.1730e+03 1.0460e+04]\n",
      "  [2.0000e+00 4.9280e+03 4.7210e+03 ... 1.0815e+04 9.1810e+03 6.6210e+03]]]\n"
     ]
    }
   ],
   "source": [
    "tf.set_random_seed(1)                             \n",
    "seed = 3            \n",
    "from utils.data_utils import *\n",
    "\n",
    "tf.reset_default_graph()\n",
    "config = Config()\n",
    "spj = SPJ(config)\n",
    "global_step = tf.Variable(0, name='global_step', trainable=False)\n",
    "checkpoint_dir = \"/home/songzeli/checkpoints_100_train_attention_in_graph\"\n",
    "latest_checkpoint = tf.train.latest_checkpoint(checkpoint_dir)\n",
    "saver = tf.train.Saver()\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate = learning_rate).minimize(spj._loss, global_step=global_step)\n",
    "# train_minibatches = random_mini_batches(H_train, Ipast_train, Ifuture_train, Ycaptions_train, Xcaptions_train, minibatch_size, seed)\n",
    "train_minibatches = random_mini_batches(VideoIds_train, Framestamps_train, H_train, Ipast_train, Ifuture_train, Ycaptions_train, Xcaptions_train, minibatch_size, seed)\n",
    "# (minibatch_H_train, minibatch_Ipast_train, minibatch_Ifuture_train, minibatch_Ycaptions_train, minibatch_Xcaptions_train) = train_minibatches[0]   \n",
    "(minibatch_VideoIds_train, minibatch_Framestamps_train, minibatch_H_train, minibatch_Ipast_train, minibatch_Ifuture_train, minibatch_Ycaptions_train, minibatch_Xcaptions_train) = train_minibatches[0]\n",
    "with tf.Session() as sess:   \n",
    "    saver.restore(sess, latest_checkpoint)\n",
    "    word_id = spj.caption_generation(sess,minibatch_H_train, minibatch_Ipast_train, minibatch_Ifuture_train, minibatch_Xcaptions_train, minibatch_Ycaptions_train)\n",
    "print (\"word_id: \", word_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b'<sta>'\n",
      "then\n",
      "boys\n",
      "him\n",
      "smiling\n",
      "using\n",
      "welding\n",
      "two\n",
      "welding\n",
      "through\n",
      "rides\n",
      "a\n",
      "ocean\n",
      "kitchen\n",
      "event\n",
      "event\n",
      "her\n",
      "jason\n",
      "have\n",
      "moving\n",
      "practiced\n",
      "and\n",
      "ping\n",
      "ground\n",
      "the\n",
      "she\n",
      "b'<end>'\n",
      "while\n",
      "by\n"
     ]
    }
   ],
   "source": [
    "word_id.shape\n",
    "bat = 1\n",
    "length = 5\n",
    "for k in range(word_id.shape[2]):\n",
    "    print(id2word[word_id[bat,length,k]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def direct_inference(data, learning_rate, minibatch_size,home_dir, version):\n",
    "\n",
    "    # Extract Test Data\n",
    "    (VideoIds, Framestamps, H, Ipast, Ifuture, Ycaptions, Xcaptions) = data\n",
    "    num_data = H.shape[0]\n",
    "    \n",
    "    # Setup Graph\n",
    "    spj, saver, global_step, optimizer, init, seed = setup_graph_and_saver(learning_rate)\n",
    "    # Directory Where Saved Checkpoint\n",
    "    checkpoint_dir = home_dir + \"/checkpoints_\" + str(version) + \"/\"\n",
    "    \n",
    "    # Start Session\n",
    "    with tf.Session() as sess:\n",
    "\n",
    "        # Check for Latest Checkpoint\n",
    "        latest_checkpoint = tf.train.latest_checkpoint(checkpoint_dir)\n",
    "        print(\"Restoring from latest checkpoint...\")\n",
    "        saver.restore(sess, latest_checkpoint)\n",
    "        \n",
    "        # Get minibatches\n",
    "        num_minibatches = num_data // minibatch_size  \n",
    "        seed = seed + 1\n",
    "        minibatches = random_mini_batches(VideoIds, Framestamps, H, Ipast, Ifuture, Ycaptions, Xcaptions, minibatch_size, seed) \n",
    "        \n",
    "        losses = []\n",
    "        \n",
    "        # For all batchs\n",
    "        for counter, minibatch in enumerate(minibatches):\n",
    "            \n",
    "            # Select minibatch\n",
    "            (minibatch_VideoIds, minibatch_Framestamps, minibatch_H, minibatch_Ipast, minibatch_Ifuture, minibatch_Ycaptions, minibatch_Xcaptions) = minibatch\n",
    "            minibatch_Ycaptions = id_2_one_hot_void_padding(minibatch_Ycaptions, spj.config.num_classes, void_dim=0)\n",
    "            \n",
    "            # Feed\n",
    "            feed={spj._H: minibatch_H, \n",
    "                  spj._Ipast: minibatch_Ipast, \n",
    "                  spj._Ifuture: minibatch_Ifuture, \n",
    "                  spj._x: minibatch_Xcaptions, \n",
    "                  spj._y: minibatch_Ycaptions, \n",
    "                  spj._keep_prob: 1.0,\n",
    "                  spj._reg: 0.0\n",
    "                 }\n",
    "            \n",
    "            # Run Predictions\n",
    "            loss, pred, lab = sess.run([spj._loss, spj._predictions, spj._y], feed_dict=feed) \n",
    "            lab = np.argmax(lab,axis=3)\n",
    "            losses.append(loss)\n",
    "            \n",
    "            # Cache Results\n",
    "            if counter == 0:\n",
    "                predictions = pred\n",
    "                labels = lab\n",
    "                ids = minibatch_VideoIds\n",
    "            else:\n",
    "                predictions = np.concatenate((predictions,pred),axis=0)\n",
    "                labels = np.concatenate((labels,lab),axis=0)\n",
    "                ids = np.concatenate((ids, minibatch_VideoIds),axis=0)\n",
    "        avg_loss = np.mean(losses)\n",
    "        print(avg_loss)\n",
    "\n",
    "    return predictions, labels, ids\n",
    "data = (VideoIds_train, Framestamps_train, H_train, Ipast_train, Ifuture_train, Ycaptions_train, Xcaptions_train)\n",
    "#data =   (VideoIds_val, Framestamps_val, H_val,   Ipast_val,   Ifuture_val,   Ycaptions_val,   Xcaptions_val)\n",
    "#data =   (VideoIds_test, Framestamps_test, H_test,   Ipast_test,   Ifuture_test,   Ycaptions_test,   Xcaptions_test)\n",
    "predictions2, labels2, ids2 = direct_inference(data, learning_rate, minibatch_size, home_dir, version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(VideoIds, Framestamps, H, Ipast, Ifuture, Ycaptions, Xcaptions) = data\n",
    "num_data = H.shape[0]\n",
    "\n",
    "# Setup Graph\n",
    "spj, saver, global_step, optimizer, init, seed = setup_graph_and_saver(learning_rate)\n",
    "# Directory Where Saved Checkpoint\n",
    "checkpoint_dir = home_dir + \"/checkpoints_\" + str(version) + \"/\"\n",
    "\n",
    "# Start Session\n",
    "with tf.Session() as sess:\n",
    "\n",
    "    # Check for Latest Checkpoint\n",
    "    latest_checkpoint = tf.train.latest_checkpoint(checkpoint_dir)\n",
    "    print(\"Restoring from latest checkpoint...\")\n",
    "    saver.restore(sess, latest_checkpoint)\n",
    "\n",
    "    # Get minibatches\n",
    "    num_minibatches = num_data // minibatch_size  \n",
    "    seed = seed + 1\n",
    "    minibatches = random_mini_batches(VideoIds, Framestamps, H, Ipast, Ifuture, Ycaptions, Xcaptions, minibatch_size, seed) \n",
    "\n",
    "    losses = []\n",
    "\n",
    "    # For all batchs\n",
    "    for counter, minibatch in enumerate(minibatches):\n",
    "\n",
    "        # Select minibatch\n",
    "        (minibatch_VideoIds, minibatch_Framestamps, minibatch_H, minibatch_Ipast, minibatch_Ifuture, minibatch_Ycaptions, minibatch_Xcaptions) = minibatch\n",
    "        minibatch_Ycaptions = id_2_one_hot_void_padding(minibatch_Ycaptions, spj.config.num_classes, void_dim=0)\n",
    "\n",
    "        # Feed\n",
    "        feed={spj._H: minibatch_H, \n",
    "              spj._Ipast: minibatch_Ipast, \n",
    "              spj._Ifuture: minibatch_Ifuture, \n",
    "              spj._x: minibatch_Xcaptions, \n",
    "              spj._y: minibatch_Ycaptions, \n",
    "              spj._keep_prob: 1.0,\n",
    "              spj._reg: 0.0\n",
    "             }\n",
    "\n",
    "        # Run Predictions\n",
    "        loss, pred, lab,lstm_outputs,Hout = sess.run([spj._loss, spj._predictions, spj._y, spj._lstm_outputs,spj._Hout], feed_dict=feed) \n",
    "        lab = np.argmax(lab,axis=3)\n",
    "        losses.append(loss)\n",
    "        \n",
    "#         print (lstm_outputs.shape)\n",
    "\n",
    "        # Cache Results\n",
    "        if counter == 0:\n",
    "            predictions = pred\n",
    "            labels = lab\n",
    "            ids = minibatch_VideoIds\n",
    "            lstm_out = lstm_outputs\n",
    "            mini_H = minibatch_H\n",
    "            mini_Ipast = minibatch_Ipast\n",
    "            mini_Ifuture = minibatch_Ifuture\n",
    "            Hout_output = Hout\n",
    "        else:\n",
    "            predictions = np.concatenate((predictions,pred),axis=0)\n",
    "            labels = np.concatenate((labels,lab),axis=0)\n",
    "            ids = np.concatenate((ids, minibatch_VideoIds),axis=0)\n",
    "            lstm_out = np.concatenate((lstm_out,lstm_outputs),axis=0)\n",
    "            mini_H = np.concatenate((mini_H,minibatch_H),axis=0)\n",
    "            mini_Ipast = np.concatenate((mini_Ipast,minibatch_Ipast),axis=0)\n",
    "            mini_Ifuture = np.concatenate((mini_Ifuture,minibatch_Ifuture),axis=0)\n",
    "            Hout_output = np.concatenate((Hout_output,Hout),axis=0)\n",
    "    avg_loss = np.mean(losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = (VideoIds_train, Framestamps_train, H_train, Ipast_train, Ifuture_train, Ycaptions_train, Xcaptions_train)\n",
    "#data =   (VideoIds_val, Framestamps_val, H_val,   Ipast_val,   Ifuture_val,   Ycaptions_val,   Xcaptions_val)\n",
    "#data =   (VideoIds_test, Framestamps_test, H_test,   Ipast_test,   Ifuture_test,   Ycaptions_test,   Xcaptions_test)\n",
    "predictions2, labels2, ids2 = direct_inference(data, learning_rate, minibatch_size, home_dir, version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_pred_and_labels(predictions2, labels2, ids2, id2word, example=78, proposal=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "np.random.seed(0)\n",
    "import seaborn as sns\n",
    "sns.set()\n",
    "att = np.abs(lstm_out[60,3,:-128].reshape(3,-1))\n",
    "# ax = sns.heatmap(att)\n",
    "ax = sns.heatmap(np.sum(att,axis = 1).reshape(1,-1))\n",
    "print('past',np.sum(att,axis = 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.Session() as sess:\n",
    "        # Check for Latest Checkpoint\n",
    "    latest_checkpoint = tf.train.latest_checkpoint(checkpoint_dir)\n",
    "    print(\"Restoring from latest checkpoint...\")\n",
    "    saver.restore(sess, latest_checkpoint)\n",
    "    variables_names =[v.name for v in tf.trainable_variables()]\n",
    "    print(variables_names)\n",
    "    w_lstm = 'rnn/multi_rnn_cell/cell_1/lstm_cell/kernel:0'\n",
    "    values = sess.run(w_lstm)\n",
    "    print(values.shape)\n",
    "#     for k,v in zip(variables_names, values):\n",
    "#         print(k)\n",
    "#          print('rnn/multi_rnn_cell/cell_1/lstm_cell/kernel:0', v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = sns.heatmap(att_Past[10,0:10].reshape(1,-1),xticklabels=2, yticklabels=False,cmap=\"YlGnBu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = sns.heatmap(att_H[100,0:10].reshape(1,-1),xticklabels=2, yticklabels=False,cmap=\"YlGnBu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = sns.heatmap(att_Future[100,0:10].reshape(1,-1),xticklabels=2, yticklabels=False,cmap=\"YlGnBu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Attention Level of Each Word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pylab as pl\n",
    "example = 78\n",
    "proposal = 3\n",
    "ind = example*10+proposal\n",
    "attention_Past = np.transpose(lstm_out,[0,2,1])[:,:500,:]*np.expand_dims(Hout_output.reshape(-1,1500)[:,0:500],2)\n",
    "attention_H = np.transpose(lstm_out,[0,2,1])[:,:500,:]*np.expand_dims(Hout_output.reshape(-1,1500)[:,500:1000],2)\n",
    "attention_Future = np.transpose(lstm_out,[0,2,1])[:,:500,:]*np.expand_dims(Hout_output.reshape(-1,1500)[:,1000:1500],2)\n",
    "att_Past = np.sum(np.abs(attention_Past),axis=1)\n",
    "att_H = np.sum(np.abs(attention_H),axis=1)\n",
    "att_Future = np.sum(np.abs(attention_Future),axis=1)\n",
    "import matplotlib.pyplot as plt\n",
    "x = np.array([i for i in range(10)])\n",
    "my_xticks =[str(id2word[labels[example,proposal,i]]) for i in range(10)]\n",
    "plt.xticks(x, my_xticks)\n",
    "plt.plot((att_Past[ind,0:10]-min(att_Past[ind,0:10]))/max(att_Past[ind,0:10]),label='Attention_To_Past',marker='o',markersize=10)\n",
    "plt.plot((att_H[ind,0:10]-min(att_H[ind,0:10]))/max(att_H[ind,0:10]),label='Attention_To_Current',marker='s',markersize=10)\n",
    "plt.plot((att_Future[ind,0:10]-min(att_Future[ind,0:10]))/max(att_Future[ind,0:10]),label='Attention_To_Future',marker='*',markersize=10)\n",
    "plt.ylabel('Attention_Level')\n",
    "plt.legend()\n",
    "pl.xticks(rotation=-60,size=15)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Beam_Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(25, 10, 30)\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n"
     ]
    }
   ],
   "source": [
    "def softmax(x,ax = 0):\n",
    "    return np.exp(x) / np.exp(x).sum(axis = ax,keepdims =True)\n",
    "\n",
    "tf.reset_default_graph()    \n",
    "tf.set_random_seed(1)                             \n",
    "seed = 3 \n",
    "global_step = tf.Variable(0, name='global_step', trainable=False)\n",
    "config = Config()\n",
    "spj = SPJ(config)\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate = learning_rate).minimize(spj._loss, global_step=global_step)\n",
    "init = tf.global_variables_initializer()\n",
    "saver = tf.train.Saver()\n",
    "\n",
    "batch_size = 25\n",
    "beam_size = 1\n",
    "data = (H_train, Ipast_train, Ifuture_train, Ycaptions_train, Xcaptions_train)\n",
    "word_pred = np.ones([batch_size,config.num_proposals,beam_size*beam_size])*2 #id\n",
    "prob_cur = np.ones([batch_size,config.num_proposals,beam_size*beam_size]) # word_pred = word_pred*softmax(word_pred)\n",
    "prob_prev_caption = np.ones([batch_size,config.num_proposals,beam_size]) #\n",
    "temp_caption = np.zeros([batch_size,config.num_proposals,config.num_steps,beam_size*beam_size]) #beam_size*beam_size \n",
    "beam_caption = np.zeros([batch_size,config.num_proposals,config.num_steps,beam_size]) #50wordsbeam_size\n",
    "beam_caption[:,:,0,:] = 2 \n",
    "temp_caption[:,:,0,:] = 2\n",
    "num = 1\n",
    "\n",
    "\n",
    "# logits_beam = np.zeros([batch_size,config.num_proposals,config.num_steps,config.num_classes,beam_size])\n",
    "#sesssoftmaxword------beam_caption\n",
    "\n",
    "train_minibatches = random_mini_batches(VideoIds_train, Framestamps_train, H_train, Ipast_train, Ifuture_train, Ycaptions_train, Xcaptions_train, minibatch_size, seed)\n",
    "#(minibatch_H_train, minibatch_Ipast_train, minibatch_Ifuture_train, minibatch_Ycaptions_train, minibatch_Xcaptions_train) = train_minibatches[0]   \n",
    "(minibatch_VideoIds_train, minibatch_Framestamps_train, minibatch_H, minibatch_Ipast, minibatch_Ifuture, minibatch_Ycaptions, minibatch_Xcaptions) = train_minibatches[0]\n",
    "minibatch_Ycaptions = id_2_one_hot_void_padding(minibatch_Ycaptions, spj.config.num_classes, void_dim=0)\n",
    "spj, saver, global_step, optimizer, init, seed = setup_graph_and_saver(learning_rate)\n",
    "\n",
    "\n",
    "\n",
    "print(minibatch_Xcaptions.shape)\n",
    "with tf.Session() as sess:\n",
    "# (minibatch_H, minibatch_Ipast, minibatch_Ifuture, minibatch_Ycaptions, minibatch_Xcaptions) = train_minibatches[1]\n",
    "    sess.run(init)\n",
    "    while num < config.num_steps-1: \n",
    "        print(num)\n",
    "        for i in range(beam_size):\n",
    "            feed = {spj._H: minibatch_H,\n",
    "                spj._Ipast: minibatch_Ipast,\n",
    "                spj._Ifuture: minibatch_Ifuture,\n",
    "                spj._x: beam_caption[:,:,:,i],\n",
    "                spj._y: minibatch_Ycaptions,\n",
    "                spj._keep_prob: 1.0,\n",
    "                spj._reg: 0.0\n",
    "               }\n",
    "            \n",
    "            logits = sess.run(spj.logits,feed_dict=feed)\n",
    "#             print('logits',logits.shape)\n",
    "            logits_softmax = softmax(logits,3)\n",
    "            beam_softmax = np.sort(logits_softmax)[:,:,:,::-1][:,:,:,:beam_size] # softmax \n",
    "            prob_cur[:,:,i*beam_size:(i+1)*beam_size] = beam_softmax[:,:,num,:]\n",
    "            beam_word_id = np.argsort(logits_softmax[:,:,:,:],axis =3)[:,:,:,::-1][:,:,:,:beam_size] #beam_size index\n",
    "            word_pred[:,:,i*beam_size:(i+1)*beam_size] = beam_word_id[:,:,num,:]\n",
    "            temp_caption[:,:,num,i*beam_size:(i+1)*beam_size] = word_pred[:,:,i*beam_size:(i+1)*beam_size] #pred_wordtemp_caption. \n",
    "            prob_cur[:,:,i*beam_size:(i+1)*beam_size] = prob_cur[:,:,i*beam_size:(i+1)*beam_size] * np.expand_dims(prob_prev_caption[:,:,i],axis=2)\n",
    "#             print(id2word[int(temp_caption[1,1,num,0])])\n",
    "        caption_id = np.argsort(prob_cur,axis=2)[:,:,::-1][:,:,:beam_size]\n",
    "#       \n",
    "        for i in range(temp_caption.shape[0]):\n",
    "            for j in range(temp_caption.shape[1]):\n",
    "                for k in range(caption_id.shape[2]):\n",
    "#                     print(id2word[int(temp_caption[i,j,num,caption_id[i,j,k]])])\n",
    "                    beam_caption[i,j,0:num+1,k] = temp_caption[i,j,0:num+1,caption_id[i,j,k]]\n",
    "#                     assert beam_caption[i,j,num,k]==temp_caption[i,j,num,caption_id[i,j,k]]\n",
    "#                     print(id2word[int(beam_caption[i,j,num,k])])\n",
    "        num += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
